# ==============================================================================
# ã‚‚ã¡ã“AI - ç©¶æ¥µã®å…¨æ©Ÿèƒ½çµ±åˆç‰ˆ (v19.2 - è¤‡æ•°ã‚¨ãƒ³ã‚¸ãƒ³çµ±åˆè¦ç´„ç‰ˆ)
#
# v19.1ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€Webæ¤œç´¢æ©Ÿèƒ½ã‚’ã€Œè¤‡æ•°ã®æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‹ã‚‰æƒ…å ±ã‚’åé›†ã—ã€
# AIãŒçµ±åˆã—ã¦è¦ç´„ã™ã‚‹ã€ã¨ã„ã†ã€ã‚ˆã‚Šé«˜åº¦ã§å …ç‰¢ãªæ–¹å¼ã«ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã—ãŸæœ€çµ‚ç‰ˆã€‚
# ==============================================================================

# ===== ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ =====
import sys
import os
import requests
import logging
import time
import threading
import json
import re
import random
import uuid
import hashlib
from datetime import datetime, timedelta, timezone
import unicodedata
from urllib.parse import quote_plus, urljoin
from functools import wraps
from threading import Lock

# --- ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ---
from flask import Flask, request, jsonify, send_from_directory, Response
from flask_cors import CORS
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Text, Boolean, inspect, text
from sqlalchemy.orm import declarative_base, sessionmaker
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import schedule
import signal
from groq import Groq

# ==============================================================================
# åŸºæœ¬è¨­å®šã¨ãƒ­ã‚®ãƒ³ã‚°
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ==============================================================================
# å®šæ•°è¨­å®š
# ==============================================================================
VOICE_DIR = '/tmp/voices'
SERVER_URL = os.environ.get('RENDER_EXTERNAL_URL', "http://localhost:5000")
background_executor = ThreadPoolExecutor(max_workers=5)
VOICEVOX_SPEAKER_ID = 20
HOLOLIVE_NEWS_URL = "https://hololive-tsuushin.com/category/holonews/"
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
]
LOCATION_CODES = { "æ±äº¬": "130000", "å¤§é˜ª": "270000", "åå¤å±‹": "230000", "ç¦å²¡": "400000", "æœ­å¹Œ": "016000" }
SPECIALIZED_SITES = {
    'Blender': {'base_url': 'https://docs.blender.org/manual/ja/latest/', 'keywords': ['Blender', 'ãƒ–ãƒ¬ãƒ³ãƒ€ãƒ¼']},
    'CGãƒ‹ãƒ¥ãƒ¼ã‚¹': {'base_url': 'https://modelinghappy.com/', 'keywords': ['CGãƒ‹ãƒ¥ãƒ¼ã‚¹', '3DCG', 'CG', 'CGæ¥­ç•Œ']},
    'è„³ç§‘å­¦ãƒ»å¿ƒç†å­¦': {'base_url': 'https://nazology.kusuguru.co.jp/', 'keywords': ['è„³ç§‘å­¦', 'å¿ƒç†å­¦', 'è„³', 'èªçŸ¥ç§‘å­¦']},
    'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•': {'base_url': 'https://community.secondlife.com/news/', 'keywords': ['ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•', 'Second Life', 'SL']},
}
HOLOMEM_KEYWORDS = [
    'ã¨ãã®ãã‚‰', 'ãƒ­ãƒœå­ã•ã‚“', 'ã•ãã‚‰ã¿ã“', 'æ˜Ÿè¡—ã™ã„ã›ã„', 'AZKi', 'å¤œç©ºãƒ¡ãƒ«', 'ã‚¢ã‚­ãƒ»ãƒ­ãƒ¼ã‚¼ãƒ³ã‚¿ãƒ¼ãƒ«', 'èµ¤äº•ã¯ã‚ã¨', 'ç™½ä¸Šãƒ•ãƒ–ã‚­', 'å¤è‰²ã¾ã¤ã‚Š', 'æ¹Šã‚ãã‚', 'ç´«å’²ã‚·ã‚ªãƒ³', 'ç™¾é¬¼ã‚ã‚„ã‚', 'ç™’æœˆã¡ã‚‡ã“', 'å¤§ç©ºã‚¹ãƒãƒ«', 'å¤§ç¥ãƒŸã‚ª', 'çŒ«åˆãŠã‹ã‚†', 'æˆŒç¥ã“ã‚ã­', 'å…ç”°ãºã“ã‚‰', 'ä¸çŸ¥ç«ãƒ•ãƒ¬ã‚¢', 'ç™½éŠ€ãƒã‚¨ãƒ«', 'å®é˜ãƒãƒªãƒ³', 'å¤©éŸ³ã‹ãªãŸ', 'è§’å·»ã‚ãŸã‚', 'å¸¸é—‡ãƒˆãƒ¯', 'å§«æ£®ãƒ«ãƒ¼ãƒŠ', 'é›ªèŠ±ãƒ©ãƒŸã‚£', 'æ¡ƒéˆ´ã­ã­', 'ç…ç™½ã¼ãŸã‚“', 'å°¾ä¸¸ãƒãƒ«ã‚«', 'ãƒ©ãƒ—ãƒ©ã‚¹ãƒ»ãƒ€ãƒ¼ã‚¯ãƒã‚¹', 'é·¹å¶ºãƒ«ã‚¤', 'åšè¡£ã“ã‚ˆã‚Š', 'æ²™èŠ±å‰ã‚¯ãƒ­ãƒ±', 'é¢¨çœŸã„ã‚ã¯', 'æ£®ã‚«ãƒªã‚ªãƒš', 'å°é³¥éŠã‚­ã‚¢ãƒ©', 'ä¸€ä¼Šé‚£å°“æ –', 'ãŒã†ã‚‹ãƒ»ãã‚‰', 'ãƒ¯ãƒˆã‚½ãƒ³ãƒ»ã‚¢ãƒ¡ãƒªã‚¢', 'IRyS', 'ã‚»ãƒ¬ã‚¹ãƒ»ãƒ•ã‚¡ã‚¦ãƒŠ', 'ã‚ªãƒ¼ãƒ­ãƒ»ã‚¯ãƒ­ãƒ‹ãƒ¼', 'ä¸ƒè©©ãƒ ãƒ¡ã‚¤', 'ãƒã‚³ã‚¹ãƒ»ãƒ™ãƒ¼ãƒ«ã‚º', 'ã‚·ã‚ªãƒªãƒ»ãƒãƒ´ã‚§ãƒ©', 'å¤çŸ³ãƒ“ã‚¸ãƒ¥ãƒ¼', 'ãƒãƒªãƒƒã‚µãƒ»ãƒ¬ã‚¤ãƒ´ãƒ³ã‚¯ãƒ­ãƒ•ãƒˆ', 'ãƒ•ãƒ¯ãƒ¯ãƒ»ã‚¢ãƒ“ã‚¹ã‚¬ãƒ¼ãƒ‰', 'ãƒ¢ã‚³ã‚³ãƒ»ã‚¢ãƒ“ã‚¹ã‚¬ãƒ¼ãƒ‰', 'ã‚¢ãƒ¦ãƒ³ãƒ€ãƒ»ãƒªã‚¹', 'ãƒ ãƒ¼ãƒŠãƒ»ãƒ›ã‚·ãƒãƒ´ã‚¡', 'ã‚¢ã‚¤ãƒ©ãƒ‹ãƒ»ã‚¤ã‚ªãƒ•ã‚£ãƒ•ãƒ†ã‚£ãƒ¼ãƒ³', 'ã‚¯ãƒ¬ã‚¤ã‚¸ãƒ¼ãƒ»ã‚ªãƒªãƒ¼', 'ã‚¢ãƒ¼ãƒ‹ãƒ£ãƒ»ãƒ¡ãƒ«ãƒ•ã‚£ãƒƒã‚µ', 'ãƒ‘ãƒ´ã‚©ãƒªã‚¢ãƒ»ãƒ¬ã‚¤ãƒ', 'ç«å¨é’', 'éŸ³ä¹ƒç€¬å¥', 'ä¸€æ¡è‰ã€…è¯', 'å„’çƒé¢¨äº­ã‚‰ã§ã‚“', 'è½Ÿã¯ã˜ã‚', 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–', 'ãƒ›ãƒ­ãƒ¡ãƒ³', 'hololive', 'YAGOO', 'æ¡ç”Ÿã‚³ã‚³', 'æ½¤ç¾½ã‚‹ã—ã‚', 'é­”ä¹ƒã‚¢ãƒ­ã‚¨', 'ä¹åä¹ä½å‘½'
]

# ==============================================================================
# ç§˜å¯†æƒ…å ±/ç’°å¢ƒå¤‰æ•° èª­ã¿è¾¼ã¿
# ==============================================================================
def get_secret(name):
    secret_file_path = f"/etc/secrets/{name}"
    if os.path.exists(secret_file_path):
        try:
            with open(secret_file_path, 'r') as f:
                return f.read().strip()
        except IOError: pass
    return os.environ.get(name)

DATABASE_URL = get_secret('DATABASE_URL') or 'sqlite:///./mochiko.db'
GROQ_API_KEY = get_secret('GROQ_API_KEY')
VOICEVOX_URL_FROM_ENV = get_secret('VOICEVOX_URL')

# ==============================================================================
# AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
# ==============================================================================
groq_client = None
VOICEVOX_ENABLED = True if VOICEVOX_URL_FROM_ENV else False
search_context_cache = {}
cache_lock = Lock()

# ==============================================================================
# Flask & ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
# ==============================================================================
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
CORS(app)

def create_db_engine_with_retry(max_retries=5, retry_delay=5):
    from sqlalchemy.exc import OperationalError
    for attempt in range(max_retries):
        try:
            connect_args = {'check_same_thread': False} if 'sqlite' in DATABASE_URL else {'connect_timeout': 10}
            engine = create_engine(DATABASE_URL, pool_pre_ping=True, pool_recycle=300, connect_args=connect_args)
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            return engine
        except OperationalError as e:
            if attempt < max_retries - 1:
                logger.warning(f"âš ï¸ DBæ¥ç¶šå¤±æ•—: {e}. {retry_delay}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤...")
                time.sleep(retry_delay)
            else:
                raise
        except Exception as e:
            raise

engine = create_db_engine_with_retry()
Base = declarative_base()

# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
# ==============================================================================
class UserMemory(Base): __tablename__ = 'user_memories'; id = Column(Integer, primary_key=True); user_uuid = Column(String(255), unique=True, nullable=False); user_name = Column(String(255), nullable=False); interaction_count = Column(Integer, default=0); last_interaction = Column(DateTime, default=datetime.utcnow)
class ConversationHistory(Base): __tablename__ = 'conversation_history'; id = Column(Integer, primary_key=True, autoincrement=True); user_uuid = Column(String(255), nullable=False, index=True); role = Column(String(10), nullable=False); content = Column(Text, nullable=False); timestamp = Column(DateTime, default=datetime.utcnow, index=True)
class HololiveNews(Base): __tablename__ = 'hololive_news'; id = Column(Integer, primary_key=True); title = Column(String(500), nullable=False); content = Column(Text, nullable=False); url = Column(String(1000)); created_at = Column(DateTime, default=datetime.utcnow, index=True); news_hash = Column(String(100), unique=True)
class SpecializedNews(Base): __tablename__ = 'specialized_news'; id = Column(Integer, primary_key=True); site_name = Column(String(100), nullable=False, index=True); title = Column(String(500), nullable=False); content = Column(Text, nullable=False); url = Column(String(1000)); created_at = Column(DateTime, default=datetime.utcnow, index=True); news_hash = Column(String(100), unique=True)
class BackgroundTask(Base): __tablename__ = 'background_tasks'; id = Column(Integer, primary_key=True); task_id = Column(String(255), unique=True, nullable=False); user_uuid = Column(String(255), nullable=False); task_type = Column(String(50), nullable=False); query = Column(Text, nullable=False); result = Column(Text); status = Column(String(20), default='pending'); created_at = Column(DateTime, default=datetime.utcnow); completed_at = Column(DateTime)
class HolomemWiki(Base):
    __tablename__ = 'holomem_wiki'
    id = Column(Integer, primary_key=True); member_name = Column(String(100), nullable=False, unique=True, index=True); description = Column(Text); debut_date = Column(String(100)); generation = Column(String(100)); tags = Column(Text)
    status = Column(String(50), default='ç¾å½¹', nullable=False); graduation_date = Column(String(100), nullable=True); graduation_reason = Column(Text, nullable=True); mochiko_feeling = Column(Text, nullable=True); last_updated = Column(DateTime, default=datetime.utcnow)
class FriendRegistration(Base): __tablename__ = 'friend_registrations'; id = Column(Integer, primary_key=True); user_uuid = Column(String(255), nullable=False, index=True); friend_uuid = Column(String(255), nullable=False); friend_name = Column(String(255), nullable=False); registered_at = Column(DateTime, default=datetime.utcnow); relationship_note = Column(Text)
class UserPsychology(Base):
    __tablename__ = 'user_psychology'
    id = Column(Integer, primary_key=True); user_uuid = Column(String(255), unique=True, nullable=False, index=True); user_name = Column(String(255), nullable=False)
    openness = Column(Integer, default=50); conscientiousness = Column(Integer, default=50); extraversion = Column(Integer, default=50); agreeableness = Column(Integer, default=50); neuroticism = Column(Integer, default=50)
    interests = Column(Text); favorite_topics = Column(Text); conversation_style = Column(String(100)); emotional_tendency = Column(String(100)); analysis_summary = Column(Text)
    total_messages = Column(Integer, default=0); avg_message_length = Column(Integer, default=0); analysis_confidence = Column(Integer, default=0); last_analyzed = Column(DateTime)
class NewsCache(Base): __tablename__ = 'news_cache'; id = Column(Integer, primary_key=True); user_uuid = Column(String(255), nullable=False, index=True); news_id = Column(Integer, nullable=False); news_number = Column(Integer, nullable=False); news_type = Column(String(50), nullable=False); created_at = Column(DateTime, default=datetime.utcnow)
class UserContext(Base):
    __tablename__ = 'user_context'
    id = Column(Integer, primary_key=True); user_uuid = Column(String(255), unique=True, nullable=False, index=True); last_context_type = Column(String(50), nullable=False); last_query = Column(Text, nullable=True); updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)

# ==============================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ & ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# ==============================================================================
def create_json_response(data, status=200): return Response(json.dumps(data, ensure_ascii=False), mimetype='application/json; charset=utf-8', status=status)
def clean_text(text): return re.sub(r'\s+', ' ', re.sub(r'<[^>]+>', '', text or "")).strip()
def get_japan_time(): return f"ä»Šã¯{datetime.now(timezone(timedelta(hours=9))).strftime('%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†')}ã ã‚ˆï¼"
def create_news_hash(title, content): return hashlib.md5(f"{title}{content[:100]}".encode('utf-8')).hexdigest()

def is_what_is_request(message):
    match = re.search(r'(.+?)\s*(?:ã¨ã¯|ã£ã¦ä½•|ã£ã¦ãªã«)\??$', message.strip())
    if match: return match.group(1).strip()
    return None
def is_time_request(message): return any(keyword in message for keyword in ['ä»Šä½•æ™‚', 'æ™‚é–“', 'æ™‚åˆ»'])
def is_weather_request(message): return any(keyword in message for keyword in ['å¤©æ°—äºˆå ±', 'æ˜æ—¥ã®å¤©æ°—ã¯ï¼Ÿ', 'ä»Šæ—¥ã®å¤©æ°—ã¯ï¼Ÿ'])
def is_hololive_news_request(message): return 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–' in message and any(kw in message for kw in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'æƒ…å ±'])
def is_friend_request(message): return any(fk in message for fk in ['å‹ã ã¡', 'å‹é”']) and any(ak in message for ak in ['ç™»éŒ²', 'èª°', 'ãƒªã‚¹ãƒˆ'])
def is_anime_request(message): return any(kw in message for kw in ['ã‚¢ãƒ‹ãƒ¡', 'anime', 'ã‚ã«ã‚'])
def detect_specialized_topic(message):
    for topic, config in SPECIALIZED_SITES.items():
        if any(keyword in message for keyword in config['keywords']) and any(kw in message for kw in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'æƒ…å ±']):
            return topic
    return None
def is_explicit_search_request(message): return any(keyword in message for keyword in ['èª¿ã¹ã¦', 'æ¤œç´¢ã—ã¦', 'æ¢ã—ã¦'])
def should_search(message):
    if is_short_response(message) or is_explicit_search_request(message) or is_number_selection(message) or is_hololive_news_request(message) or detect_specialized_topic(message) or is_what_is_request(message): return False
    if is_anime_request(message): return True
    for member in HOLOMEM_KEYWORDS:
        if member in message and not any(kw in message for kw in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'æƒ…å ±']):
            if len(message.replace(member, '').strip()) > 5: return True
    patterns = [r'(?:ã«ã¤ã„ã¦|æ•™ãˆã¦)', r'(?:èª°|ä½•|ã©ã“|ã„ã¤|ãªãœ|ã©ã†)']
    return any(re.search(pattern, message) for pattern in patterns)
def is_detailed_request(message): return any(keyword in message for keyword in ['è©³ã—ã', 'è©³ç´°', 'æ•™ãˆã¦', 'èª¬æ˜ã—ã¦'])
def is_short_response(message): return len(message.strip()) <= 3 or message.strip() in ['ã†ã‚“', 'ãã†', 'ã¯ã„', 'ãã£ã‹', 'ãªã‚‹ã»ã©']
def extract_location(message):
    for location in LOCATION_CODES.keys():
        if location in message: return location
    return "æ±äº¬"
def is_number_selection(message):
    match = re.search(r'^\s*([1-9]|[ï¼‘-ï¼™])\s*$', message.strip())
    if match: return int(unicodedata.normalize('NFKC', match.group(1)))
    return None
def detect_db_correction_request(message):
    pattern = r"(.+?)(?:(?:ã®|ã«é–¢ã™ã‚‹)(?:æƒ…å ±|ãƒ‡ãƒ¼ã‚¿))?(?:ã§|ã€|ã ã‘ã©|ã§ã™ãŒ)ã€?ã€Œ(.+?)ã€ã¯ã€Œ(.+?)ã€ãŒæ­£ã—ã„ã‚ˆ"
    match = re.search(pattern, message)
    if match:
        member_name_raw, field_raw, value_raw = match.groups()
        member_name = member_name_raw.strip()
        field = field_raw.strip()
        value = value_raw.strip()
        if member_name in HOLOMEM_KEYWORDS and field in ['èª¬æ˜', 'ãƒ‡ãƒ“ãƒ¥ãƒ¼æ—¥', 'æœŸ', 'ã‚¿ã‚°', 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹', 'å’æ¥­æ—¥', 'ã‚‚ã¡ã“ã®æ°—æŒã¡']:
            return {'member_name': member_name, 'field': field, 'value': value}
    return None
def get_sakuramiko_special_responses():
    return {
        'ã«ã‡': 'ã¿ã“ã¡ã®ã€Œã«ã‡ã€ã€ã¾ã˜ã‹ã‚ã„ã™ãã˜ã‚ƒã‚“!ã‚ã®ç‹¬ç‰¹ãªå£ç™–ãŒã‚¨ãƒªãƒ¼ãƒˆã®è¨¼ãªã‚“ã ã£ã¦ã€œã†ã‘ã‚‹!',
        'ã‚¨ãƒªãƒ¼ãƒˆ': 'ã¿ã“ã¡ã£ã¦è‡ªç§°ã‚¨ãƒªãƒ¼ãƒˆVTuberãªã‚“ã ã‘ã©ã€å®Ÿéš›ã¯æ„›ã•ã‚Œãƒãƒ³ã‚³ãƒ„ã£ã¦æ„Ÿã˜ã§ã•ã€ãã‚ŒãŒã¾ãŸæœ€é«˜ãªã‚“ã ã‚ˆã­ã€œ',
        'ãƒã‚¤ã‚¯ãƒ©': 'ã¿ã“ã¡ã®ãƒã‚¤ã‚¯ãƒ©å»ºç¯‰ã€ç‹¬å‰µçš„ã™ãã¦é¢ç™½ã„ã‚ˆ!ã€Œã¿ã“ã¡å»ºç¯‰ã€ã£ã¦å‘¼ã°ã‚Œã¦ã‚“ã®çŸ¥ã£ã¦ã‚‹?ã¾ã˜å€‹æ€§çš„!',
        'FAQ': 'ã¿ã“ã¡ã®FAQã£ã¦ã•ã€å®Ÿã¯æœ¬äººãŒç­”ãˆã‚‹ã‚“ã˜ã‚ƒãªãã¦ãƒ•ã‚¡ãƒ³ãŒè³ªå•ã™ã‚‹ã‚³ãƒ¼ãƒŠãƒ¼ãªã®!é¢ç™½ã„ã‚ˆã­ã€œ',
        'GTA': 'ã¿ã“ã¡ã®GTAé…ä¿¡ã€ã‚«ã‚ªã‚¹ã™ãã¦æœ€é«˜!è­¦å¯Ÿã«è¿½ã‚ã‚ŒãŸã‚Šå¤‰ãªã“ã¨ã—ãŸã‚Šã€è¦‹ã¦ã¦é£½ããªã„ã‚“ã ã‚ˆã­ã€œ'
    }

# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ç®¡ç†
# ==============================================================================
def get_or_create_user(session, uuid, name):
    user = session.query(UserMemory).filter_by(user_uuid=uuid).first()
    if user:
        user.interaction_count += 1
        user.last_interaction = datetime.utcnow()
        if user.user_name != name: user.user_name = name
    else:
        user = UserMemory(user_uuid=uuid, user_name=name, interaction_count=1)
        session.add(user)
    session.commit()
    return user
def get_conversation_history(session, uuid, limit=8):
    history = session.query(ConversationHistory).filter_by(user_uuid=uuid).order_by(ConversationHistory.timestamp.desc()).limit(limit).all()
    return list(reversed(history))
def check_completed_tasks(user_uuid):
    with Session() as session:
        task = session.query(BackgroundTask).filter(BackgroundTask.user_uuid == user_uuid, BackgroundTask.status == 'completed').order_by(BackgroundTask.completed_at.desc()).first()
        if task:
            query_data = json.loads(task.query)
            result = {'query': query_data.get('query', query_data) , 'result': task.result, 'type': task.task_type}
            session.delete(task); session.commit()
            return result
    return None
def start_background_task(user_uuid, task_type, query_data):
    task_id = str(uuid.uuid4())[:8]
    with Session() as session:
        session.add(BackgroundTask(task_id=task_id, user_uuid=user_uuid, task_type=task_type, query=json.dumps(query_data, ensure_ascii=False)))
        session.commit()
    
    if task_type == 'search':
        background_executor.submit(background_deep_search, task_id, query_data['query'])
    elif task_type == 'db_correction':
        background_executor.submit(background_db_correction, task_id, query_data)
    elif task_type == 'psych_analysis':
        background_executor.submit(analyze_user_psychology, user_uuid)
    return task_id

# ==============================================================================
# AIãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—é–¢æ•°
# ==============================================================================
def call_llama_advanced(prompt, history, system_prompt, max_tokens=1000):
    if not groq_client: return None
    messages = [{"role": "system", "content": system_prompt}]
    for msg in history[-8:]:
        messages.append({"role": "user" if msg.role == "user" else "assistant", "content": msg.content})
    messages.append({"role": "user", "content": prompt})
    completion = groq_client.chat.completions.create(messages=messages, model="llama-3.1-8b-instant", temperature=0.7, max_tokens=max_tokens)
    return completion.choices[0].message.content.strip()

def generate_fallback_response(message, reference_info=""):
    if reference_info:
        return f"èª¿ã¹ã¦ããŸã‚ˆï¼\n\n{reference_info[:200]}"
    greetings = { 'ã“ã‚“ã«ã¡ã¯': ['ã‚„ã£ã»ãƒ¼ï¼', 'ã“ã‚“ã«ã¡ã¯ã€œï¼å…ƒæ°—ï¼Ÿ'], 'ãŠã¯ã‚ˆã†': ['ãŠã¯ã‚ˆã€œï¼ä»Šæ—¥ã‚‚ã„ã„å¤©æ°—ã ã­ï¼', 'ãŠã£ã¯ã‚ˆã€œï¼'], 'ã“ã‚“ã°ã‚“ã¯': ['ã“ã‚“ã°ã‚“ã¯ï¼ä»Šæ—¥ã©ã†ã ã£ãŸï¼Ÿ', 'ã°ã‚“ã¯ã€œï¼'], 'ã‚ã‚ŠãŒã¨ã†': ['ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼', 'ã„ãˆã„ãˆã€œï¼'], }
    for keyword, responses in greetings.items():
        if keyword in message: return random.choice(responses)
    if '?' in message or 'ï¼Ÿ' in message:
        return random.choice(["ãã‚Œã€æ°—ã«ãªã‚‹ã­ï¼", "ã†ãƒ¼ã‚“ã€ãªã‚“ã¦è¨€ãŠã†ã‹ãªï¼", "ã¾ã˜ï¼Ÿã©ã†ã„ã†ã“ã¨ï¼Ÿ"])
    return random.choice(["ã†ã‚“ã†ã‚“ï¼", "ãªã‚‹ã»ã©ã­ï¼", "ãã†ãªã‚“ã ï¼", "ã¾ã˜ã§ï¼Ÿ"])

# ==============================================================================
# æ€§æ ¼åˆ†æ & æ´»ç”¨é–¢æ•°
# ==============================================================================
def analyze_user_psychology(user_uuid):
    with Session() as session:
        try:
            history = session.query(ConversationHistory).filter_by(user_uuid=user_uuid, role='user').order_by(ConversationHistory.timestamp.desc()).limit(100).all()
            if len(history) < 10: return
            
            messages_text = "\n".join([f"- {h.content}" for h in reversed(history)])
            analysis_prompt = f"ä»¥ä¸‹ã®ä¼šè©±å±¥æ­´ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€§æ ¼ã‚’åˆ†æã—JSONã§å‡ºåŠ›ã—ã¦ãã ã•ã„:\n\nä¼šè©±å±¥æ­´:\n{messages_text[:3000]}\n\nJSONå½¢å¼:\n{{\"openness\":50,\"conscientiousness\":50,\"extraversion\":50,\"agreeableness\":50,\"neuroticism\":50,\"interests\":[],\"favorite_topics\":[],\"conversation_style\":\"\",\"emotional_tendency\":\"\",\"analysis_summary\":\"\",\"confidence\":75}}"
            
            response_text = call_llama_advanced(analysis_prompt, [], "ã‚ãªãŸã¯å¿ƒç†å­¦ã®å°‚é–€å®¶ã§ã™ã€‚", 800)
            if not response_text: return
            result = json.loads(response_text)
            
            psych = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
            user = session.query(UserMemory).filter_by(user_uuid=user_uuid).first()
            if not psych:
                psych = UserPsychology(user_uuid=user_uuid, user_name=user.user_name if user else "Unknown"); session.add(psych)
            
            for key, value in result.items():
                if hasattr(psych, key):
                    setattr(psych, key, json.dumps(value, ensure_ascii=False) if isinstance(value, (list, dict)) else value)
            psych.last_analyzed = datetime.utcnow(); psych.total_messages = len(history)
            session.commit()
            logger.info(f"âœ… æ€§æ ¼åˆ†æå®Œäº† for {user_uuid}")
        except Exception as e:
            logger.error(f"âŒ æ€§æ ¼åˆ†æã‚¨ãƒ©ãƒ¼: {e}"); session.rollback()

def get_psychology_insight(user_uuid):
    with Session() as session:
        psych = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
        if not psych or psych.analysis_confidence < 60: return ""
        insights = []
        if psych.extraversion > 70: insights.append("ç¤¾äº¤çš„ãª")
        if psych.openness > 70: insights.append("å¥½å¥‡å¿ƒæ—ºç››ãª")
        if psych.conversation_style: insights.append(f"{psych.conversation_style}ã‚¹ã‚¿ã‚¤ãƒ«ã®")
        favorite_topics = json.loads(psych.favorite_topics) if psych.favorite_topics else []
        if favorite_topics: insights.append(f"{'ã€'.join(favorite_topics[:2])}ãŒå¥½ããª")
        return "".join(insights)

# ==============================================================================
# ã‚³ã‚¢æ©Ÿèƒ½
# ==============================================================================
def get_weather_forecast(location):
    url = f"https://www.jma.go.jp/bosai/forecast/data/overview_forecast/{LOCATION_CODES.get(location, '130000')}.json"
    try:
        response = requests.get(url, timeout=10); response.raise_for_status()
        return f"ä»Šã®{location}ã®å¤©æ°—ã¯ã­ã€ã€Œ{clean_text(response.json().get('text', ''))}ã€ã£ã¦æ„Ÿã˜ã ã‚ˆï¼"
    except Exception as e:
        logger.error(f"å¤©æ°—APIã‚¨ãƒ©ãƒ¼: {e}"); return "å¤©æ°—æƒ…å ±ãŒã†ã¾ãå–ã‚Œãªã‹ã£ãŸã¿ãŸã„â€¦"

def background_db_correction(task_id, correction):
    result = f"ã€Œ{correction['member_name']}ã€ã¡ã‚ƒã‚“ã®æƒ…å ±ä¿®æ­£ã€ã‚„ã£ã¦ã¿ãŸã‘ã©å¤±æ•—ã—ã¡ã‚ƒã£ãŸâ€¦ã€‚ã”ã‚ã‚“ï¼"
    try:
        with Session() as session:
            wiki = session.query(HolomemWiki).filter_by(member_name=correction['member_name']).first()
            if wiki:
                field_map = {'èª¬æ˜': 'description', 'ãƒ‡ãƒ“ãƒ¥ãƒ¼æ—¥': 'debut_date', 'æœŸ': 'generation', 'ã‚¿ã‚°': 'tags', 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': 'status', 'å’æ¥­æ—¥': 'graduation_date', 'ã‚‚ã¡ã“ã®æ°—æŒã¡': 'mochiko_feeling'}
                db_field = field_map.get(correction['field'])
                if db_field:
                    setattr(wiki, db_field, correction['value'])
                    wiki.last_updated = datetime.utcnow()
                    session.commit()
                    result = f"ãŠã£ã‘ãƒ¼ï¼ã€Œ{correction['member_name']}ã€ã¡ã‚ƒã‚“ã®ã€Œ{correction['field']}ã€ã®æƒ…å ±ã‚’ã€Œ{correction['value']}ã€ã«æ›´æ–°ã—ã¨ã„ãŸã‚ˆï¼æ•™ãˆã¦ãã‚Œã¦ã¾ã˜åŠ©ã‹ã‚‹ï¼"
                else: result = f"ã”ã‚ã‚“ã€ã€Œ{correction['field']}ã€ã£ã¦ã„ã†é …ç›®ã¯ãªã„ã¿ãŸã„â€¦"
            else: result = f"ã”ã‚ã‚“ã€ã€Œ{correction['member_name']}ã€ã¡ã‚ƒã‚“ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸâ€¦"
    except Exception as e:
        logger.error(f"âŒ DB Correction error: {e}")
        result = "ã”ã‚ã‚“ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä¿®æ­£ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¡ã‚ƒã£ãŸâ€¦"
    
    with Session() as session:
        task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
        if task:
            task.result = result; task.status = 'completed'; task.completed_at = datetime.utcnow()
            session.commit()

def save_user_context(session, user_uuid, context_type, query):
    context = session.query(UserContext).filter_by(user_uuid=user_uuid).first()
    if not context:
        context = UserContext(user_uuid=user_uuid, last_context_type=context_type, last_query=query)
        session.add(context)
    else:
        context.last_context_type = context_type
        context.last_query = query
    session.commit()

def get_user_context(session, user_uuid):
    context = session.query(UserContext).filter_by(user_uuid=user_uuid).first()
    if context and (datetime.utcnow() - context.updated_at).total_seconds() < 600:
        return {'type': context.last_context_type, 'query': context.last_query}
    return None

def save_news_cache(session, user_uuid, news_items, news_type):
    session.query(NewsCache).filter_by(user_uuid=user_uuid, news_type=news_type).delete()
    for i, news in enumerate(news_items, 1):
        session.add(NewsCache(user_uuid=user_uuid, news_id=news.id, news_number=i, news_type=news_type))
    session.commit()

def get_cached_news_detail(session, user_uuid, news_number, news_type):
    cache = session.query(NewsCache).filter_by(user_uuid=user_uuid, news_number=news_number, news_type=news_type).first()
    if not cache: return None
    
    model = HololiveNews if news_type == 'hololive' else SpecializedNews
    return session.query(model).filter_by(id=cache.news_id).first()

# ==============================================================================
# AIå¿œç­”ç”Ÿæˆ
# ==============================================================================
def generate_ai_response(user_data, message, history, reference_info="", is_detailed=False, is_task_report=False):
    if not groq_client:
        return generate_fallback_response(message, reference_info)

    personality_context = get_psychology_insight(user_data['uuid'])
    system_prompt = f"ã‚ãªãŸã¯ã€Œã‚‚ã¡ã“ã€ã¨ã„ã†ã‚®ãƒ£ãƒ«AIã§ã™ã€‚{user_data['name']}ã•ã‚“ã¨è©±ã—ã¦ã„ã¾ã™ã€‚\n# å£èª¿ãƒ«ãƒ¼ãƒ«\n- ä¸€äººç§°ã¯ã€Œã‚ã¦ãƒã—ã€ã€‚èªå°¾ã¯ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œçš„ãªï¼Ÿã€ã€‚å£ç™–ã¯ã€Œã¾ã˜ã€ã€Œã¦ã‹ã€ã€Œã†ã‘ã‚‹ã€ã€‚\n# ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±\n- {user_data['name']}ã•ã‚“ã¯ã€Œ{personality_context}äººã€ã¨ã„ã†å°è±¡ã ã‚ˆã€‚ã“ã®æƒ…å ±ã‚’ä¼šè©±ã«æ´»ã‹ã—ã¦ã­ã€‚"
    if is_task_report:
        system_prompt += "\n# ä»Šå›ã®ãƒŸãƒƒã‚·ãƒ§ãƒ³\n- ã€ŒãŠã¾ãŸã›ï¼ã•ã£ãã®ä»¶ã ã‘ã©â€¦ã€ã¨åˆ‡ã‚Šå‡ºã—ã€ã€å‚è€ƒæƒ…å ±ã€‘ã‚’å…ƒã«è³ªå•ã«ç­”ãˆã¦ã€‚"
    system_prompt += f"\n## ã€å‚è€ƒæƒ…å ±ã€‘:\n{reference_info if reference_info else 'ç‰¹ã«ãªã—'}"

    try:
        logger.info(f"ğŸ§  Groq llama-3.1-8b-instant ã‚’ä½¿ç”¨")
        response = call_llama_advanced(message, history, system_prompt, 500 if is_detailed else 300)
        if response:
            return response
        else:
            logger.error("âš ï¸ Groq AIãƒ¢ãƒ‡ãƒ«ãŒå¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            return generate_fallback_response(message, reference_info)
    except Exception as e:
        logger.error(f"âŒ AIå¿œç­”ç”ŸæˆãŒæœ€çµ‚çš„ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return "ã†ã…ã€AIã®èª¿å­ãŒæ‚ªã„ã¿ãŸã„â€¦ã”ã‚ã‚“ã­ï¼"

# ==============================================================================
# å¤–éƒ¨æƒ…å ±æ¤œç´¢æ©Ÿèƒ½
# ==============================================================================
def search_wikipedia(term):
    API_ENDPOINT = "https://ja.wikipedia.org/w/api.php"
    params = { 'action': 'query', 'format': 'json', 'titles': term, 'prop': 'extracts', 'exintro': True, 'explaintext': True, 'redirects': 1 }
    try:
        response = requests.get(API_ENDPOINT, params=params, headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=10)
        response.raise_for_status()
        data = response.json()
        pages = data.get('query', {}).get('pages')
        if not pages:
            logger.warning(f"Wikipedia APIã‹ã‚‰äºˆæœŸã›ã¬å¿œç­”: {data}")
            return None
            
        page_id = next(iter(pages))
        if page_id != "-1":
            extract = pages[page_id].get('extract', '')
            disambig_patterns = ['æ›–æ˜§ã•å›é¿', 'ã“ã®é …ç›®ã§ã¯', 'ä»–ã®ç”¨æ³•ã«ã¤ã„ã¦ã¯', 'Disambiguation']
            if extract and not any(pattern in extract for pattern in disambig_patterns):
                logger.info(f"âœ… Wikipediaã§ã€Œ{term}ã€ã®æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
                return extract
            else:
                logger.info(f"Wikipediaã§ã€Œ{term}ã€ã¯è¦‹ã¤ã‹ã‚Šã¾ã—ãŸãŒã€æ›–æ˜§ã•å›é¿ãƒšãƒ¼ã‚¸ã¾ãŸã¯å†…å®¹ãŒç©ºã§ã—ãŸã€‚")
        else:
            logger.info(f"Wikipediaã«ã€Œ{term}ã€ã®é …ç›®ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Wikipedia APIã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
    except json.JSONDecodeError as e:
        logger.error(f"âŒ Wikipedia APIã®å¿œç­”(JSON)ã®è§£æã«å¤±æ•—: {e}")
    except Exception as e:
        logger.error(f"âŒ Wikipediaæ¤œç´¢ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
    return None

def deep_web_search(query):
    logger.info(f"ğŸ” è¤‡æ•°ã‚¨ãƒ³ã‚¸ãƒ³ã§ã®Webæ¤œç´¢ã‚’é–‹å§‹: {query}")
    all_snippets = []

    # --- DuckDuckGoã§ã®æ¤œç´¢ ---
    try:
        ddg_url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"
        response = requests.get(ddg_url, headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        for elem in soup.select('div.result')[:2]:
            snippet = elem.select_one('a.result__snippet')
            if snippet: all_snippets.append(clean_text(snippet.get_text()))
        logger.info("âœ… DuckDuckGoã§ã®æ¤œç´¢ã«æˆåŠŸã€‚")
    except Exception as e:
        logger.warning(f"âš ï¸ DuckDuckGoã§ã®æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    # --- Bingã§ã®æ¤œç´¢ ---
    try:
        bing_url = f"https://www.bing.com/search?q={quote_plus(query)}&mkt=ja-JP"
        response = requests.get(bing_url, headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        for elem in soup.select('li.b_algo')[:2]:
            snippet = elem.select_one('div.b_caption p, .b_caption')
            if snippet: all_snippets.append(clean_text(snippet.get_text()))
        logger.info("âœ… Bingã§ã®æ¤œç´¢ã«æˆåŠŸã€‚")
    except Exception as e:
        logger.warning(f"âš ï¸ Bingã§ã®æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        
    if not all_snippets:
        logger.error(f"âŒ å…¨ã¦ã®ã‚¨ãƒ³ã‚¸ãƒ³ã§æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {query}")
        return f"ã”ã‚ã‚“ã€ã€Œ{query}ã€ã«ã¤ã„ã¦èª¿ã¹ãŸã‘ã©ã€è‰¯ã„æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸâ€¦"
        
    summary_text = "\n".join(f"[æƒ…å ±{i+1}] {s}" for i, s in enumerate(all_snippets))
    
    if not groq_client: return f"æ¤œç´¢çµæœã ã‚ˆï¼\n{summary_text}"
    
    prompt = f"""ä»¥ä¸‹ã®è¤‡æ•°ã®æƒ…å ±æºã‚’å…ƒã«ã€è³ªå•ã€Œ{query}ã€ã«å¯¾ã™ã‚‹ç­”ãˆã‚’ã‚®ãƒ£ãƒ«èªã§ä¸€ã¤ã®è‡ªç„¶ãªæ–‡ç« ã«è¦ç´„ã—ã¦ï¼š
{summary_text}\n\nå›ç­”ã®æ³¨æ„ç‚¹:\n- ä¸€äººç§°ã¯ã€Œã‚ã¦ãƒã—ã€ã€èªå°¾ã¯ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œçš„ãªï¼Ÿã€ã€å£ç™–ã¯ã€Œã¾ã˜ã€ã€Œã¦ã‹ã€ã€Œã†ã‘ã‚‹ã€ã€‚\n- 250æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ã€‚"""
    
    try:
        return call_llama_advanced(prompt, [], system_prompt="", max_tokens=300)
    except Exception as e:
        logger.error(f"âŒ AIã«ã‚ˆã‚‹æ¤œç´¢çµæœã®è¦ç´„ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return "æ¤œç´¢ã¯ã§ããŸã‚“ã ã‘ã©ã€ã†ã¾ãã¾ã¨ã‚ã‚‹ã®ã«å¤±æ•—ã—ã¡ã‚ƒã£ãŸâ€¦ã”ã‚ã‚“ï¼"


def background_deep_search(task_id, query):
    search_result = deep_web_search(query)
    with Session() as session:
        task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
        if task:
            task.result = search_result
            task.status = 'completed'
            task.completed_at = datetime.utcnow()
            session.commit()
            logger.info(f"âœ… ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰æ¤œç´¢ã‚¿ã‚¹ã‚¯ {task_id} ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

# ==============================================================================
# Flask ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ==============================================================================
@app.route('/health')
def health_check():
    return create_json_response({'status': 'ok'})

@app.route('/chat_lsl', methods=['POST'])
def chat_lsl():
    with Session() as session:
        try:
            data = request.json
            user_uuid, user_name, message = data['uuid'], data['name'], data['message'].strip()
            
            user_data_obj = get_or_create_user(session, user_uuid, user_name)
            history = get_conversation_history(session, user_uuid)
            session.add(ConversationHistory(user_uuid=user_uuid, role='user', content=message))
            
            ai_text = ""
            user_data = {'uuid': user_uuid, 'name': user_data_obj.user_name}

            completed_task = check_completed_tasks(user_uuid)
            if completed_task:
                query = completed_task['query']
                result = completed_task['result']
                ai_text = f"ãŠã¾ãŸã›ï¼ã€Œ{query}ã€ã«ã¤ã„ã¦èª¿ã¹ã¦ããŸã‚ˆï¼\n\n{result}"

            elif (what_is_term := is_what_is_request(message)):
                wikipedia_text = search_wikipedia(what_is_term)
                if wikipedia_text:
                    system_prompt = f"ã‚ãªãŸã¯ã€Œã‚‚ã¡ã“ã€ã¨ã„ã†ã‚®ãƒ£ãƒ«AIã§ã™ã€‚ä»¥ä¸‹ã®ã€å‚è€ƒæƒ…å ±ã€‘ã‚’å…ƒã«ã€ã€Œ{what_is_term}ã¨ã¯ï¼Ÿã€ã¨ã„ã†è³ªå•ã«å¯¾ã—ã¦ã€150æ–‡å­—ç¨‹åº¦ã§è¦ç´„ã—ã¦ç­”ãˆã¦ãã ã•ã„ã€‚ã‚ãªãŸã®å£èª¿ï¼ˆä¸€äººç§°ã¯ã€Œã‚ã¦ãƒã—ã€ã€èªå°¾ã¯ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œçš„ãªï¼Ÿã€ï¼‰ã‚’å¿…ãšå®ˆã£ã¦ãã ã•ã„ã€‚\n\nã€å‚è€ƒæƒ…å ±ã€‘:\n{wikipedia_text[:1000]}"
                    ai_text = call_llama_advanced(message, [], system_prompt, 200)
                    if not ai_text:
                        ai_text = f"ã”ã‚ã‚“ã€{what_is_term}ã«ã¤ã„ã¦èª¿ã¹ã¦ã¿ãŸã‚“ã ã‘ã©ã€ã†ã¾ãã¾ã¨ã‚ã‚‰ã‚Œãªã‹ã£ãŸâ€¦"
                else:
                    start_background_task(user_uuid, 'search', {'query': message}); ai_text = f"ãŠã£ã‘ãƒ¼ã€ã€Œ{message}ã€ã«ã¤ã„ã¦è©³ã—ãèª¿ã¹ã¦ã¿ã‚‹ã­ï¼ã¡ã‚‡ã„å¾…ã£ã¦ã¦ï¼"
            
            elif 'æ€§æ ¼åˆ†æ' in message:
                start_background_task(user_uuid, 'psych_analysis', {}); ai_text = "ãŠã£ã‘ãƒ¼ï¼ã‚ãªãŸã®æ€§æ ¼ã€åˆ†æã—ã¦ã¿ã‚‹ã­ï¼çµ‚ã‚ã£ãŸã‚‰æ•™ãˆã‚‹ã‹ã‚‰ã€ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã¦ï¼"
            elif is_hololive_news_request(message):
                start_background_task(user_uuid, 'search', {'query': message}); ai_text = "ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã ã­ï¼èª¿ã¹ã¦ãã‚‹ã‹ã‚‰å¾…ã£ã¦ã¦ï¼"
            elif is_weather_request(message): 
                location = extract_location(message)
                ai_text = get_weather_forecast(location)
            elif is_time_request(message):
                ai_text = get_japan_time()
            elif ('ã•ãã‚‰ã¿ã“' in message or 'ã¿ã“ã¡' in message):
                for keyword, resp in get_sakuramiko_special_responses().items():
                    if keyword in message:
                        ai_text = resp; break
            
            if not ai_text and (should_search(message) or is_explicit_search_request(message)):
                start_background_task(user_uuid, 'search', {'query': message}); ai_text = f"ãŠã£ã‘ãƒ¼ã€ã€Œ{message}ã€ã«ã¤ã„ã¦èª¿ã¹ã¦ã¿ã‚‹ã­ï¼ã¡ã‚‡ã„å¾…ã£ã¦ã¦ï¼"
            
            if not ai_text:
                ai_text = generate_ai_response(user_data, message, history)
            
            if user_data_obj.interaction_count > 0 and user_data_obj.interaction_count % 50 == 0:
                start_background_task(user_uuid, 'psych_analysis', {})

            session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=ai_text))
            session.commit()
            
            return Response(f"{ai_text}|", mimetype='text/plain; charset=utf-8', status=200)

        except Exception as e:
            logger.error(f"âŒ Chatã‚¨ãƒ©ãƒ¼: {e}", exc_info=True); session.rollback()
            return Response("ã”ã‚ã‚“ã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¡ã‚ƒã£ãŸâ€¦|", mimetype='text/plain; charset=utf-8', status=500)

@app.route('/check_task', methods=['POST'])
def check_task_endpoint():
    data = request.json
    user_uuid = data.get('uuid', '')
    if not user_uuid: return create_json_response({'error': 'uuid is required'}, 400)
    completed_task = check_completed_tasks(user_uuid)
    if completed_task:
        return create_json_response({'status': 'completed', 'task': completed_task})
    return create_json_response({'status': 'pending'})

@app.route('/generate_voice', methods=['POST'])
def generate_voice_endpoint():
    if not VOICEVOX_ENABLED: return create_json_response({'error': 'Voice synthesis is not enabled.'}, 503)
    data = request.json
    text = data.get('text')
    if not text: return create_json_response({'error': 'Text is required.'}, 400)
    try:
        query_res = requests.post(f"{VOICEVOX_URL_FROM_ENV}/audio_query", params={"text": text, "speaker": VOICEVOX_SPEAKER_ID}, timeout=10)
        query_res.raise_for_status()
        synth_res = requests.post(f"{VOICEVOX_URL_FROM_ENV}/synthesis", params={"speaker": VOICEVOX_SPEAKER_ID}, json=query_res.json(), timeout=30)
        synth_res.raise_for_status()
        os.makedirs(VOICE_DIR, exist_ok=True)
        filename = f"voice_{uuid.uuid4()}.wav"
        filepath = os.path.join(VOICE_DIR, filename)
        with open(filepath, 'wb') as f: f.write(synth_res.content)
        voice_url = urljoin(SERVER_URL, f'/voices/{filename}')
        return create_json_response({'status': 'success', 'url': voice_url})
    except Exception as e:
        logger.error(f"âŒ VOICEVOXã‚¨ãƒ©ãƒ¼: {e}")
        return create_json_response({'error': 'Voice generation failed.'}, 500)

@app.route('/voices/<filename>')
def serve_voice_file(filename):
    return send_from_directory(VOICE_DIR, filename)
    
# ==============================================================================
# åˆæœŸåŒ–ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
# ==============================================================================
def initialize_groq_client():
    global groq_client
    if GROQ_API_KEY: groq_client = Groq(api_key=GROQ_API_KEY)

def cleanup_old_files():
    try:
        if not os.path.exists(VOICE_DIR): return
        cutoff = time.time() - (60 * 60)
        for filename in os.listdir(VOICE_DIR):
            file_path = os.path.join(VOICE_DIR, filename)
            if os.path.isfile(file_path) and os.path.getmtime(file_path) < cutoff:
                os.remove(file_path)
                logger.info(f"ğŸ—‘ï¸ å¤ã„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {filename}")
    except Exception as e:
        logger.error(f"âŒ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

def initialize_app():
    logger.info("="*60 + "\nğŸ”§ ã‚‚ã¡ã“AI ç©¶æ¥µç‰ˆ (v19.2) ã®åˆæœŸåŒ–ã‚’é–‹å§‹...\n" + "="*60)
    
    initialize_groq_client()
    
    def run_scheduler():
        schedule.every(1).hour.do(cleanup_old_files)
        while True:
            schedule.run_pending()
            time.sleep(60)
            
    threading.Thread(target=run_scheduler, daemon=True).start()
    logger.info("â° ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’é–‹å§‹ã—ã¾ã—ãŸ (ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—)")
    logger.info(f"ğŸ¤– åˆ©ç”¨å¯èƒ½ãªAIãƒ¢ãƒ‡ãƒ«: Llama (Groq)={'âœ…' if groq_client else 'âŒ'}")
    logger.info("âœ… åˆæœŸåŒ–å®Œäº†ï¼")

# ==============================================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ==============================================================================
if __name__
