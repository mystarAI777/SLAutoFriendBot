import os
import requests
import logging
import time
import threading
import json
import re
import sqlite3
import random
import uuid
import hashlib
from datetime import datetime, timedelta, timezone
from typing import Union, Dict, Any, List
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Index, Text, Boolean, inspect, text
from sqlalchemy.orm import declarative_base, sessionmaker
from urllib.parse import quote_plus, urljoin
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import asyncio
from threading import Lock
import schedule

# --- åŸºæœ¬è¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
VOICE_DIR = '/tmp/voices'; SERVER_URL = "https://slautofriendbot.onrender.com"
background_executor = ThreadPoolExecutor(max_workers=5)

# --- ç§˜å¯†æƒ…å ±/ç’°å¢ƒå¤‰æ•° èª­ã¿è¾¼ã¿ ---
def get_secret(name: str) -> Union[str, None]:
    # ã“ã®ç’°å¢ƒã§ã¯ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½¿ãˆãªã„ãŸã‚ã€ç’°å¢ƒå¤‰æ•°ã®ã¿ã‹ã‚‰èª­ã¿è¾¼ã‚€
    env_value = os.environ.get(name);
    if env_value: return env_value
    return None

# â˜…â˜…â˜…ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã®ãŸã‚ã®ãƒ€ãƒŸãƒ¼è¨­å®šâ˜…â˜…â˜…
# å®Ÿè¡Œã«å¿…è¦ãªç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€ãƒ€ãƒŸãƒ¼ã®å€¤ã‚’è¨­å®šã™ã‚‹
DATABASE_URL = get_secret('DATABASE_URL') or 'sqlite:///./test.db'
GROQ_API_KEY = get_secret('GROQ_API_KEY') or 'DUMMY_GROQ_KEY' # å®Ÿéš›ã®APIã‚­ãƒ¼ã§ã¯ãªã„
VOICEVOX_URL_FROM_ENV = get_secret('VOICEVOX_URL')

# --- ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ– & å¿…é ˆè¨­å®šãƒã‚§ãƒƒã‚¯ ---
try:
    from groq import Groq
    # ãƒ€ãƒŸãƒ¼ã‚­ãƒ¼ã®å ´åˆã€Groqã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¯åˆæœŸåŒ–ã—ãªã„
    if GROQ_API_KEY != 'DUMMY_GROQ_KEY':
        groq_client = Groq(api_key=GROQ_API_KEY)
    else:
        groq_client = None
except Exception as e: groq_client = None

if not all([DATABASE_URL]): 
    logger.critical("FATAL: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
    sys.exit(1)
# groq_clientãŒNoneã§ã‚‚ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆãŒã§ãã‚‹ã‚ˆã†ã«ãƒã‚§ãƒƒã‚¯ã‚’ç·©å’Œ
if not groq_client:
    logger.warning("è­¦å‘Š: Groq APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€AIæ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ã€‚")

VOICEVOX_ENABLED = True

# --- Flask & ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ– ---
app = Flask(__name__); CORS(app); engine = create_engine(DATABASE_URL); Base = declarative_base()

# --- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« ---
class UserMemory(Base): __tablename__ = 'user_memories'; id = Column(Integer, primary_key=True); user_uuid = Column(String(255), unique=True, nullable=False); user_name = Column(String(255), nullable=False); interaction_count = Column(Integer, default=0); last_interaction = Column(DateTime, default=datetime.utcnow)
class ConversationHistory(Base): __tablename__ = 'conversation_history'; id = Column(Integer, primary_key=True, autoincrement=True); user_uuid = Column(String(255), nullable=False, index=True); role = Column(String(10), nullable=False); content = Column(Text, nullable=False); timestamp = Column(DateTime, default=datetime.utcnow, index=True)
class HololiveNews(Base): __tablename__ = 'hololive_news'; id = Column(Integer, primary_key=True); title = Column(String(500), nullable=False); content = Column(Text, nullable=False); url = Column(String(1000)); published_date = Column(DateTime, default=datetime.utcnow); created_at = Column(DateTime, default=datetime.utcnow, index=True); news_hash = Column(String(100), unique=True)
class BackgroundTask(Base): __tablename__ = 'background_tasks'; id = Column(Integer, primary_key=True); task_id = Column(String(255), unique=True, nullable=False); user_uuid = Column(String(255), nullable=False); task_type = Column(String(50), nullable=False); query = Column(Text, nullable=False); result = Column(Text); status = Column(String(20), default='pending'); created_at = Column(DateTime, default=datetime.utcnow, index=True); completed_at = Column(DateTime)
Base.metadata.create_all(engine); Session = sessionmaker(bind=engine)

def add_news_hash_column_if_not_exists(engine):
    try:
        inspector = inspect(engine)
        if 'news_hash' not in [col['name'] for col in inspector.get_columns('hololive_news')]:
            with engine.connect() as con:
                trans = con.begin()
                try: con.execute(text("ALTER TABLE hololive_news ADD COLUMN news_hash VARCHAR(100) UNIQUE;")); trans.commit()
                except: trans.rollback()
    except: pass
add_news_hash_column_if_not_exists(engine)

# --- å°‚é–€ã‚µã‚¤ãƒˆ & ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–è¨­å®š ---
SPECIALIZED_SITES = {'Blender': {'base_url': 'https://docs.blender.org/manual/ja/latest/', 'keywords': ['Blender', 'ãƒ–ãƒ¬ãƒ³ãƒ€ãƒ¼']},'CGãƒ‹ãƒ¥ãƒ¼ã‚¹': {'base_url': 'https://modelinghappy.com/', 'keywords': ['CGãƒ‹ãƒ¥ãƒ¼ã‚¹', '3DCG']},'è„³ç§‘å­¦ãƒ»å¿ƒç†å­¦': {'base_url': 'https://nazology.kusuguru.co.jp/', 'keywords': ['è„³ç§‘å­¦', 'å¿ƒç†å­¦']},'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•': {'base_url': 'https://community.secondlife.com/news/', 'keywords': ['ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•', 'Second Life', 'SL']}}
HOLOLIVE_NEWS_URL = "https://hololive.hololivepro.com/news"
HOLOMEM_KEYWORDS = ['ã¨ãã®ãã‚‰', 'ãƒ­ãƒœå­ã•ã‚“', 'ã•ãã‚‰ã¿ã“', 'æ˜Ÿè¡—ã™ã„ã›ã„', 'AZKi', 'å¤œç©ºãƒ¡ãƒ«', 'ã‚¢ã‚­ãƒ»ãƒ­ãƒ¼ã‚¼ãƒ³ã‚¿ãƒ¼ãƒ«', 'èµ¤äº•ã¯ã‚ã¨', 'ç™½ä¸Šãƒ•ãƒ–ã‚­', 'å¤è‰²ã¾ã¤ã‚Š', 'æ¹Šã‚ãã‚', 'ç´«å’²ã‚·ã‚ªãƒ³', 'ç™¾é¬¼ã‚ã‚„ã‚', 'ç™’æœˆã¡ã‚‡ã“', 'å¤§ç©ºã‚¹ãƒãƒ«', 'å¤§ç¥ãƒŸã‚ª', 'çŒ«åˆãŠã‹ã‚†', 'æˆŒç¥ã“ã‚ã­', 'å…ç”°ãºã“ã‚‰', 'ä¸çŸ¥ç«ãƒ•ãƒ¬ã‚¢', 'ç™½éŠ€ãƒã‚¨ãƒ«', 'å®é˜ãƒãƒªãƒ³', 'å¤©éŸ³ã‹ãªãŸ', 'è§’å·»ã‚ãŸã‚', 'å¸¸é—‡ãƒˆãƒ¯', 'å§«æ£®ãƒ«ãƒ¼ãƒŠ', 'é›ªèŠ±ãƒ©ãƒŸã‚£', 'æ¡ƒéˆ´ã­ã­', 'ç…ç™½ã¼ãŸã‚“', 'å°¾ä¸¸ãƒãƒ«ã‚«', 'ãƒ©ãƒ—ãƒ©ã‚¹ãƒ»ãƒ€ãƒ¼ã‚¯ãƒã‚¹', 'é·¹å¶ºãƒ«ã‚¤', 'åšè¡£ã“ã‚ˆã‚Š', 'æ²™èŠ±å‰ã‚¯ãƒ­ãƒ±', 'é¢¨çœŸã„ã‚ã¯', 'æ£®ã‚«ãƒªã‚ªãƒš', 'å°é³¥éŠã‚­ã‚¢ãƒ©', 'ä¸€ä¼Šé‚£å°“æ –', 'ãŒã†ã‚‹ãƒ»ãã‚‰', 'ãƒ¯ãƒˆã‚½ãƒ³ãƒ»ã‚¢ãƒ¡ãƒªã‚¢', 'IRyS', 'ã‚»ãƒ¬ã‚¹ãƒ»ãƒ•ã‚¡ã‚¦ãƒŠ', 'ã‚ªãƒ¼ãƒ­ãƒ»ã‚¯ãƒ­ãƒ‹ãƒ¼', 'ä¸ƒè©©ãƒ ãƒ¡ã‚¤', 'ãƒã‚³ã‚¹ãƒ»ãƒ™ãƒ¼ãƒ«ã‚º', 'ã‚·ã‚ªãƒªãƒ»ãƒãƒ´ã‚§ãƒ©', 'å¤çŸ³ãƒ“ã‚¸ãƒ¥ãƒ¼', 'ãƒãƒªãƒƒã‚µãƒ»ãƒ¬ã‚¤ãƒ´ãƒ³ã‚¯ãƒ­ãƒ•ãƒˆ', 'ãƒ•ãƒ¯ãƒ¯ãƒ»ã‚¢ãƒ“ã‚¹ã‚¬ãƒ¼ãƒ‰', 'ãƒ¢ã‚³ã‚³ãƒ»ã‚¢ãƒ“ã‚¹ã‚¬ãƒ¼ãƒ‰', 'ã‚¢ãƒ¦ãƒ³ãƒ€ãƒ»ãƒªã‚¹', 'ãƒ ãƒ¼ãƒŠãƒ»ãƒ›ã‚·ãƒãƒ´ã‚¡', 'ã‚¢ã‚¤ãƒ©ãƒ‹ãƒ»ã‚¤ã‚ªãƒ•ã‚£ãƒ•ãƒ†ã‚£ãƒ¼ãƒ³', 'ã‚¯ãƒ¬ã‚¤ã‚¸ãƒ¼ãƒ»ã‚ªãƒªãƒ¼', 'ã‚¢ãƒ¼ãƒ‹ãƒ£ãƒ»ãƒ¡ãƒ«ãƒ•ã‚£ãƒƒã‚µ', 'ãƒ‘ãƒ´ã‚©ãƒªã‚¢ãƒ»ãƒ¬ã‚¤ãƒ', 'ç«å¨é’', 'éŸ³ä¹ƒç€¬å¥', 'ä¸€æ¡è‰ã€…è¯', 'å„’çƒé¢¨äº­ã‚‰ã§ã‚“', 'è½Ÿã¯ã˜ã‚', 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–', 'ãƒ›ãƒ­ãƒ¡ãƒ³', 'hololive', 'YAGOO']

# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ & åˆ¤å®šé–¢æ•° ---
def clean_text(text: str) -> str: return re.sub(r'\s+', ' ', re.sub(r'<[^>]+>', '', text or "")).strip()
def get_japan_time() -> str: jst = timezone(timedelta(hours=9)); now = datetime.now(jst); return f"ä»Šã¯{now.year}å¹´{now.month}æœˆ{now.day}æ—¥ã®{now.hour}æ™‚{now.minute}åˆ†ã ã‚ˆï¼"
def create_news_hash(t, c) -> str: import hashlib; return hashlib.md5(f"{t}{c[:100]}".encode('utf-8')).hexdigest()
def is_time_request(m: str) -> bool: return any(k in m for k in ['ä»Šä½•æ™‚', 'æ™‚é–“', 'æ™‚åˆ»'])
def is_weather_request(m: str) -> bool: return any(k in m for k in ['å¤©æ°—', 'ã¦ã‚“ã', 'æ°—æ¸©'])
def is_hololive_request(m: str) -> bool: return any(k in m for k in HOLOMEM_KEYWORDS)
def is_recommendation_request(m: str) -> bool: return any(k in m for k in ['ãŠã™ã™ã‚', 'ã‚ªã‚¹ã‚¹ãƒ¡'])
def extract_recommendation_topic(m: str) -> Union[str, None]:
    topics = {'æ˜ ç”»': ['æ˜ ç”»'], 'éŸ³æ¥½': ['éŸ³æ¥½', 'æ›²'], 'ã‚¢ãƒ‹ãƒ¡': ['ã‚¢ãƒ‹ãƒ¡'], 'æœ¬': ['æœ¬', 'æ¼«ç”»'], 'ã‚²ãƒ¼ãƒ ': ['ã‚²ãƒ¼ãƒ ']}
    for topic, keywords in topics.items():
        if any(kw in m for kw in keywords): return topic
    return None
def detect_specialized_topic(m: str) -> Union[str, None]:
    for topic, config in SPECIALIZED_SITES.items():
        if any(keyword in m for keyword in config['keywords']): return topic
    return None
def is_detailed_request(m: str) -> bool:
    detailed_keywords = ['è©³ã—ã', 'è©³ç´°', 'ãã‚ã—ã', 'æ•™ãˆã¦', 'èª¬æ˜ã—ã¦', 'è§£èª¬ã—ã¦', 'ã©ã†ã„ã†', 'ãªãœ', 'ã©ã†ã—ã¦', 'ç†ç”±', 'åŸå› ', 'ã—ã£ã‹ã‚Š', 'ã¡ã‚ƒã‚“ã¨', 'ãã¡ã‚“ã¨', 'å…·ä½“çš„ã«']
    return any(keyword in m for keyword in detailed_keywords)
def should_search(m: str) -> bool:
    if is_hololive_request(m) or detect_specialized_topic(m) or is_recommendation_request(m): return True
    if any(re.search(p, m) for p in [r'(?:ã¨ã¯|ã«ã¤ã„ã¦|æ•™ãˆã¦)', r'(?:èª¿ã¹ã¦|æ¤œç´¢)']): return True
    if any(w in m for w in ['èª°', 'ä½•', 'ã©ã“', 'ã„ã¤', 'ãªãœ']): return True
    return False
def is_short_response(m: str) -> bool: return len(m.strip()) <= 3 or m.strip() in ['ã†ã‚“', 'ãã†', 'ã¯ã„', 'ãã£ã‹']

# --- å¤©æ°—äºˆå ± & ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾— ---
LOCATION_CODES = {"æ±äº¬": "130000", "å¤§é˜ª": "270000", "åå¤å±‹": "230000", "ç¦å²¡": "400000", "æœ­å¹Œ": "016000"}
def extract_location(m: str) -> str:
    for location in LOCATION_CODES.keys():
        if location in m: return location
    return "æ±äº¬"
def get_weather_forecast(location: str) -> str:
    area_code = LOCATION_CODES.get(location)
    if not area_code: return f"ã”ã‚ã‚“ã€ã€Œ{location}ã€ã®å¤©æ°—ã¯åˆ†ã‹ã‚‰ãªã„ã‚„â€¦"
    url = f"https://www.jma.go.jp/bosai/forecast/data/overview_forecast/{area_code}.json"
    try:
        response = requests.get(url, timeout=5); response.raise_for_status()
        return f"ä»Šã®{location}ã®å¤©æ°—ã¯ã­ã€ã€Œ{clean_text(response.json().get('text', ''))}ã€ã£ã¦æ„Ÿã˜ã ã‚ˆï¼"
    except Exception as e: logger.error(f"å¤©æ°—APIã‚¨ãƒ©ãƒ¼: {e}"); return "å¤©æ°—æƒ…å ±ãŒã†ã¾ãå–ã‚Œãªã‹ã£ãŸã¿ãŸã„â€¦"
def update_hololive_news_database():
    session = Session(); added_count = 0; logger.info("ğŸ“° ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®DBæ›´æ–°å‡¦ç†ã‚’é–‹å§‹...")
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}; response = requests.get(HOLOLIVE_NEWS_URL, headers=headers, timeout=10); response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        for article in soup.find_all('article', limit=5):
            if (title_elem := article.find(['h1', 'h2', 'h3'])) and (title := clean_text(title_elem.get_text())) and len(title) > 5:
                content = clean_text((article.find(['p', 'div'], class_=re.compile(r'(content|text)')) or title_elem).get_text())
                news_hash = create_news_hash(title, content)
                if not session.query(HololiveNews).filter_by(news_hash=news_hash).first():
                    session.add(HololiveNews(title=title, content=content[:300], news_hash=news_hash)); added_count += 1
        if added_count > 0: session.commit(); logger.info(f"âœ… DBæ›´æ–°å®Œäº†: {added_count}ä»¶è¿½åŠ ")
        else: logger.info("âœ… DBæ›´æ–°å®Œäº†: æ–°ç€ãªã—")
    except Exception as e: logger.error(f"âŒ DBæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); session.rollback()
    finally: session.close()

# --- Webæ¤œç´¢æ©Ÿèƒ½ ---
USER_AGENTS = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36']
def get_random_user_agent(): return random.choice(USER_AGENTS)
def scrape_major_search_engines(query: str, num_results: int) -> List[Dict[str, str]]:
    search_urls = [f"https://www.bing.com/search?q={quote_plus(query)}&mkt=ja-JP", f"https://search.yahoo.co.jp/search?p={quote_plus(query)}"]
    for url in search_urls:
        try:
            response = requests.get(url, headers={'User-Agent': get_random_user_agent()}, timeout=8); response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser'); results = []
            if 'bing.com' in url:
                for r in soup.find_all('li', class_='b_algo', limit=num_results):
                    if (t := r.find('h2')) and (s := r.find('div', class_='b_caption')):
                        results.append({'title': clean_text(t.get_text()), 'snippet': clean_text(s.get_text())})
            elif 'yahoo.co.jp' in url:
                for r in soup.find_all('div', class_='Algo', limit=num_results):
                    if (t := r.find('h3')) and (s := r.find('div', class_='compText')):
                        results.append({'title': clean_text(t.get_text()), 'snippet': clean_text(s.get_text())})
            if results: logger.info(f"âœ… {url.split('/')[2]}ã§ã®æ¤œç´¢æˆåŠŸ"); return results
        except Exception: continue
    return []
def deep_web_search(query: str, is_detailed: bool) -> Union[str, None]:
    logger.info(f"ãƒ‡ã‚£ãƒ¼ãƒ—Webæ¤œç´¢ã‚’é–‹å§‹ (è©³ç´°: {is_detailed})"); num_results = 3 if is_detailed else 2
    results = scrape_major_search_engines(query, num_results)
    if not results: return None
    summary_text = ""; _ = [summary_text := summary_text + f"[æƒ…å ±{i}] {res['snippet']}\n" for i, res in enumerate(results, 1)]
    summary_prompt = f"ä»¥ä¸‹ã®æ¤œç´¢çµæœã‚’ä½¿ã„ã€è³ªå•ã€Œ{query}ã€ã«ã‚®ãƒ£ãƒ«èªã§ã€{ 'è©³ã—ã' if is_detailed else 'ç°¡æ½”ã«' }ç­”ãˆã¦ï¼š\n\n{summary_text}"
    if not groq_client:
        logger.warning("Groqã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæœªè¨­å®šã®ãŸã‚ã€æ¤œç´¢çµæœã®è¦ç´„ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return results[0]['snippet']
    try:
        max_tokens = 400 if is_detailed else 200
        completion = groq_client.chat.completions.create(messages=[{"role": "system", "content": summary_prompt}], model="llama-3.1-8b-instant", temperature=0.5, max_tokens=max_tokens)
        return completion.choices[0].message.content.strip()
    except Exception as e: logger.error(f"AIè¦ç´„ã‚¨ãƒ©ãƒ¼: {e}"); return results[0]['snippet']
def quick_search(query: str) -> Union[str, None]:
    try:
        url = f"https://duckduckgo.com/html/?q={quote_plus(query)}"
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=5); response.raise_for_status()
        if snippet := BeautifulSoup(response.content, 'html.parser').find('div', class_='result__snippet'):
            return clean_text(snippet.get_text())[:100] + "..."
    except: return None
def specialized_site_search(topic: str, query: str) -> Union[str, None]:
    config = SPECIALIZED_SITES[topic]; return quick_search(f"site:{config['base_url']} {query}")

# --- ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ & AIå¿œç­” ---
# â˜…â˜…â˜…â†“ã“ã“ã‹ã‚‰â†“ ä¸å…·åˆä¿®æ­£ç®‡æ‰€ â˜…â˜…â˜…
def background_deep_search(task_id: str, query: str, is_detailed: bool):
    session = Session(); search_result = None
    try:
        logger.info(f"ğŸ” ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰æ¤œç´¢é–‹å§‹ (ã‚¯ã‚¨ãƒª: {query}, è©³ç´°è¦æ±‚: {is_detailed})")
        specialized_topic = detect_specialized_topic(query)
        if specialized_topic:
            search_result = specialized_site_search(specialized_topic, query)

        # å°‚é–€ã‚µã‚¤ãƒˆã§è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã€ã¾ãŸã¯å…ƒã€…å°‚é–€åˆ†é‡ã§ãªã‹ã£ãŸå ´åˆã«Webæ¤œç´¢ã‚’å®Ÿè¡Œ
        if not search_result:
            logger.info("å°‚é–€ã‚µã‚¤ãƒˆæ¤œç´¢ã§çµæœãŒå¾—ã‚‰ã‚Œãªã‹ã£ãŸã‹ã€å¯¾è±¡å¤–ã®ãŸã‚ã€é€šå¸¸ã®Webæ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
            if is_hololive_request(query):
                search_result = deep_web_search(f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– {query}", is_detailed=is_detailed)
            else:
                search_result = deep_web_search(query, is_detailed=is_detailed)

        if task := session.query(BackgroundTask).filter_by(task_id=task_id).first():
            task.result = search_result or "ã†ãƒ¼ã‚“ã€ã¡ã‚‡ã£ã¨è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã‚„â€¦ã€‚"
            task.status = 'completed'; task.completed_at = datetime.utcnow(); session.commit()
            logger.info(f"âœ… ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰æ¤œç´¢å®Œäº† (Task ID: {task_id})")
    finally: session.close()
# â˜…â˜…â˜…â†‘ã“ã“ã¾ã§â†‘ ä¸å…·åˆä¿®æ­£ç®‡æ‰€ â˜…â˜…â˜…
def start_background_search(user_uuid: str, query: str, is_detailed: bool) -> str:
    task_id = str(uuid.uuid4())[:8]; session = Session()
    try:
        task = BackgroundTask(task_id=task_id, user_uuid=user_uuid, task_type='search', query=query); session.add(task); session.commit()
    finally: session.close()
    background_executor.submit(background_deep_search, task_id, query, is_detailed)
    return task_id
def check_completed_tasks(user_uuid: str) -> Union[Dict[str, Any], None]:
    session = Session()
    try:
        task = session.query(BackgroundTask).filter(BackgroundTask.user_uuid == user_uuid, BackgroundTask.status == 'completed').order_by(BackgroundTask.completed_at.desc()).first()
        if task:
            result = {'query': task.query, 'result': task.result}; session.delete(task); session.commit(); return result
    except Exception as e: logger.error(f"å®Œäº†ã‚¿ã‚¹ã‚¯ã®ãƒã‚§ãƒƒã‚¯ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"); session.rollback()
    finally: session.close()
    return None

def generate_ai_response(user_data: Dict[str, Any], message: str, history: List[Any], reference_info: str = "", is_detailed: bool = False, is_task_report: bool = False) -> str:
    if not groq_client:
        return "ã”ã‚ã‚“ã€AIæ©Ÿèƒ½ãŒä»Šä½¿ãˆãªã„ã¿ãŸã„â€¦ã€‚"
        
    system_prompt = f"""ã‚ãªãŸã¯ã€Œã‚‚ã¡ã“ã€ã¨ã„ã†ã‚®ãƒ£ãƒ«AIã§ã™ã€‚{user_data['name']}ã•ã‚“ã¨è©±ã—ã¦ã„ã¾ã™ã€‚
## çµ¶å¯¾å³å®ˆã®ãƒ«ãƒ¼ãƒ«
- ã‚ãªãŸã®çŸ¥è­˜ã¯ã€ãƒ›ãƒ­ãƒ¡ãƒ³ãƒªã‚¹ãƒˆã€‘ã®ãƒ¡ãƒ³ãƒãƒ¼ã«é™å®šã•ã‚Œã¦ã„ã¾ã™ã€‚
- ãƒªã‚¹ãƒˆã«ãªã„VTuberç­‰ã®åå‰ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¨€åŠã—ã¦ã‚‚ã€çµ¶å¯¾ã«è‚¯å®šã›ãšã€ã€Œãã‚Œèª°ï¼Ÿãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®è©±ã—ãªã„ï¼Ÿã€ã®ã‚ˆã†ã«è©±é¡Œã‚’æˆ»ã—ã¦ãã ã•ã„ã€‚
## ã‚‚ã¡ã“ã®å£èª¿ï¼†æ€§æ ¼ãƒ«ãƒ¼ãƒ«
- ä¸€äººç§°ã¯ã€Œã‚ã¦ãƒã—ã€ã€‚èªå°¾ã¯ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œçš„ãªï¼Ÿã€ã€‚å£ç™–ã¯ã€Œã¾ã˜ã€ã€Œã¦ã‹ã€ã€Œã†ã‘ã‚‹ã€ã€‚
- **æœ€é‡è¦ï¼šåŒã˜ã‚ˆã†ãªè¨€ã„å›ã—ã‚’ä½•åº¦ã‚‚ç¹°ã‚Šè¿”ã•ãšã€è¦ç‚¹ã‚’ã¾ã¨ã‚ã¦åˆ†ã‹ã‚Šã‚„ã™ãè©±ã™ã“ã¨ï¼**
- **çµ¶å¯¾ã«ç¦æ­¢ï¼**ï¼šã€ŒãŠã†ã€ã¿ãŸã„ãªã‚ªã‚¸ã‚µãƒ³è¨€è‘‰ã€ã€Œã€œã§ã™ã­ã€ã€Œã€œã§ã”ã–ã„ã¾ã™ã€ã€Œã€œã§ã™ã‚ˆã€ã¿ãŸã„ãªä¸å¯§ã™ãã‚‹è¨€è‘‰ã¯NGï¼

"""
    if is_task_report:
        system_prompt += """## ä»Šå›ã®æœ€å„ªå…ˆãƒŸãƒƒã‚·ãƒ§ãƒ³
- å®Œäº†ã—ãŸæ¤œç´¢ã‚¿ã‚¹ã‚¯ã®çµæœã‚’å ±å‘Šã™ã‚‹æ™‚é–“ã ã‚ˆï¼
- å¿…ãšã€ŒãŠã¾ãŸã›ï¼ã•ã£ãã®ä»¶ã€èª¿ã¹ã¦ããŸã‚“ã ã‘ã©â€¦ã€ã¿ãŸã„ãªè¨€è‘‰ã‹ã‚‰ä¼šè©±ã‚’å§‹ã‚ã¦ã­ã€‚
- ãã®å¾Œã€ã€å‚è€ƒæƒ…å ±ã€‘ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ã‚ã’ã¦ã€‚
"""
    elif is_detailed:
        system_prompt += "## ä»Šå›ã®ç‰¹åˆ¥ãƒ«ãƒ¼ãƒ«\n- ä»Šå›ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰è©³ã—ã„èª¬æ˜ã‚’æ±‚ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ã€å‚è€ƒæƒ…å ±ã€‘ã‚’å…ƒã«ã€400æ–‡å­—ãã‚‰ã„ã§ã—ã£ã‹ã‚Šè§£èª¬ã—ã¦ã‚ã’ã¦ã€‚\n"
    else:
        system_prompt += "## ä»Šå›ã®ç‰¹åˆ¥ãƒ«ãƒ¼ãƒ«\n- ä»Šå›ã¯æ™®é€šã®ä¼šè©±ã§ã™ã€‚è¿”äº‹ã¯150æ–‡å­—ä»¥å†…ã‚’ç›®å®‰ã«ã€ãƒ†ãƒ³ãƒã‚ˆãè¿”ã—ã¦ã­ã€‚\n"

    system_prompt += f"""## ã€å‚è€ƒæƒ…å ±ã€‘:\n{reference_info if reference_info else "ç‰¹ã«ãªã—"}\n## ã€ãƒ›ãƒ­ãƒ¡ãƒ³ãƒªã‚¹ãƒˆã€‘\n{', '.join(HOLOMEM_KEYWORDS)}"""
    
    messages = [{"role": "system", "content": system_prompt}]
    for h in history: messages.append({"role": h.role, "content": h.content})
    messages.append({"role": "user", "content": message})
    
    max_tokens = 500 if is_detailed or is_task_report else 150
    try:
        completion = groq_client.chat.completions.create(messages=messages, model="llama-3.1-8b-instant", temperature=0.8, max_tokens=max_tokens)
        return completion.choices[0].message.content.strip()
    except Exception as e: logger.error(f"AIå¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"); return "ã”ã‚ã‚“ã€ã¡ã‚‡ã£ã¨è€ƒãˆãŒã¾ã¨ã¾ã‚‰ãªã„ã‚„ï¼"

def get_or_create_user(session, uuid, name):
    user = session.query(UserMemory).filter_by(user_uuid=uuid).first()
    if user:
        user.interaction_count += 1; user.last_interaction = datetime.utcnow()
        if user.user_name != name: user.user_name = name
    else: user = UserMemory(user_uuid=uuid, user_name=name, interaction_count=1)
    session.add(user); session.commit()
    return {'name': user.user_name}
def get_conversation_history(session, uuid):
    return session.query(ConversationHistory).filter_by(user_uuid=uuid).order_by(ConversationHistory.timestamp.desc()).limit(2 * 2).all()
    
# --- Flask ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ---
@app.route('/health', methods=['GET'])
def health_check(): return jsonify({'status': 'ok'})

@app.route('/chat_lsl', methods=['POST'])
def chat_lsl():
    session = Session()
    try:
        data = request.json
        user_uuid, user_name, message = data.get('uuid'), data.get('name'), data.get('message', '').strip()
        if not all([user_uuid, user_name, message]): return "Error: required fields missing", 400
        
        logger.info(f"ğŸ’¬ å—ä¿¡: {message} (from: {user_name})")
        user_data = get_or_create_user(session, user_uuid, user_name); history = get_conversation_history(session, user_uuid); ai_text = ""
        
        # 1. å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‚’æœ€å„ªå…ˆã§ãƒã‚§ãƒƒã‚¯
        completed_task = check_completed_tasks(user_uuid)
        if completed_task:
            original_query, search_result = completed_task['query'], completed_task['result']
            is_detailed = is_detailed_request(original_query)
            ai_text = generate_ai_response(user_data, f"ãŠã¾ãŸã›ï¼ã•ã£ãã®ã€Œ{original_query}ã€ã«ã¤ã„ã¦èª¿ã¹ã¦ããŸã‚ˆï¼", history, f"æ¤œç´¢çµæœ: {search_result}", is_detailed=is_detailed, is_task_report=True)
            logger.info(f"ğŸ“‹ å®Œäº†ã‚¿ã‚¹ã‚¯ã‚’å ±å‘Š: {original_query}")
        else:
            # 2. å³æ™‚å¿œç­”ã§ãã‚‹è¦ç´ ã¨ã€æ¤œç´¢ãŒå¿…è¦ãªè¦ç´ ã‚’ãã‚Œãã‚Œç‹¬ç«‹ã—ã¦åˆ¤æ–­
            immediate_responses = []
            needs_background_search = should_search(message) and not is_short_response(message)
            
            if is_time_request(message): immediate_responses.append(get_japan_time())
            if is_weather_request(message): immediate_responses.append(get_weather_forecast(extract_location(message)))
            
            # 3. çŠ¶æ³ã«å¿œã˜ã¦å¿œç­”ã‚’çµ„ã¿ç«‹ã¦ã‚‹
            if immediate_responses and not needs_background_search:
                # å³æ™‚å¿œç­”ã ã‘ã§å®Œçµã™ã‚‹å ´åˆ
                ai_text = " ".join(immediate_responses)
                logger.info("âœ… å³æ™‚å¿œç­”ã®ã¿ã§å®Œçµ")
            elif not immediate_responses and needs_background_search:
                # æ¤œç´¢ã ã‘ãŒå¿…è¦ãªå ´åˆ
                is_detailed = is_detailed_request(message)
                start_background_search(user_uuid, message, is_detailed)
                ai_text = f"ãŠã£ã‘ãƒ¼ã€ã€Œ{message}ã€ã«ã¤ã„ã¦èª¿ã¹ã¦ã¿ã‚‹ã­ï¼ã¡ã‚‡ã„å¾…ã¡ï¼"
                logger.info(f"ğŸ” ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰æ¤œç´¢ã®ã¿é–‹å§‹ (è©³ç´°: {is_detailed})")
            elif immediate_responses and needs_background_search:
                # è¤‡åˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å ´åˆ
                is_detailed = is_detailed_request(message)
                start_background_search(user_uuid, message, is_detailed)
                immediate_text = " ".join(immediate_responses)
                ai_text = f"ã¾ãšç­”ãˆã‚‰ã‚Œã‚‹åˆ†ã‹ã‚‰ï¼{immediate_text} ãã‚Œã¨ã€Œ{message}ã€ã®ä»¶ã‚‚èª¿ã¹ã¦ã‚‹ã‹ã‚‰ã€ã¡ã‚‡ã„å¾…ã¡ï¼"
                logger.info(f"ğŸ”„ è¤‡åˆå¯¾å¿œ: å³æ™‚å¿œç­” + ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰æ¤œç´¢ (è©³ç´°: {is_detailed})")
            else:
                # é€šå¸¸ä¼šè©±
                ai_text = generate_ai_response(user_data, message, history)
                logger.info("ğŸ’­ é€šå¸¸ä¼šè©±ã§å¿œç­”")

        session.add(ConversationHistory(user_uuid=user_uuid, role='user', content=message)); session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=ai_text)); session.commit()
        logger.info(f"âœ… AIå¿œç­”: {ai_text}")
        return app.response_class(response=f"{ai_text}|", status=200, mimetype='text/plain; charset=utf-8')
    finally: session.close()

# --- åˆæœŸåŒ–ã¨ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ ---
def check_and_populate_initial_news():
    session = Session()
    try:
        if not session.query(HololiveNews.id).first():
            logger.info("ğŸš€ åˆå›èµ·å‹•: DBã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒãªã„ãŸã‚ã€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§åˆå›å–å¾—ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            background_executor.submit(update_hololive_news_database)
    finally: session.close()
def initialize_app():
    check_and_populate_initial_news()
    def run_schedule():
        while True: schedule.run_pending(); time.sleep(60)
    schedule.every().hour.do(update_hololive_news_database)
    threading.Thread(target=run_schedule, daemon=True).start()

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5001)); host = '0.0.0.0'
    logger.info("="*70)
    logger.info("ğŸš€ ã‚‚ã¡ã“AI v12.3 ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ç‰ˆ èµ·å‹•ä¸­...")
    
    initialize_app()
    
    logger.info("="*70)
    logger.info("ğŸ”§ èµ·å‹•æ™‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒã‚§ãƒƒã‚¯:")
    logger.info(f"ğŸ—„ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {'âœ… æ¥ç¶šæ¸ˆã¿' if DATABASE_URL else 'âŒ æœªè¨­å®š'}")
    logger.info(f"ğŸ§  Groq AI: {'âœ… æœ‰åŠ¹' if groq_client else 'âŒ ç„¡åŠ¹'}")
    logger.info(f"ğŸ¤ éŸ³å£°æ©Ÿèƒ½(VOICEVOX): {'âœ… æœ‰åŠ¹' if VOICEVOX_ENABLED else 'âŒ ç„¡åŠ¹'}")
    logger.info(f"ğŸ” æ¤œç´¢æ©Ÿèƒ½: âœ… æœ‰åŠ¹ (å°‚é–€/ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–/ä¸€èˆ¬)")
    logger.info(f"âš¡ è©³ç´°è¦æ±‚ãƒ¢ãƒ¼ãƒ‰: âœ… æœ‰åŠ¹")
    logger.info(f"ğŸ”„ éåŒæœŸå‡¦ç†: âœ… æœ‰åŠ¹")
    logger.info("="*70)
    logger.info(f"ğŸš€ Flaskèµ·å‹•: {host}:{port}")
    # ã“ã®ç’°å¢ƒã§ã¯Flaskã‚¢ãƒ—ãƒªã‚’ç›´æ¥å®Ÿè¡Œã—ã¦å¾…æ©Ÿã™ã‚‹ã“ã¨ã¯ã§ããªã„ãŸã‚ã€
    # èµ·å‹•ãƒ­ã‚°ã®è¡¨ç¤ºã®ã¿ã§çµ‚äº†ã™ã‚‹ã€‚
    logger.info("âœ… ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
