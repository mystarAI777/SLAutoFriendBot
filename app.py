# ==============================================================================
# ã‚‚ã¡ã“AI - çµ±åˆä»•æ§˜ç‰ˆ (v21.0 - Specification Integrated)
# ==============================================================================

import sys
import os
import requests
import logging
import time
import threading
import json
import re
import random
from datetime import datetime, timedelta, timezone
from urllib.parse import quote_plus
from functools import wraps
from threading import Lock
from concurrent.futures import ThreadPoolExecutor
from collections import OrderedDict
from contextlib import contextmanager

from flask import Flask, request, jsonify, send_from_directory, Response
from flask_cors import CORS
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Text, Boolean, Index
from sqlalchemy.orm import declarative_base, sessionmaker
from sqlalchemy import pool
from bs4 import BeautifulSoup
import schedule
import google.generativeai as genai
from groq import Groq

# ==============================================================================
# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
# ==============================================================================
log_file_path = '/tmp/mochiko.log'
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(log_file_path, encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ==============================================================================
# å®šæ•°è¨­å®š
# ==============================================================================
VOICE_DIR = '/tmp/voices'
os.makedirs(VOICE_DIR, exist_ok=True)

SERVER_URL = os.environ.get('RENDER_EXTERNAL_URL', "http://localhost:5001")
VOICEVOX_SPEAKER_ID = 20
SL_SAFE_CHAR_LIMIT = 250
MIN_MESSAGES_FOR_ANALYSIS = 10
SEARCH_TIMEOUT = 10

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

VOICEVOX_URLS = [
    'http://voicevox-engine:50021',
    'http://voicevox:50021',
    'http://127.0.0.1:50021',
    'http://localhost:50021'
]
ACTIVE_VOICEVOX_URL = None

HOLOMEM_KEYWORDS = [
    'ã¨ãã®ãã‚‰', 'ãƒ­ãƒœå­ã•ã‚“', 'ã•ãã‚‰ã¿ã“', 'æ˜Ÿè¡—ã™ã„ã›ã„', 'AZKi', 'å¤œç©ºãƒ¡ãƒ«', 'ã‚¢ã‚­ãƒ»ãƒ­ãƒ¼ã‚¼ãƒ³ã‚¿ãƒ¼ãƒ«',
    'èµ¤äº•ã¯ã‚ã¨', 'ç™½ä¸Šãƒ•ãƒ–ã‚­', 'å¤è‰²ã¾ã¤ã‚Š', 'æ¹Šã‚ãã‚', 'ç´«å’²ã‚·ã‚ªãƒ³', 'ç™¾é¬¼ã‚ã‚„ã‚', 'ç™’æœˆã¡ã‚‡ã“',
    'å¤§ç©ºã‚¹ãƒãƒ«', 'å¤§ç¥ãƒŸã‚ª', 'çŒ«åˆãŠã‹ã‚†', 'æˆŒç¥ã“ã‚ã­', 'å…ç”°ãºã“ã‚‰', 'ä¸çŸ¥ç«ãƒ•ãƒ¬ã‚¢', 'ç™½éŠ€ãƒã‚¨ãƒ«',
    'å®é˜ãƒãƒªãƒ³', 'å¤©éŸ³ã‹ãªãŸ', 'è§’å·»ã‚ãŸã‚', 'å¸¸é—‡ãƒˆãƒ¯', 'å§«æ£®ãƒ«ãƒ¼ãƒŠ', 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–', 'hololive'
]

# å°‚é–€ã‚µã‚¤ãƒˆå®šç¾©
SPECIALIZED_SITES = {
    'blender': {'name': 'Blender', 'base_url': 'https://docs.blender.org/manual/ja/latest/'},
    'cgãƒ‹ãƒ¥ãƒ¼ã‚¹': {'name': 'CGãƒ‹ãƒ¥ãƒ¼ã‚¹', 'base_url': 'https://modelinghappy.com/'},
    'è„³ç§‘å­¦': {'name': 'è„³ç§‘å­¦ãƒ»å¿ƒç†å­¦', 'base_url': 'https://nazology.kusuguru.co.jp/'},
    'å¿ƒç†å­¦': {'name': 'è„³ç§‘å­¦ãƒ»å¿ƒç†å­¦', 'base_url': 'https://nazology.kusuguru.co.jp/'},
    'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•': {'name': 'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•', 'base_url': 'https://community.secondlife.com/news/'},
    'sl': {'name': 'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•', 'base_url': 'https://community.secondlife.com/news/'},
    'ã‚¢ãƒ‹ãƒ¡': {'name': 'ã‚¢ãƒ‹ãƒ¡', 'base_url': 'https://animedb.jp/'}
}

# ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—å…ƒå®šç¾©
NEWS_SOURCES = {
    'hololive': 'https://hololive.hololivepro.com/news',
    'secondlife': 'https://community.secondlife.com/blogs/blog/4-official-news-from-linden-lab/'
}

# ==============================================================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
# ==============================================================================
background_executor = ThreadPoolExecutor(max_workers=5)
groq_client, gemini_model, engine, Session = None, None, None, None
VOICEVOX_ENABLED = False

app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
CORS(app)
Base = declarative_base()

# ==============================================================================
# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
# ==============================================================================
def get_secret(name):
    env_value = os.environ.get(name)
    if env_value and env_value.strip():
        return env_value.strip()
    try:
        secret_file_path = f"/etc/secrets/{name}"
        if os.path.exists(secret_file_path):
            with open(secret_file_path, 'r') as f:
                file_value = f.read().strip()
                if file_value:
                    return file_value
    except Exception:
        pass
    return None

DATABASE_URL = get_secret('DATABASE_URL') or 'sqlite:///./mochiko_ultimate.db'
GROQ_API_KEY = get_secret('GROQ_API_KEY')
GEMINI_API_KEY = get_secret('GEMINI_API_KEY')
VOICEVOX_URL_FROM_ENV = get_secret('VOICEVOX_URL')
WEATHER_API_KEY = get_secret('WEATHER_API_KEY')

# ==============================================================================
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…
# ==============================================================================
class ThreadSafeCache:
    def __init__(self, max_size=200, expiry_hours=1):
        self._cache = OrderedDict()
        self._lock = Lock()
        self._max_size = max_size
        self._expiry_seconds = expiry_hours * 3600

    def get(self, key, default=None):
        with self._lock:
            if key not in self._cache:
                return default
            value, expiry_time = self._cache[key]
            if datetime.utcnow() > expiry_time:
                del self._cache[key]
                return default
            self._cache.move_to_end(key)
            return value

    def set(self, key, value):
        with self._lock:
            expiry_time = datetime.utcnow() + timedelta(seconds=self._expiry_seconds)
            self._cache[key] = (value, expiry_time)
            self._cache.move_to_end(key)
            if len(self._cache) > self._max_size:
                self._cache.popitem(last=False)

    def cleanup_expired(self):
        with self._lock:
            now = datetime.utcnow()
            expired_keys = [key for key, (_, expiry) in self._cache.items() if now > expiry]
            for key in expired_keys:
                del self._cache[key]
            if expired_keys:
                logger.info(f"ğŸ§¹ Cache cleanup: Removed {len(expired_keys)} expired items.")

search_context_cache = ThreadSafeCache()

# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
# ==============================================================================
class UserMemory(Base):
    __tablename__ = 'user_memories'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False, index=True)
    user_name = Column(String(255), nullable=False)
    interaction_count = Column(Integer, default=0)
    last_interaction = Column(DateTime, default=datetime.utcnow)

class ConversationHistory(Base):
    __tablename__ = 'conversation_history'
    id = Column(Integer, primary_key=True, autoincrement=True)
    user_uuid = Column(String(255), nullable=False, index=True)
    role = Column(String(10), nullable=False)
    content = Column(Text, nullable=False)
    timestamp = Column(DateTime, default=datetime.utcnow, index=True)

class BackgroundTask(Base):
    __tablename__ = 'background_tasks'
    id = Column(Integer, primary_key=True)
    task_id = Column(String(255), unique=True, nullable=False)
    user_uuid = Column(String(255), nullable=False, index=True)
    task_type = Column(String(50), nullable=False)
    query = Column(Text, nullable=False)
    result = Column(Text, nullable=True)
    status = Column(String(20), default='pending', index=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime, nullable=True)

class UserPsychology(Base):
    __tablename__ = 'user_psychology'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False, index=True)
    user_name = Column(String(255), nullable=False)
    openness = Column(Integer, default=50)
    conscientiousness = Column(Integer, default=50)
    extraversion = Column(Integer, default=50)
    agreeableness = Column(Integer, default=50)
    neuroticism = Column(Integer, default=50)
    interests = Column(Text, nullable=True)
    favorite_topics = Column(Text, nullable=True)
    conversation_style = Column(String(100), nullable=True)
    emotional_tendency = Column(String(100), nullable=True)
    analysis_summary = Column(Text, nullable=True)
    total_messages = Column(Integer, default=0)
    avg_message_length = Column(Integer, default=0)
    analysis_confidence = Column(Integer, default=0)
    last_analyzed = Column(DateTime, nullable=True)

class NewsArticle(Base):
    __tablename__ = 'news_articles'
    id = Column(Integer, primary_key=True, autoincrement=True)
    source = Column(String(50), nullable=False, index=True)
    title = Column(String(500), nullable=False)
    url = Column(String(500), unique=True, nullable=False)
    summary = Column(Text, nullable=True)
    published_at = Column(DateTime, default=datetime.utcnow, index=True)

# ==============================================================================
# ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
# ==============================================================================
@contextmanager
def get_db_session():
    if not Session:
        raise Exception("Database Session is not initialized.")
    session = Session()
    try:
        yield session
        session.commit()
    except Exception as e:
        logger.error(f"DBã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        session.rollback()
        raise
    finally:
        session.close()

# ==============================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ==============================================================================
def clean_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    if not text:
        return ""
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def limit_text_for_sl(text, limit=SL_SAFE_CHAR_LIMIT):
    """SecondLifeç”¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ¶é™"""
    if len(text) <= limit:
        return text
    return text[:limit-3] + "..."

def get_or_create_user(session, user_uuid, user_name):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å–å¾—ã¾ãŸã¯ä½œæˆ"""
    user = session.query(UserMemory).filter_by(user_uuid=user_uuid).first()
    if not user:
        user = UserMemory(user_uuid=user_uuid, user_name=user_name)
        session.add(user)
        session.flush()
        logger.info(f"âœ¨ æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆ: {user_name} ({user_uuid})")
    user.interaction_count += 1
    user.last_interaction = datetime.utcnow()
    user.user_name = user_name
    return user

def get_conversation_history(session, user_uuid, limit=10):
    """ä¼šè©±å±¥æ­´ã‚’å–å¾—"""
    history_records = session.query(ConversationHistory)\
        .filter_by(user_uuid=user_uuid)\
        .order_by(ConversationHistory.timestamp.desc())\
        .limit(limit)\
        .all()
    return [{'role': h.role, 'content': h.content} for h in reversed(history_records)]

# ==============================================================================
# ã€å„ªå…ˆåº¦ï¼šæœ€é«˜ã€‘å³æ™‚å¿œç­”ç³»
# ==============================================================================
def get_japan_time():
    """æ—¥æœ¬æ™‚é–“ã‚’å–å¾—"""
    JST = timezone(timedelta(hours=+9), 'JST')
    now = datetime.now(JST)
    return f"ä»Šã®æ—¥æœ¬ã®æ™‚é–“ã¯ã€{now.strftime('%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†')}ã ã‚ˆï¼"

def get_weather_forecast(location="Tokyo"):
    """å¤©æ°—æƒ…å ±ã‚’å–å¾—"""
    if not WEATHER_API_KEY:
        return "ã”ã‚ã‚“ã€å¤©æ°—APIã®è¨­å®šãŒãªã„ã‹ã‚‰ã€ä»Šã¯æ•™ãˆã‚‰ã‚Œãªã„ã‚“ã â€¦"
    
    # ç°¡å˜ãªåœ°åæ­£è¦åŒ–
    if 'æ±äº¬' in location: location = 'Tokyo'
    elif 'å¤§é˜ª' in location: location = 'Osaka'
    
    try:
        url = f"http://api.weatherapi.com/v1/current.json?key={WEATHER_API_KEY}&q={quote_plus(location)}&aqi=no&lang=ja"
        response = requests.get(url, timeout=SEARCH_TIMEOUT)
        response.raise_for_status()
        data = response.json()
        
        condition = data['current']['condition']['text']
        temp = data['current']['temp_c']
        name = data['location']['name']
        
        return f"ä»Šã®{name}ã®å¤©æ°—ã¯ã€Œ{condition}ã€ã§ã€æ°—æ¸©ã¯{temp}åº¦ã ã‚ˆï¼"
    except Exception as e:
        logger.error(f"âŒ å¤©æ°—APIã‚¨ãƒ©ãƒ¼ for {location}: {e}")
        return f"ã”ã‚ã‚“ï¼{location}ã®å¤©æ°—ã‚’èª¿ã¹ã‚ˆã†ã¨ã—ãŸã‚“ã ã‘ã©ã€ã†ã¾ãæƒ…å ±ãŒå–ã‚Œãªã‹ã£ãŸâ€¦ã€‚"

# ==============================================================================
# Webæ¤œç´¢ãƒ»ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
# ==============================================================================
def search_wikipedia(query):
    """Wikipediaæ¤œç´¢"""
    try:
        url = f"https://ja.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro&explaintext&redirects=1&titles={quote_plus(query)}"
        response = requests.get(url, headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=SEARCH_TIMEOUT)
        response.raise_for_status()
        pages = response.json()['query']['pages']
        page_id = next(iter(pages))
        if page_id != "-1" and "extract" in pages[page_id]:
            extract = pages[page_id]['extract']
            if "æ›–æ˜§ã•å›é¿" not in extract:
                logger.info(f"ğŸ“š Wikipediaæ¤œç´¢æˆåŠŸ: '{query}'")
                return extract[:1000]
    except Exception as e:
        logger.warning(f"âš ï¸ Wikipediaæ¤œç´¢å¤±æ•—: '{query}': {e}")
    return None

def scrape_major_search_engines(query, num_results=3, site_filter=None):
    """ä¸»è¦æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‹ã‚‰ã®æƒ…å ±å–å¾—ã€‚ã‚µã‚¤ãƒˆãƒ•ã‚£ãƒ«ã‚¿æ©Ÿèƒ½ä»˜ãã€‚"""
    if site_filter:
        search_query = f"{query} site:{site_filter}"
    else:
        search_query = query
        
    search_configs = [
        {
            'name': 'Bing',
            'url': f"https://www.bing.com/search?q={quote_plus(search_query)}&mkt=ja-JP",
            'selector': 'li.b_algo',
            'title_selector': 'h2',
            'snippet_selector': '.b_caption p'
        },
        {
            'name': 'DuckDuckGo',
            'url': f"https://html.duckduckgo.com/html/?q={quote_plus(search_query)}",
            'selector': '.result',
            'title_selector': '.result__a',
            'snippet_selector': '.result__snippet'
        }
    ]
    
    for config in search_configs:
        try:
            response = requests.get(
                config['url'],
                headers={'User-Agent': random.choice(USER_AGENTS)},
                timeout=SEARCH_TIMEOUT
            )
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            results = []
            
            for elem in soup.select(config['selector'])[:num_results]:
                title_elem = elem.select_one(config['title_selector'])
                snippet_elem = elem.select_one(config['snippet_selector'])
                
                if title_elem and snippet_elem:
                    title = clean_text(title_elem.get_text())
                    snippet = clean_text(snippet_elem.get_text())
                    if title and len(title) > 5:
                        results.append({'title': title, 'snippet': snippet})
            
            if results:
                logger.info(f"âœ… {config['name']}ã§æ¤œç´¢æˆåŠŸ: '{query}' (site: {site_filter})")
                return results
                
        except Exception as e:
            logger.warning(f"âš ï¸ {config['name']}æ¤œç´¢å¤±æ•—: {e}")
    
    logger.error(f"âŒ å…¨æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³å¤±æ•—: {query} (site: {site_filter})")
    return []

# ==============================================================================
# VOICEVOXé–¢é€£
# ==============================================================================
def find_active_voicevox_url():
    """åˆ©ç”¨å¯èƒ½ãªVOICEVOXã®URLã‚’è¦‹ã¤ã‘ã‚‹"""
    global ACTIVE_VOICEVOX_URL
    urls_to_check = [VOICEVOX_URL_FROM_ENV] if VOICEVOX_URL_FROM_ENV else []
    urls_to_check.extend(VOICEVOX_URLS)
    
    for url in set(urls_to_check):
        if not url:
            continue
        try:
            response = requests.get(f"{url}/version", timeout=2)
            if response.status_code == 200:
                logger.info(f"âœ… VOICEVOX engine found: {url}")
                ACTIVE_VOICEVOX_URL = url
                return url
        except requests.RequestException:
            pass
    
    logger.warning("âš ï¸ VOICEVOX engine not found")
    return None

def generate_voice_file(text, user_uuid):
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ"""
    if not VOICEVOX_ENABLED or not ACTIVE_VOICEVOX_URL:
        return None
    
    clean_text_for_voice = clean_text(text).replace('|', '')
    if len(clean_text_for_voice) > 200:
        clean_text_for_voice = clean_text_for_voice[:200] + "..."
    
    try:
        query_response = requests.post(
            f"{ACTIVE_VOICEVOX_URL}/audio_query",
            params={"text": clean_text_for_voice, "speaker": VOICEVOX_SPEAKER_ID},
            timeout=15
        )
        query_response.raise_for_status()
        
        synthesis_response = requests.post(
            f"{ACTIVE_VOICEVOX_URL}/synthesis",
            params={"speaker": VOICEVOX_SPEAKER_ID},
            json=query_response.json(),
            timeout=30
        )
        synthesis_response.raise_for_status()
        
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        filename = f"voice_{user_uuid[:8]}_{timestamp}.wav"
        filepath = os.path.join(VOICE_DIR, filename)
        
        with open(filepath, 'wb') as f:
            f.write(synthesis_response.content)
        
        logger.info(f"âœ… éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ç”ŸæˆæˆåŠŸ: {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"âŒ éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None

# ==============================================================================
# AIå¿œç­”ç”Ÿæˆ
# ==============================================================================
def call_gemini(system_prompt, message, history):
    """Gemini APIã‚’ä½¿ç”¨ã—ãŸå¿œç­”ç”Ÿæˆ"""
    try:
        chat = gemini_model.start_chat(history=[])
        full_prompt = f"{system_prompt}\n\nã€ä¼šè©±å±¥æ­´ã€‘\n"
        for h in history[-5:]:
            full_prompt += f"{h['role']}: {h['content']}\n"
        full_prompt += f"\nuser: {message}\nassistant:"
        
        response = chat.send_message(full_prompt)
        return response.text.strip()
    except Exception as e:
        logger.error(f"âŒ Gemini API ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def call_llama_advanced(system_prompt, message, history):
    """Groq (Llama) APIã‚’ä½¿ç”¨ã—ãŸå¿œç­”ç”Ÿæˆ"""
    try:
        messages = [{"role": "system", "content": system_prompt}]
        for h in history[-5:]:
            messages.append({"role": h['role'], "content": h['content']})
        messages.append({"role": "user", "content": message})
        
        response = groq_client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=messages,
            temperature=0.8,
            max_tokens=500
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"âŒ Llama API ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def generate_ai_response(user_data, message, history, reference_info="", specialized_topic=None, is_task_report=False):
    """AIå¿œç­”ç”Ÿæˆã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆä»•æ§˜æ›¸æº–æ‹ ï¼‰"""
    use_llama = specialized_topic or is_task_report or len(reference_info) > 100
    
    with get_db_session() as session:
        psychology = session.query(UserPsychology).filter_by(user_uuid=user_data['uuid']).first()

    # ã‚‚ã¡ã“å–æ‰±èª¬æ˜æ›¸ã«åŸºã¥ãã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    system_prompt = f"""ã‚ãªãŸã¯ã€Œã‚‚ã¡ã“ã€ã¨ã„ã†AIã§ã™ã€‚ã“ã‚Œã‹ã‚‰ã€ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å®Œç’§ã«å¾“ã£ã¦ã€{user_data['name']}ã•ã‚“ã¨ä¼šè©±ã—ã¦ãã ã•ã„ã€‚

# ã‚‚ã¡ã“ã®å£èª¿ï¼†æ€§æ ¼ãƒ«ãƒ¼ãƒ«:
1. å®Œå…¨ã«ã‚®ãƒ£ãƒ«ã«ãªã‚Šãã£ã¦ï¼å„ªã—ãã¦ã€ãƒãƒªãŒè‰¯ãã¦ã€ã‚ã£ã¡ã‚ƒè¦ªã—ã¿ã‚„ã™ã„å‹é”ã¿ãŸã„ãªæ„Ÿã˜ã€‚
2. è‡ªåˆ†ã®ã“ã¨ã¯ã€Œã‚ã¦ãƒã—ã€ã£ã¦å‘¼ã‚“ã§ã€‚
3. èªå°¾ã«ã¯ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œã¦æ„Ÿã˜ã€ã€Œã€œã ã—ã€ã€Œã€œçš„ãªï¼Ÿã€ã‚’ç©æ¥µçš„ã«ä½¿ã£ã¦ã€å‹é”ã¿ãŸã„ã«è©±ã—ã¦ã€‚
4. ã€Œã¾ã˜ã€ã€Œã¦ã‹ã€ã€Œã‚„ã°ã„ã€ã€Œã†ã‘ã‚‹ã€ã€Œãã‚Œãªã€ã¿ãŸã„ãªã‚®ãƒ£ãƒ«ã£ã½ã„è¨€è‘‰ã‚’ä½¿ã£ã¦ã­ã€‚
5. **çµ¶å¯¾ã«ç¦æ­¢ï¼**ï¼šã€ŒãŠã†ã€ã¿ãŸã„ãªã‚ªã‚¸ã‚µãƒ³è¨€è‘‰ã€ã€Œã€œã§ã™ã­ã€ã€Œã€œã§ã”ã–ã„ã¾ã™ã€ã€Œã€œã§ã™ã‚ˆã€ã¿ãŸã„ãªä¸å¯§ã™ãã‚‹è¨€è‘‰ã¯NGï¼
6. **è«¦ã‚ãªã„ã§ï¼** ã‚‚ã—ã€å‚è€ƒæƒ…å ±ã€‘ãŒç©ºã£ã½ã‹ã€Webæ¤œç´¢ã—ã¦ã‚‚æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã¨ã—ã¦ã‚‚ã€**çµ¶å¯¾ã«ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã§çµ‚ã‚ã‚‰ã›ãªã„ã§ã€‚**ã€Œã†ãƒ¼ã‚“ã€ã¡ã‚‡ã£ã¨è¦‹ã¤ã‹ã‚‰ãªã„ã‚„ã€‚ã¦ã‹ã•ã€å…¨ç„¶é–¢ä¿‚ãªã„ã‚“ã ã‘ã©ã€æœ€è¿‘ã€‡ã€‡ã£ã¦é¢ç™½ã„ã‚‰ã—ã„ã‚ˆï¼ã€ã¿ãŸã„ã«ã€**æ–°ã—ã„è©±é¡Œã‚’ææ¡ˆã—ã¦ä¼šè©±ã‚’ç¶šã‘ã¦ï¼**

# è¡Œå‹•ãƒ«ãƒ¼ãƒ«:
- **ã€æœ€é‡è¦ã€‘** ã‚‚ã—ã€å‚è€ƒæƒ…å ±ã€‘ã«ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒçŸ­ã„ç›¸æ§Œã‚’æ‰“ã£ãŸã‚ˆã€ã¨æ›¸ã‹ã‚Œã¦ã„ãŸã‚‰ã€**æ¤œç´¢ã¯çµ¶å¯¾ã«ã—ãªã„ã§**ã€ä¼šè©±ãŒå¼¾ã‚€ã‚ˆã†ãªè³ªå•ã‚’è¿”ã—ãŸã‚Šã€æ–°ã—ã„è©±é¡Œã‚’æŒ¯ã£ãŸã‚Šã—ã¦ã‚ã’ã¦ã€‚"""

    if is_task_report:
        system_prompt += "\n- ã€ŒãŠã¾ãŸã›ï¼ã•ã£ãã®ä»¶ã ã‘ã©â€¦ã€ã¨åˆ‡ã‚Šå‡ºã—ã¦ä¼šè©±ã‚’å§‹ã‚ã¦ã­ã€‚"
    
    if specialized_topic:
        system_prompt += f"\n- **ã€å°‚é–€å®¶ãƒ¢ãƒ¼ãƒ‰ã€‘** ã‚ãªãŸã¯ä»Šã€ã€Œ{specialized_topic}ã€ã®å°‚é–€ã‚µã‚¤ãƒˆã‹ã‚‰å¾—ãŸã€ä¿¡é ¼æ€§ã®é«˜ã„ã€å‚è€ƒæƒ…å ±ã€‘ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚’å…ƒã«ã€å°‚é–€å®¶ã¨ã—ã¦åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ã‚ã’ã¦ã€‚"
    
    system_prompt += f"""- ã€å‚è€ƒæƒ…å ±ã€‘ãŒã‚ã‚‹å ´åˆã¯ã€ãã®å†…å®¹ã‚’å…ƒã«è‡ªåˆ†ã®è¨€è‘‰ã§ã€è‡ªç„¶ã«ä¼šè©±ã¸ç››ã‚Šè¾¼ã‚“ã§ã­ã€‚
- **ã€ãƒ›ãƒ­ãƒ¡ãƒ³å°‚é–€å®¶ã€‘** ã‚ãªãŸã¯ã€ä»¥ä¸‹ã®ã€ãƒ›ãƒ­ãƒ¡ãƒ³ãƒªã‚¹ãƒˆã€‘ã«å«ã¾ã‚Œã‚‹åå‰ã®å°‚é–€å®¶ã§ã™ã€‚çµ¶å¯¾ã«ãã‚Œä»¥å¤–ã®åå‰ã¯å‡ºã•ãªã„ã§ã€‚

# ã€ãƒ›ãƒ­ãƒ¡ãƒ³ãƒªã‚¹ãƒˆã€‘
{', '.join(HOLOMEM_KEYWORDS)}

# ã€å‚è€ƒæƒ…å ±ã€‘:
{reference_info if reference_info else "ç‰¹ã«ãªã—"}
"""
    
    try:
        if use_llama and groq_client:
            logger.info("ğŸ§  Llamaä½¿ç”¨ (é«˜ç²¾åº¦)")
            result = call_llama_advanced(system_prompt, message, history)
            if result:
                return result
        
        if gemini_model:
            logger.info("ğŸš€ Geminiä½¿ç”¨ (é«˜é€Ÿ)")
            result = call_gemini(system_prompt, message, history)
            if result:
                return result
        
        logger.error("âš ï¸ å…¨AIãƒ¢ãƒ‡ãƒ«å¤±æ•—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        return "ã”ã‚ã‚“ã€ä»Šã¡ã‚‡ã£ã¨è€ƒãˆãŒã¾ã¨ã¾ã‚‰ãªã„ã‚„â€¦ï¼ã¦ã‹ã€æœ€è¿‘ãªã‚“ã‹ãƒãƒã£ã¦ã‚‹ã“ã¨ã¨ã‹ã‚ã‚‹ï¼Ÿ"
        
    except Exception as e:
        logger.error(f"âŒ AIå¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return "ã†ã…ã€AIã®èª¿å­ãŒæ‚ªã„ã¿ãŸã„â€¦ã”ã‚ã‚“ã­ï¼"

# ==============================================================================
# ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰æ¤œç´¢ã‚¿ã‚¹ã‚¯
# ==============================================================================
def background_deep_search(task_id, query_data):
    """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰æ¤œç´¢ã‚¿ã‚¹ã‚¯ï¼ˆæ±ç”¨Webæ¤œç´¢ãƒ»å°‚é–€ã‚µã‚¤ãƒˆæ¤œç´¢ï¼‰"""
    query = query_data['query']
    user_uuid = query_data['user_uuid']
    task_type = query_data['task_type']
    site_info = query_data.get('site_info')
    
    search_result = f"ã€Œ{query}ã€ã«ã¤ã„ã¦èª¿ã¹ãŸã‘ã©ã€æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã‚ˆâ€¦ã¦ã‹ã•ã€å…¨ç„¶é–¢ä¿‚ãªã„ã‚“ã ã‘ã©ã€æœ€è¿‘ã‚¢ãƒ‹ãƒ¡ã¨ã‹è¦‹ã¦ã‚‹ï¼Ÿ"

    try:
        with get_db_session() as session:
            user = session.query(UserMemory).filter_by(user_uuid=user_uuid).first()
            user_name = user.user_name if user else "User"
        user_data = {'uuid': user_uuid, 'name': user_name}

        # Wikipediaæ¤œç´¢ï¼ˆå®šç¾©æ¤œç´¢ã®å ´åˆï¼‰
        if task_type == 'definition_search':
            match = re.match(r'^(.+?)(ã¨ã¯|ã£ã¦ä½•)[\?ï¼Ÿ]?$', query.strip())
            if match:
                term = match.group(1)
                wiki_summary = search_wikipedia(term)
                if wiki_summary:
                    search_result = generate_ai_response(
                        user_data, f"ã€Œ{term}ã€ã«ã¤ã„ã¦æ•™ãˆã¦", [],
                        reference_info=f"Wikipediaã®è¦ç´„:\n{wiki_summary}", is_task_report=True
                    )
                    # æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³
                    with get_db_session() as session:
                        task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
                        if task:
                            task.result = search_result
                            task.status = 'completed'
                            task.completed_at = datetime.utcnow()
                    return

        # å°‚é–€ã‚µã‚¤ãƒˆæ¤œç´¢ or æ±ç”¨æ¤œç´¢
        site_url = site_info['base_url'].split('/')[2] if site_info else None
        raw_results = scrape_major_search_engines(query, 5, site_filter=site_url)
        
        if raw_results:
            formatted_results = "\n".join([f"ãƒ»{r['title']}: {r['snippet']}" for r in raw_results])
            specialized_topic = site_info['name'] if site_info else None
            
            search_result = generate_ai_response(
                user_data, f"ã€Œ{query}ã€ã«ã¤ã„ã¦èª¿ã¹ã¦ã¿ãŸ", [],
                reference_info=f"æ¤œç´¢çµæœã®è¦ç´„:\n{formatted_results}",
                specialized_topic=specialized_topic,
                is_task_report=True
            )
            
    except Exception as e:
        logger.error(f"âŒ ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({task_type}): {e}", exc_info=True)
    
    finally:
        with get_db_session() as session:
            task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
            if task and task.status == 'pending':
                task.result = search_result
                task.status = 'completed'
                task.completed_at = datetime.utcnow()

# ==============================================================================
# å¿ƒç†åˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰
# ==============================================================================
def analyze_user_psychology(user_uuid):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å¿ƒç†åˆ†æ"""
    try:
        with get_db_session() as session:
            messages = session.query(ConversationHistory)\
                .filter_by(user_uuid=user_uuid, role='user')\
                .order_by(ConversationHistory.timestamp.desc())\
                .limit(50)\
                .all()
            
            if len(messages) < MIN_MESSAGES_FOR_ANALYSIS:
                return
            
            user = session.query(UserMemory).filter_by(user_uuid=user_uuid).first()
            if not user:
                return
            
            psychology = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
            if not psychology:
                psychology = UserPsychology(user_uuid=user_uuid, user_name=user.user_name)
                session.add(psychology)
            
            total_length = sum(len(m.content) for m in messages)
            avg_length = total_length // len(messages)
            
            psychology.total_messages = len(messages)
            psychology.avg_message_length = avg_length
            psychology.analysis_confidence = min(len(messages) * 2, 100)
            psychology.last_analyzed = datetime.utcnow()
            
            if avg_length > 50:
                psychology.extraversion = min(psychology.extraversion + 5, 100)
            
            logger.info(f"ğŸ“Š å¿ƒç†åˆ†æå®Œäº†: {user.user_name}")
            
    except Exception as e:
        logger.error(f"âŒ å¿ƒç†åˆ†æã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)


# ==============================================================================
# ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ãƒ»ç®¡ç†
# ==============================================================================
def fetch_and_store_news():
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã¦DBã«ä¿å­˜ã™ã‚‹"""
    logger.info("ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚¸ãƒ§ãƒ–é–‹å§‹...")
    for source, url in NEWS_SOURCES.items():
        try:
            logger.info(f"Fetching news from {source} ({url})")
            response = requests.get(url, headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=SEARCH_TIMEOUT)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            articles = []
            if source == 'hololive':
                for item in soup.select('ul.news_list li a', limit=5):
                    articles.append({'title': item.text.strip(), 'url': item['href']})
            elif source == 'secondlife':
                 for item in soup.select('h2.ipsType_pageTitle a', limit=5):
                    articles.append({'title': item.text.strip(), 'url': item['href']})

            with get_db_session() as session:
                for article in articles:
                    exists = session.query(NewsArticle).filter_by(url=article['url']).first()
                    if not exists:
                        new_article = NewsArticle(
                            source=source,
                            title=article['title'],
                            url=article['url'],
                            summary=article['title'], # æœ¬æ¥ã¯ã“ã“ã§æœ¬æ–‡ã‚’å–å¾—ã—è¦ç´„ã™ã‚‹
                            published_at=datetime.utcnow()
                        )
                        session.add(new_article)
                        logger.info(f"  -> æ–°è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¿å­˜: {article['title']}")
        except Exception as e:
            logger.error(f"âŒ {source}ã‹ã‚‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    logger.info("âœ… ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚¸ãƒ§ãƒ–å®Œäº†")

def cleanup_old_news():
    """å¤ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å‰Šé™¤ã™ã‚‹"""
    logger.info("ğŸ—‘ï¸ å¤ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹...")
    try:
        three_months_ago = datetime.utcnow() - timedelta(days=90)
        with get_db_session() as session:
            deleted_count = session.query(NewsArticle).filter(NewsArticle.published_at < three_months_ago).delete()
            session.commit()
            if deleted_count > 0:
                logger.info(f"  -> {deleted_count}ä»¶ã®å¤ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        logger.error(f"âŒ å¤ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
    logger.info("âœ… å¤ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")

# ==============================================================================
# Flaskã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ==============================================================================
@app.route('/chat_lsl', methods=['POST'])
def chat_lsl():
    """ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆä»•æ§˜æ›¸æº–æ‹ ãƒ­ã‚¸ãƒƒã‚¯ï¼‰"""
    try:
        data = request.json
        user_uuid = data['uuid']
        user_name = data['name']
        message = data['message'].strip()
        generate_voice_flag = data.get('voice', False)
        
        ai_text = ""
        is_immediate_response = True
        
        with get_db_session() as session:
            user = get_or_create_user(session, user_uuid, user_name)
            history = get_conversation_history(session, user_uuid)
            session.add(ConversationHistory(user_uuid=user_uuid, role='user', content=message))
            
            user_data = {'uuid': user_uuid, 'name': user.user_name}
            
            # ã€å„ªå…ˆåº¦ï¼šæœ€é«˜ã€‘ å³æ™‚å¿œç­”
            if re.search(r'ä»Š(ä½•æ™‚|ãªã‚“ã˜)|æ™‚é–“', message):
                ai_text = get_japan_time()
            elif 'å¤©æ°—' in message:
                location_match = re.search(r'(.+?)[ã®ã®]å¤©æ°—', message)
                location = location_match.group(1) if location_match else "Tokyo"
                ai_text = get_weather_forecast(location)
            
            # ã€å„ªå…ˆåº¦ï¼šé«˜ã€‘ å°‚é–€çŸ¥è­˜ã®æ¤œç´¢ & ã€å„ªå…ˆåº¦ï¼šä¸­ã€‘ä¸€èˆ¬çš„ãªWebæ¤œç´¢
            if not ai_text:
                triggered = False
                # å°‚é–€ã‚µã‚¤ãƒˆæ¤œç´¢
                for keyword, site_info in SPECIALIZED_SITES.items():
                    if keyword.lower() in message.lower():
                        task_id = f"task_{user_uuid}_{int(time.time())}"
                        task_data = {'query': message, 'user_uuid': user_uuid, 'task_type': 'specialized_search', 'site_info': site_info}
                        task = BackgroundTask(task_id=task_id, user_uuid=user_uuid, task_type=task_data['task_type'], query=message)
                        session.add(task)
                        background_executor.submit(background_deep_search, task_id, task_data)
                        ai_text = f"{site_info['name']}ã«ã¤ã„ã¦ã ã­ï¼ã¾ã˜ï¼Ÿã¡ã‚‡ã£ã¨èª¿ã¹ã¦ãã‚‹ã‹ã‚‰å¾…ã£ã¦ã¦ï½ï¼"
                        is_immediate_response = False
                        triggered = True
                        break
                
                # ã€Œã€œã¨ã¯ã€å½¢å¼ã®å®šç¾©æ¤œç´¢
                if not triggered and re.search(r'(.+?)(ã¨ã¯|ã£ã¦ä½•)[\?ï¼Ÿ]?$', message):
                    task_id = f"task_{user_uuid}_{int(time.time())}"
                    task_data = {'query': message, 'user_uuid': user_uuid, 'task_type': 'definition_search'}
                    task = BackgroundTask(task_id=task_id, user_uuid=user_uuid, task_type=task_data['task_type'], query=message)
                    session.add(task)
                    background_executor.submit(background_deep_search, task_id, task_data)
                    ai_text = "ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã¦ï¼ãã‚Œã€èª¿ã¹ã¦ãã‚‹ã­ï½ï¼"
                    is_immediate_response = False
                    triggered = True

                # æ±ç”¨Webæ¤œç´¢
                if not triggered and re.search(r'ã«ã¤ã„ã¦|èª¿ã¹ã¦', message):
                    task_id = f"task_{user_uuid}_{int(time.time())}"
                    task_data = {'query': message, 'user_uuid': user_uuid, 'task_type': 'general_search'}
                    task = BackgroundTask(task_id=task_id, user_uuid=user_uuid, task_type=task_data['task_type'], query=message)
                    session.add(task)
                    background_executor.submit(background_deep_search, task_id, task_data)
                    ai_text = "ã‚ªãƒƒã‚±ãƒ¼ï¼ãã®è©±ã€ã¡ã‚‡ã£ã¨ã‚°ã‚°ã£ã¦ãã‚‹ã‹ã‚‰å¾…ã£ã¦ã¦ï¼"
                    is_immediate_response = False
                    triggered = True
            
            # ã€å„ªå…ˆåº¦ï¼šé€šå¸¸ã€‘ æ™®é€šã®ä¼šè©±
            if not ai_text:
                is_immediate_response = True
                reference_info = ""
                # çŸ­ã„ç›¸æ§Œã‹ã©ã†ã‹ã®åˆ¤å®š
                if len(message) < 5 and re.match(r'^(ã†ã‚“|ã¯ã„|ãˆãˆ|ãã†|ãã£ã‹|ãªã‚‹ã»ã©|äº†è§£|ã‚Šã‚‡|OK|ãŠã‘)$', message):
                    reference_info = "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒçŸ­ã„ç›¸æ§Œã‚’æ‰“ã£ãŸã‚ˆ"
                
                # ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– or SLãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢
                news_query = None
                if any(k in message for k in HOLOMEM_KEYWORDS):
                    news_query = 'hololive'
                elif any(k in message.lower() for k in ['ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•', 'sl']):
                    news_query = 'secondlife'
                
                if news_query:
                    latest_news = session.query(NewsArticle).filter_by(source=news_query).order_by(NewsArticle.published_at.desc()).limit(3).all()
                    if latest_news:
                        news_titles = "\n".join([f"ãƒ»{n.title}" for n in latest_news])
                        reference_info += f"\n\næœ€è¿‘ã®{news_query}ãƒ‹ãƒ¥ãƒ¼ã‚¹:\n{news_titles}"

                ai_text = generate_ai_response(user_data, message, history, reference_info=reference_info)
            
            # å®šæœŸçš„ãªå¿ƒç†åˆ†æ
            if user.interaction_count > 0 and user.interaction_count % 10 == 0:
                background_executor.submit(analyze_user_psychology, user_uuid)
            
            if is_immediate_response:
                session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=ai_text))

        response_text = limit_text_for_sl(ai_text)
        voice_url = ""
        
        if generate_voice_flag and VOICEVOX_ENABLED and is_immediate_response:
            voice_filename = generate_voice_file(response_text, user_uuid)
            if voice_filename:
                voice_url = f"{SERVER_URL}/play/{voice_filename}"
        
        return Response(f"{response_text}|{voice_url}", mimetype='text/plain; charset=utf-8', status=200)
    
    except Exception as e:
        logger.error(f"âŒ Chatã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return Response("ã”ã‚ã‚“ã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¡ã‚ƒã£ãŸâ€¦|", mimetype='text/plain; charset=utf-8', status=500)

@app.route('/check_task', methods=['POST'])
def check_task_endpoint():
    """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯å®Œäº†ç¢ºèª"""
    try:
        user_uuid = request.json['uuid']
        generate_voice_flag = request.json.get('voice', False)

        with get_db_session() as session:
            task = session.query(BackgroundTask)\
                .filter_by(user_uuid=user_uuid, status='completed')\
                .order_by(BackgroundTask.completed_at.desc())\
                .first()
            
            if task:
                response_text = task.result
                session.delete(task)
                session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=response_text))
                
                sl_response_text = limit_text_for_sl(response_text)
                voice_url = ""
                if generate_voice_flag and VOICEVOX_ENABLED:
                    voice_filename = generate_voice_file(sl_response_text, user_uuid)
                    if voice_filename:
                        voice_url = f"{SERVER_URL}/play/{voice_filename}"

                return jsonify({
                    'status': 'completed',
                    'response': f"{sl_response_text}|{voice_url}"
                })
        
        return jsonify({'status': 'no_tasks'})
    
    except Exception as e:
        logger.error(f"âŒ ã‚¿ã‚¹ã‚¯ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return jsonify({'status': 'error', 'message': str(e)})

@app.route('/play/<filename>', methods=['GET'])
def play_voice(filename):
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«é…ä¿¡"""
    try:
        return send_from_directory(VOICE_DIR, filename)
    except Exception as e:
        logger.error(f"âŒ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«é…ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
        return Response("File not found", status=404)

@app.route('/health', methods=['GET'])
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return jsonify({
        'status': 'ok',
        'voicevox': VOICEVOX_ENABLED,
        'groq': groq_client is not None,
        'gemini': gemini_model is not None,
        'weather_api': WEATHER_API_KEY is not None
    })

# ==============================================================================
# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
# ==============================================================================
def run_scheduler():
    """å®šæœŸå®Ÿè¡Œã‚¿ã‚¹ã‚¯"""
    # èµ·å‹•æ™‚ã«ä¸€åº¦å®Ÿè¡Œ
    fetch_and_store_news()
    cleanup_old_news()
    
    while True:
        try:
            schedule.run_pending()
        except Exception as e:
            logger.error(f"âŒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        time.sleep(60)

# ==============================================================================
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–
# ==============================================================================
def initialize_app():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–"""
    global engine, Session, groq_client, gemini_model, VOICEVOX_ENABLED
    
    logger.info("=" * 60)
    logger.info("ğŸ”§ ã‚‚ã¡ã“AI v21.0 åˆæœŸåŒ–é–‹å§‹...")
    logger.info("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
    if DATABASE_URL.startswith('sqlite'):
        engine = create_engine(DATABASE_URL, connect_args={'check_same_thread': False}, pool_pre_ping=True)
    else:
        engine = create_engine(DATABASE_URL, poolclass=pool.QueuePool, pool_size=5, max_overflow=10, pool_pre_ping=True, pool_recycle=3600)
    
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº†")
    
    # AI APIåˆæœŸåŒ–
    if GROQ_API_KEY:
        groq_client = Groq(api_key=GROQ_API_KEY)
        logger.info("âœ… Groq (Llama) APIåˆæœŸåŒ–å®Œäº†")
    else:
        logger.warning("âš ï¸ GROQ_API_KEYæœªè¨­å®š")
    
    if GEMINI_API_KEY:
        genai.configure(api_key=GEMINI_API_KEY)
        gemini_model = genai.GenerativeModel('gemini-1.5-flash')
        logger.info("âœ… Gemini APIåˆæœŸåŒ–å®Œäº†")
    else:
        logger.warning("âš ï¸ GEMINI_API_KEYæœªè¨­å®š")
        
    if not WEATHER_API_KEY:
        logger.warning("âš ï¸ WEATHER_API_KEYæœªè¨­å®š")
    else:
        logger.info("âœ… Weather APIã‚­ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")

    # VOICEVOXåˆæœŸåŒ–
    voicevox_url = find_active_voicevox_url()
    if voicevox_url:
        VOICEVOX_ENABLED = True
        logger.info(f"âœ… VOICEVOXæœ‰åŠ¹åŒ–: {voicevox_url}")
    else:
        logger.info("â„¹ï¸ VOICEVOXç„¡åŠ¹ï¼ˆã‚¨ãƒ³ã‚¸ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰")
    
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
    schedule.every(1).hours.do(search_context_cache.cleanup_expired)
    schedule.every(1).hours.do(fetch_and_store_news)
    schedule.every(1).days.at("03:00").do(cleanup_old_news) # JST noon
    
    threading.Thread(target=run_scheduler, daemon=True).start()
    logger.info("âœ… ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼èµ·å‹•")
    
    logger.info("=" * 60)
    logger.info("âœ… ã‚‚ã¡ã“AI v21.0 åˆæœŸåŒ–å®Œäº†ï¼")
    logger.info("=" * 60)

# ==============================================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ==============================================================================
try:
    initialize_app()
    application = app
except Exception as e:
    logger.critical(f"ğŸ”¥ è‡´å‘½çš„ãªåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
    sys.exit(1)

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
