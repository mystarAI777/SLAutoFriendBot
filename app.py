import sys
import os
import requests
import logging
import time
import threading
import json
import re
import sqlite3
import random
import uuid
import hashlib
from datetime import datetime, timedelta, timezone
import unicodedata
from groq import Groq

# å‹ãƒ’ãƒ³ãƒˆç”¨ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼å¯¾ç­–ä»˜ãï¼‰
try:
    from typing import Union, Dict, Any, List, Optional
except ImportError:
    # å‹ãƒ’ãƒ³ãƒˆãŒä½¿ãˆãªã„ç’°å¢ƒç”¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    Dict = dict
    Any = object
    List = list
    Union = object
    Optional = object

from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Index, Text, Boolean, inspect, text
from sqlalchemy.orm import declarative_base, sessionmaker
from urllib.parse import quote_plus, urljoin
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import asyncio
from threading import Lock
import schedule
import signal

# --- åŸºæœ¬è¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- å®šæ•°è¨­å®š ---
VOICE_DIR = '/tmp/voices'
SERVER_URL = "https://slautofriendbot.onrender.com"
VOICEVOX_SPEAKER_ID = 20  # ã‚‚ã¡å­ã•ã‚“(ãƒãƒ¼ãƒãƒ«) ã«çµ±åˆ
HOLOLIVE_NEWS_URL = "https://hololive-tsuushin.com/category/holonews/"

SL_SAFE_CHAR_LIMIT = 300      # Second Lifeå®‰å…¨æ–‡å­—æ•°åˆ¶é™
VOICE_OPTIMAL_LENGTH = 150    # VOICEVOXæœ€é©æ–‡å­—æ•°

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
]
LOCATION_CODES = {
    "æ±äº¬": "130000", "å¤§é˜ª": "270000", "åå¤å±‹": "230000", "ç¦å²¡": "400000", "æœ­å¹Œ": "016000"
}
SPECIALIZED_SITES = {
    'Blender': {
        'base_url': 'https://docs.blender.org/manual/ja/latest/',
        'keywords': ['Blender', 'ãƒ–ãƒ¬ãƒ³ãƒ€ãƒ¼', 'blender', 'BLENDER']
    },
    'CGãƒ‹ãƒ¥ãƒ¼ã‚¹': {
        'base_url': 'https://modelinghappy.com/',
        'keywords': ['CGãƒ‹ãƒ¥ãƒ¼ã‚¹', '3DCG', 'CG', 'ï¼£ï¼§', 'ï½ƒï½‡', 'cg', '3dcg', 'ï¼“ï¼¤ï¼£ï¼§', 'CGæ¥­ç•Œ', 'CGã‚¢ãƒ‹ãƒ¡']
    },
    'è„³ç§‘å­¦ãƒ»å¿ƒç†å­¦': {
        'base_url': 'https://nazology.kusuguru.co.jp/',
        'keywords': ['è„³ç§‘å­¦', 'å¿ƒç†å­¦', 'è„³', 'å¿ƒç†', 'ã®ã†ã‹ãŒã', 'ã—ã‚“ã‚ŠãŒã']
    },
    'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•': {
        'base_url': 'https://community.secondlife.com/news/',
        'keywords': ['ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•', 'Second Life', 'SL', 'second life', 'ã‚»ã‚«ãƒ³ãƒ‰', 'SecondLife']
    },
    'ã‚¢ãƒ‹ãƒ¡': {
    'base_url': 'https://animedb.jp/',
    'keywords': ['ã‚¢ãƒ‹ãƒ¡', 'anime', 'ANIME', 'ï½±ï¾†ï¾’', 'ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³', 'ä½œç”»', 'å£°å„ª', 'OP', 'ED']
}
}
HOLOMEM_KEYWORDS = [
    'ã¨ãã®ãã‚‰', 'ãƒ­ãƒœå­ã•ã‚“', 'ã•ãã‚‰ã¿ã“', 'æ˜Ÿè¡—ã™ã„ã›ã„', 'AZKi', 'å¤œç©ºãƒ¡ãƒ«',
    'ã‚¢ã‚­ãƒ»ãƒ­ãƒ¼ã‚¼ãƒ³ã‚¿ãƒ¼ãƒ«', 'èµ¤äº•ã¯ã‚ã¨', 'ç™½ä¸Šãƒ•ãƒ–ã‚­', 'å¤è‰²ã¾ã¤ã‚Š', 'æ¹Šã‚ãã‚',
    'ç´«å’²ã‚·ã‚ªãƒ³', 'ç™¾é¬¼ã‚ã‚„ã‚', 'ç™’æœˆã¡ã‚‡ã“', 'å¤§ç©ºã‚¹ãƒãƒ«', 'å¤§ç¥ãƒŸã‚ª', 'çŒ«åˆãŠã‹ã‚†',
    'æˆŒç¥ã“ã‚ã­', 'å…ç”°ãºã“ã‚‰', 'ä¸çŸ¥ç«ãƒ•ãƒ¬ã‚¢', 'ç™½éŠ€ãƒã‚¨ãƒ«', 'å®é˜ãƒãƒªãƒ³', 'å¤©éŸ³ã‹ãªãŸ',
    'è§’å·»ã‚ãŸã‚', 'å¸¸é—‡ãƒˆãƒ¯', 'å§«æ£®ãƒ«ãƒ¼ãƒŠ', 'é›ªèŠ±ãƒ©ãƒŸã‚£', 'æ¡ƒéˆ´ã­ã­', 'ç…ç™½ã¼ãŸã‚“',
    'å°¾ä¸¸ãƒãƒ«ã‚«', 'ãƒ©ãƒ—ãƒ©ã‚¹ãƒ»ãƒ€ãƒ¼ã‚¯ãƒã‚¹', 'é·¹å¶ºãƒ«ã‚¤', 'åšè¡£ã“ã‚ˆã‚Š', 'æ²™èŠ±å‰ã‚¯ãƒ­ãƒ±',
    'é¢¨çœŸã„ã‚ã¯', 'æ£®ã‚«ãƒªã‚ªãƒš', 'å°é³¥éŠã‚­ã‚¢ãƒ©', 'ä¸€ä¼Šé‚£å°“æ –', 'ãŒã†ã‚‹ãƒ»ãã‚‰',
    'ãƒ¯ãƒˆã‚½ãƒ³ãƒ»ã‚¢ãƒ¡ãƒªã‚¢', 'IRyS', 'ã‚»ãƒ¬ã‚¹ãƒ»ãƒ•ã‚¡ã‚¦ãƒŠ', 'ã‚ªãƒ¼ãƒ­ãƒ»ã‚¯ãƒ­ãƒ‹ãƒ¼', 'ä¸ƒè©©ãƒ ãƒ¡ã‚¤',
    'ãƒã‚³ã‚¹ãƒ»ãƒ™ãƒ¼ãƒ«ã‚º', 'ã‚·ã‚ªãƒªãƒ»ãƒãƒ´ã‚§ãƒ©', 'å¤çŸ³ãƒ“ã‚¸ãƒ¥ãƒ¼', 'ãƒãƒªãƒƒã‚µãƒ»ãƒ¬ã‚¤ãƒ´ãƒ³ã‚¯ãƒ­ãƒ•ãƒˆ',
    'ãƒ•ãƒ¯ãƒ¯ãƒ»ã‚¢ãƒ“ã‚¹ã‚¬ãƒ¼ãƒ‰', 'ãƒ¢ã‚³ã‚³ãƒ»ã‚¢ãƒ“ã‚¹ã‚¬ãƒ¼ãƒ‰', 'ã‚¢ãƒ¦ãƒ³ãƒ€ãƒ»ãƒªã‚¹', 'ãƒ ãƒ¼ãƒŠãƒ»ãƒ›ã‚·ãƒãƒ´ã‚¡',
    'ã‚¢ã‚¤ãƒ©ãƒ‹ãƒ»ã‚¤ã‚ªãƒ•ã‚£ãƒ•ãƒ†ã‚£ãƒ¼ãƒ³', 'ã‚¯ãƒ¬ã‚¤ã‚¸ãƒ¼ãƒ»ã‚ªãƒªãƒ¼', 'ã‚¢ãƒ¼ãƒ‹ãƒ£ãƒ»ãƒ¡ãƒ«ãƒ•ã‚£ãƒƒã‚µ',
    'ãƒ‘ãƒ´ã‚©ãƒªã‚¢ãƒ»ãƒ¬ã‚¤ãƒ', 'ç«å¨é’', 'éŸ³ä¹ƒç€¬å¥', 'ä¸€æ¡è‰ã€…è¯', 'å„’çƒé¢¨äº­ã‚‰ã§ã‚“',
    'è½Ÿã¯ã˜ã‚', 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–', 'ãƒ›ãƒ­ãƒ¡ãƒ³', 'hololive', 'YAGOO'
]
ANIME_KEYWORDS = [
    'ã‚¢ãƒ‹ãƒ¡', 'anime', 'ANIME', 'ï½±ï¾†ï¾’', 'ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³',
    'ä½œç”»', 'å£°å„ª', 'OP', 'ED', 'ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°', 'ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°',
    'åŠ‡å ´ç‰ˆ', 'æ˜ ç”»', 'OVA', 'OAD', 'åŸä½œ', 'æ¼«ç”»', 'ãƒ©ãƒãƒ™',
    'ä¸»äººå…¬', 'ã‚­ãƒ£ãƒ©', 'ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼', 'åˆ¶ä½œä¼šç¤¾', 'ã‚¹ã‚¿ã‚¸ã‚ª'
]
# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° & Executor ---
background_executor = ThreadPoolExecutor(max_workers=5)
groq_client = None
VOICEVOX_ENABLED = True
app = Flask(__name__)
CORS(app)
app.config['JSON_AS_ASCII'] = False

@app.after_request
def after_request(response):
    """CORSãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å…¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«è¿½åŠ """
    response.headers.add('Access-Control-Allow-Origin', '*')
    response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
    response.headers.add('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
    return response

Base = declarative_base()

# --- ç§˜å¯†æƒ…å ±/ç’°å¢ƒå¤‰æ•° èª­ã¿è¾¼ã¿ ---
def get_secret(name):
    """
    ã¾ãšRenderã®Secret Fileã‹ã‚‰ç§˜å¯†æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã€è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚
    """
    secret_file_path = f"/etc/secrets/{name}"
    if os.path.exists(secret_file_path):
        try:
            with open(secret_file_path, 'r') as f:
                logger.info(f"âœ… Secret Fileã‹ã‚‰ {name} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
                return f.read().strip()
        except IOError as e:
            logger.error(f"âŒ Secret File {secret_file_path} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return None
    
    # Secret FileãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ç’°å¢ƒå¤‰æ•°ã‚’ãƒã‚§ãƒƒã‚¯
    value = os.environ.get(name)
    if value:
         logger.info(f"âœ… ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ {name} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    return value

def ensure_voice_directory():
    """éŸ³å£°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ã‚’ä¿è¨¼"""
    max_attempts = 3
    for attempt in range(max_attempts):
        try:
            if not os.path.exists(VOICE_DIR):
                os.makedirs(VOICE_DIR, mode=0o755, exist_ok=True)
                logger.info(f"âœ… Voice directory created: {VOICE_DIR}")
            
            # æ›¸ãè¾¼ã¿æ¨©é™ã‚’ç¢ºèª
            if os.access(VOICE_DIR, os.W_OK):
                logger.info(f"âœ… Voice directory is writable: {VOICE_DIR}")
                return True
            else:
                # æ¨©é™ã‚’ä¿®æ­£
                os.chmod(VOICE_DIR, 0o755)
                logger.info(f"âœ… Voice directory permissions fixed: {VOICE_DIR}")
                return True
                
        except Exception as e:
            logger.error(f"âŒ Voice directory creation failed (attempt {attempt + 1}/{max_attempts}): {e}")
            if attempt < max_attempts - 1:
                time.sleep(1)
            continue
    
    logger.critical(f"ğŸ”¥ Failed to create voice directory after {max_attempts} attempts")
    return False
    
DATABASE_URL = get_secret('DATABASE_URL') or 'sqlite:///./test.db'
GROQ_API_KEY = get_secret('GROQ_API_KEY')
VOICEVOX_URL_FROM_ENV = get_secret('VOICEVOX_URL')

# --- Hololive Wikiæ¤œç´¢æ©Ÿèƒ½ã®è¿½åŠ  ---
def search_hololive_wiki(member_name, query_topic):
    """
    Seesaawikiã®ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–Wikiã‹ã‚‰æƒ…å ±ã‚’æ¤œç´¢ã™ã‚‹ã€‚
    ãƒ¡ãƒ³ãƒãƒ¼åã¨ç‰¹å®šã®ãƒˆãƒ”ãƒƒã‚¯ã‚’çµ„ã¿åˆã‚ã›ã¦æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã€‚
    """
    base_url = "https://seesaawiki.jp/hololivetv/"
    search_query = f"{member_name} {query_topic}"
    encoded_query = quote_plus(search_query.encode('euc-jp')) # Seesaawikiã¯EUC-JPãŒå¤šã„
    search_url = f"{base_url}search?query={encoded_query}"
    
    try:
        logger.info(f"ğŸ” Searching Hololive Wiki for: {search_query} at {search_url}")
        response = requests.get(
            search_url,
            headers={'User-Agent': random.choice(USER_AGENTS)},
            timeout=15,
            allow_redirects=True
        )
        # Seesaawikiã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«åˆã‚ã›ã¦ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’è©¦ã¿ã‚‹
        response.encoding = 'euc-jp'
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã‹ã‚‰é–¢é€£æ€§ã®é«˜ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¢ã™
        # ãƒšãƒ¼ã‚¸å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã—ã€é–¢é€£éƒ¨åˆ†ã‚’æŠ½å‡ºã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        
        # ã¾ãšã€ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ã‚’ç‰¹å®š
        main_content_div = soup.find('div', id='pagebody') or soup.find('div', class_='contents')
        if not main_content_div:
            logger.warning("Hololive Wiki: Could not find main content div.")
            return None

        # é–¢é€£æ€§ã®é«˜ã„æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
        page_text = clean_text(main_content_div.get_text())
        
        # ãƒ¡ãƒ³ãƒãƒ¼åã¨ãƒˆãƒ”ãƒƒã‚¯ã‚’å«ã‚€å‘¨è¾ºã®æ–‡ç« ã‚’æŠ½å‡ºã™ã‚‹
        # ä¾‹: ã€Œã•ãã‚‰ã¿ã“ã€ã¨ã€Œãƒã‚¤ã‚¯ãƒ©ã€ã§æ¤œç´¢ã—ãŸå ´åˆã€ã€Œã•ãã‚‰ã¿ã“ã¯ãƒã‚¤ã‚¯ãƒ©ã§ç‹¬ç‰¹ã®å»ºç¯‰ã‚’ã™ã‚‹ã€ã®ã‚ˆã†ãªæ–‡ç« 
        
        # ç°¡æ˜“çš„ãªè¦ç´„ç”Ÿæˆ
        # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹æ–‡ã‚’ã„ãã¤ã‹æŠ½å‡º
        sentences = re.split(r'(ã€‚|ï¼|\n)', page_text)
        relevant_sentences = []
        for i in range(0, len(sentences), 2):
            sentence = sentences[i]
            if member_name in sentence and query_topic in sentence:
                relevant_sentences.append(sentence.strip())
            if len(" ".join(relevant_sentences)) > 500: # ã‚ã‚‹ç¨‹åº¦ã®é•·ã•ã«é”ã—ãŸã‚‰çµ‚äº†
                break

        if relevant_sentences:
            extracted_info = " ".join(relevant_sentences)[:1000] # æœ€å¤§1000æ–‡å­—
            logger.info(f"âœ… Hololive Wiki search successful for '{search_query}'. Extracted: {extracted_info[:100]}")
            return extracted_info
        
        logger.info(f"â„¹ï¸ Hololive Wiki search for '{search_query}' found no direct relevant sentences. Attempting general summary.")
        # é–¢é€£æ–‡ç« ãŒè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã€ãƒšãƒ¼ã‚¸ã®æœ€åˆã®éƒ¨åˆ†ã‚’è¦ç´„ã¨ã—ã¦è¿”ã™
        return page_text[:500] if page_text else None
        
    except requests.exceptions.Timeout:
        logger.warning(f"âš ï¸ Hololive Wiki search timeout for {search_query}")
        return None
    except requests.exceptions.RequestException as e:
        logger.warning(f"âš ï¸ Hololive Wiki search request error for {search_query}: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ Hololive Wiki search general error for {search_query}: {e}", exc_info=True)
        return None

# --- ã‚¢ãƒ‹ãƒ¡æ¤œç´¢æ©Ÿèƒ½ ---
def is_anime_request(message):
    """ã‚¢ãƒ‹ãƒ¡é–¢é€£ã®è³ªå•ã‹ã©ã†ã‹åˆ¤å®š"""
    message_normalized = unicodedata.normalize('NFKC', message).lower()
    
    # ã‚¢ãƒ‹ãƒ¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
    for keyword in ANIME_KEYWORDS:
        keyword_normalized = unicodedata.normalize('NFKC', keyword).lower()
        if keyword_normalized in message_normalized:
            return True
    
    # ã€Œã€œã£ã¦ã‚¢ãƒ‹ãƒ¡ã€ã€Œã€œã¨ã„ã†ã‚¢ãƒ‹ãƒ¡ã€ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    anime_patterns = [
        r'ã£ã¦ã‚¢ãƒ‹ãƒ¡', r'ã¨ã„ã†ã‚¢ãƒ‹ãƒ¡', r'ã®ã‚¢ãƒ‹ãƒ¡',
        r'ã‚¢ãƒ‹ãƒ¡ã§', r'ã‚¢ãƒ‹ãƒ¡ã®', r'ã‚¢ãƒ‹ãƒ¡ã¯'
    ]
    for pattern in anime_patterns:
        if re.search(pattern, message):
            return True
    
    return False


def search_anime_database(query, is_detailed=False):
    """
    https://animedb.jp/ ã‹ã‚‰ã‚¢ãƒ‹ãƒ¡æƒ…å ±ã‚’æ¤œç´¢
    """
    base_url = "https://animedb.jp/"
    
    try:
        logger.info(f"ğŸ¬ Searching anime database for: {query}")
        
        # Step 1: æ¤œç´¢ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
        search_url = f"{base_url}search?q={quote_plus(query)}"
        response = requests.get(
            search_url,
            headers={'User-Agent': random.choice(USER_AGENTS)},
            timeout=15,
            allow_redirects=True
        )
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Step 2: æ¤œç´¢çµæœã‚’è§£æ
        # animedb.jpã®æ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ï¼ˆå®Ÿéš›ã®HTMLæ§‹é€ ã‚’ç¢ºèªã—ã¦ä¿®æ­£ã—ã¦ãã ã•ã„ï¼‰
        results = []
        
        # æ¤œç´¢çµæœã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè¤‡æ•°è©¦è¡Œï¼‰
        result_selectors = [
            'div.anime-item',
            'div.search-result',
            'article.anime',
            'div[class*="anime"]',
            'li.anime-list-item'
        ]
        
        result_elements = []
        for selector in result_selectors:
            result_elements = soup.select(selector)
            if result_elements:
                logger.info(f"âœ… Found results with selector: {selector}")
                break
        
        if not result_elements:
            # ã‚»ãƒ¬ã‚¯ã‚¿ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€å…¨ä½“ã‹ã‚‰æƒ…å ±æŠ½å‡ºã‚’è©¦ã¿ã‚‹
            logger.warning("âš ï¸ No specific selectors found, trying general extraction")
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚ã‚‰ã™ã˜ã‚’å«ã‚€divã‚’æ¢ã™
            potential_results = soup.find_all(['div', 'article'], limit=10)
            result_elements = [elem for elem in potential_results if elem.find(['h2', 'h3', 'h4'])]
        
        for elem in result_elements[:3 if is_detailed else 2]:
            # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
            title_elem = elem.find(['h2', 'h3', 'h4', 'a'])
            if not title_elem:
                continue
            
            title = clean_text(title_elem.get_text())
            
            # ã‚ã‚‰ã™ã˜/èª¬æ˜æŠ½å‡º
            description_elem = elem.find(['p', 'div'], class_=lambda x: x and ('description' in x.lower() or 'summary' in x.lower()))
            if not description_elem:
                description_elem = elem.find('p')
            
            description = clean_text(description_elem.get_text()) if description_elem else ""
            
            # ãƒªãƒ³ã‚¯æŠ½å‡º
            link_elem = elem.find('a', href=True)
            link = urljoin(base_url, link_elem['href']) if link_elem else ""
            
            if title and len(title) > 2:
                results.append({
                    'title': title,
                    'description': description[:300] if description else "è©³ç´°æƒ…å ±ãªã—",
                    'url': link
                })
        
        if not results:
            logger.warning(f"âš ï¸ No anime results found for: {query}")
            return None
        
        # Step 3: çµæœã‚’æ•´å½¢
        formatted_results = []
        for i, result in enumerate(results, 1):
            formatted_results.append(
                f"ã€{i}ã€‘{result['title']}\n"
                f"{result['description'][:150]}..."
            )
        
        summary = "\n\n".join(formatted_results)
        logger.info(f"âœ… Anime search successful: {len(results)} results")
        
        return summary
        
    except requests.exceptions.Timeout:
        logger.error(f"âŒ Anime search timeout for: {query}")
        return None
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Anime search request error: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ Anime search general error: {e}", exc_info=True)
        return None


# --- å¿ƒç†åˆ†ææ©Ÿèƒ½ ---
def analyze_user_psychology(user_uuid):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éå»ã®ä¼šè©±å±¥æ­´ã‹ã‚‰å¿ƒç†åˆ†æã‚’å®Ÿè¡Œ
    """
    session = Session()
    try:
        logger.info(f"ğŸ§  Starting psychology analysis for user: {user_uuid}")
        
        # Step 1: ä¼šè©±å±¥æ­´ã‚’å–å¾—ï¼ˆæœ€æ–°100ä»¶ï¼‰
        conversations = session.query(ConversationHistory).filter_by(
            user_uuid=user_uuid,
            role='user'  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã®ã¿
        ).order_by(ConversationHistory.timestamp.desc()).limit(100).all()
        
        if len(conversations) < 10:
            logger.warning(f"âš ï¸ Not enough conversation data for analysis: {len(conversations)} messages")
            return None
        
        # Step 2: ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        messages_text = "\n".join([conv.content for conv in reversed(conversations)])
        total_messages = len(conversations)
        avg_length = sum(len(conv.content) for conv in conversations) // total_messages
        
        # Step 3: ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
        user_memory = session.query(UserMemory).filter_by(user_uuid=user_uuid).first()
        user_name = user_memory.user_name if user_memory else "ä¸æ˜"
        
        # Step 4: AI ã«ã‚ˆã‚‹å¿ƒç†åˆ†æ
        if not groq_client:
            logger.warning("âš ï¸ Groq client unavailable, skipping AI analysis")
            return None
        
        analysis_prompt = f"""ã‚ãªãŸã¯å¿ƒç†å­¦ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€Œ{user_name}ã€ã•ã‚“ã®éå»ã®ä¼šè©±ï¼ˆ{total_messages}ä»¶ï¼‰ã‚’åˆ†æã—ã€å¿ƒç†ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€ä¼šè©±å±¥æ­´ã€‘
{messages_text[:3000]}

ã€åˆ†æé …ç›®ã€‘
1. **ãƒ“ãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ–æ€§æ ¼ç‰¹æ€§**ï¼ˆå„0-100ç‚¹ã§è©•ä¾¡ï¼‰
   - é–‹æ”¾æ€§ï¼ˆOpennessï¼‰: æ–°ã—ã„çµŒé¨“ã¸ã®èˆˆå‘³ã€å‰µé€ æ€§
   - èª å®Ÿæ€§ï¼ˆConscientiousnessï¼‰: è¨ˆç”»æ€§ã€è²¬ä»»æ„Ÿ
   - å¤–å‘æ€§ï¼ˆExtraversionï¼‰: ç¤¾äº¤æ€§ã€æ´»ç™ºã•
   - å”èª¿æ€§ï¼ˆAgreeablenessï¼‰: å„ªã—ã•ã€å”åŠ›çš„
   - ç¥çµŒç—‡å‚¾å‘ï¼ˆNeuroticismï¼‰: ä¸å®‰ã€æ„Ÿæƒ…ã®å®‰å®šæ€§

2. **èˆˆå‘³ãƒ»é–¢å¿ƒ**ï¼ˆä¸»è¦ãªèˆˆå‘³åˆ†é‡ã¨ãã®å¼·åº¦ï¼‰

3. **ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«**ï¼ˆã‚«ã‚¸ãƒ¥ã‚¢ãƒ«/ä¸å¯§/ç†±å¿ƒãªã©ï¼‰

4. **æ„Ÿæƒ…å‚¾å‘**ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«/æ„Ÿæƒ…è±Šã‹ãªã©ï¼‰

5. **ã‚ˆãè©±ã™è©±é¡Œãƒˆãƒƒãƒ—3**

6. **ç·åˆçš„ãªäººç‰©åƒã®è¦ç´„**ï¼ˆ200æ–‡å­—ç¨‹åº¦ï¼‰

**é‡è¦**: ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆä»–ã®æ–‡ç« ã¯ä¸è¦ï¼‰:
{{
  "openness": 75,
  "conscientiousness": 60,
  "extraversion": 80,
  "agreeableness": 70,
  "neuroticism": 40,
  "interests": {{"ã‚¢ãƒ‹ãƒ¡": 90, "ã‚²ãƒ¼ãƒ ": 70, "éŸ³æ¥½": 60}},
  "conversation_style": "ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã§è¦ªã—ã¿ã‚„ã™ã„",
  "emotional_tendency": "ãƒã‚¸ãƒ†ã‚£ãƒ–ã§æ˜ã‚‹ã„",
  "favorite_topics": ["ã‚¢ãƒ‹ãƒ¡", "æ—¥å¸¸ã®å‡ºæ¥äº‹", "è¶£å‘³"],
  "summary": "æ˜ã‚‹ãç¤¾äº¤çš„ãªæ€§æ ¼ã§ã€ã‚¢ãƒ‹ãƒ¡ã‚„å‰µä½œæ´»å‹•ã«å¼·ã„èˆˆå‘³ã‚’æŒã¤ã€‚ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªä¼šè©±ã‚’å¥½ã¿ã€æ„Ÿæƒ…è¡¨ç¾ãŒè±Šã‹ã€‚æ–°ã—ã„ã“ã¨ã¸ã®å¥½å¥‡å¿ƒãŒæ—ºç››ã€‚",
  "confidence": 85
}}"""

        try:
            completion = groq_client.chat.completions.create(
                messages=[{"role": "user", "content": analysis_prompt}],
                model="llama-3.1-8b-instant",
                temperature=0.3,  # åˆ†æã¯æ­£ç¢ºæ€§é‡è¦–
                max_tokens=800
            )
            
            response_text = completion.choices[0].message.content.strip()
            
            # JSONã‚’æŠ½å‡ºï¼ˆ```json ... ``` ã®å ´åˆã«å¯¾å¿œï¼‰
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(1)
            
            # JSON ãƒ‘ãƒ¼ã‚¹
            analysis_data = json.loads(response_text)
            
            logger.info(f"âœ… AI analysis completed for user: {user_uuid}")
            
            # Step 5: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            psychology = session.query(UserPsychology).filter_
