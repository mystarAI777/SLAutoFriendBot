# ==============================================================================
# Mochiko AI - æœ€çµ‚çµ±åˆãƒ»å®Œå…¨ç‰ˆ
# ä½œæˆæ—¥: 2025/11/11
# ã“ã‚Œã¾ã§ã®å¯¾è©±ã®å…¨ã¦ã®æ”¹å–„ç‚¹ã‚’çµ±åˆã—ãŸã€å®Œæˆç‰ˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
# ==============================================================================

# ==============================================================================
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ==============================================================================
import sys
import os
import requests
import logging
import time
import threading
import json
import re
import random
import uuid
import hashlib
from datetime import datetime, timedelta, timezone
import unicodedata

# --- ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ---
from groq import Groq
from flask import Flask, request, jsonify, send_from_directory
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Text, BigInteger, Boolean
from sqlalchemy.orm import declarative_base, sessionmaker
from sqlalchemy.sql import text
from urllib.parse import quote_plus, urljoin
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import schedule
import signal

# ==============================================================================
# åŸºæœ¬è¨­å®šã¨ãƒ­ã‚®ãƒ³ã‚°
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ==============================================================================
# å®šæ•°è¨­å®š
# ==============================================================================
VOICE_DIR = '/tmp/voices'
SERVER_URL = "https://slautofriendbot.onrender.com" # ã”è‡ªèº«ã®Renderã‚µãƒ¼ãƒãƒ¼URLã«è¨­å®š
VOICEVOX_SPEAKER_ID = 20 # ã‚‚ã¡å­ã•ã‚“(ãƒãƒ¼ãƒãƒ«)

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
]
SPECIALIZED_SITES = {
    'Blender': {'base_url': 'https://docs.blender.org/manual/ja/latest/', 'keywords': ['Blender', 'ãƒ–ãƒ¬ãƒ³ãƒ€ãƒ¼']},
    'CGãƒ‹ãƒ¥ãƒ¼ã‚¹': {'base_url': 'https://modelinghappy.com/', 'keywords': ['CGãƒ‹ãƒ¥ãƒ¼ã‚¹', '3DCG', 'CG']},
    'è„³ç§‘å­¦ãƒ»å¿ƒç†å­¦': {'base_url': 'https://nazology.kusuguru.co.jp/', 'keywords': ['è„³ç§‘å­¦', 'å¿ƒç†å­¦', 'è„³', 'å¿ƒç†']},
    'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•': {'base_url': 'https://community.secondlife.com/news/', 'keywords': ['ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•', 'Second Life', 'SL']},
    'ã‚¢ãƒ‹ãƒ¡': {'base_url': 'https://animedb.jp/', 'keywords': ['ã‚¢ãƒ‹ãƒ¡', 'anime']}
}
ANIME_KEYWORDS = ['ã‚¢ãƒ‹ãƒ¡', 'anime', 'ä½œç”»', 'å£°å„ª', 'OP', 'ED', 'åŠ‡å ´ç‰ˆ', 'åŸä½œ', 'ä¸»äººå…¬', 'ã‚­ãƒ£ãƒ©']

# ==============================================================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
# ==============================================================================
background_executor = ThreadPoolExecutor(max_workers=5)
groq_client = None
engine = None
Session = None
app = Flask(__name__)
Base = declarative_base()
DYNAMIC_HOLOMEM_KEYWORDS = []

# ==============================================================================
# ç§˜å¯†æƒ…å ±/ç’°å¢ƒå¤‰æ•° èª­ã¿è¾¼ã¿
# ==============================================================================
def get_secret(name):
    secret_file_path = f"/etc/secrets/{name}"
    if os.path.exists(secret_file_path):
        try:
            with open(secret_file_path, 'r') as f: return f.read().strip()
        except IOError: return None
    return os.environ.get(name)

DATABASE_URL = get_secret('DATABASE_URL') or 'sqlite:///./mochiko.db'
GROQ_API_KEY = get_secret('GROQ_API_KEY')
VOICEVOX_URL_FROM_ENV = get_secret('VOICEVOX_URL')

# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å®šç¾©
# ==============================================================================
class HolomemWiki(Base): __tablename__ = 'holomem_wiki'; id = Column(Integer, primary_key=True); member_name = Column(String(100), nullable=False, unique=True, index=True); description = Column(Text); generation = Column(String(100)); status = Column(String(50), default='ç¾å½¹', nullable=False); status_reason = Column(Text, nullable=True); mochiko_feeling = Column(Text, nullable=True); graduation_date = Column(String(100), nullable=True); last_updated = Column(DateTime, default=datetime.utcnow)
class UserMemory(Base): __tablename__ = 'user_memories'; id = Column(Integer, primary_key=True); user_uuid = Column(String(255), unique=True, nullable=False); user_name = Column(String(255), nullable=False); interaction_count = Column(BigInteger, default=0); last_interaction = Column(DateTime, default=datetime.utcnow)
class ConversationHistory(Base): __tablename__ = 'conversation_history'; id = Column(Integer, primary_key=True, autoincrement=True); user_uuid = Column(String(255), nullable=False, index=True); role = Column(String(10), nullable=False); content = Column(Text, nullable=False); timestamp = Column(DateTime, default=datetime.utcnow, index=True)
class HololiveNews(Base): __tablename__ = 'hololive_news'; id = Column(Integer, primary_key=True); title = Column(String(500), nullable=False); content = Column(Text, nullable=False); url = Column(String(1000)); created_at = Column(DateTime, default=datetime.utcnow, index=True); news_hash = Column(String(100), unique=True)
class SpecializedNews(Base): __tablename__ = 'specialized_news'; id = Column(Integer, primary_key=True); site_name = Column(String(100), nullable=False, index=True); title = Column(String(500), nullable=False); content = Column(Text, nullable=False); url = Column(String(1000)); created_at = Column(DateTime, default=datetime.utcnow, index=True); news_hash = Column(String(100), unique=True)
class BackgroundTask(Base): __tablename__ = 'background_tasks'; id = Column(Integer, primary_key=True); task_id = Column(String(255), unique=True, nullable=False); user_uuid = Column(String(255), nullable=False); task_type = Column(String(50), nullable=False); query = Column(Text, nullable=False); result = Column(Text); status = Column(String(20), default='pending'); created_at = Column(DateTime, default=datetime.utcnow); completed_at = Column(DateTime)
class FriendRegistration(Base): __tablename__ = 'friend_registrations'; id = Column(Integer, primary_key=True); user_uuid = Column(String(255), nullable=False, index=True); friend_uuid = Column(String(255), nullable=False); friend_name = Column(String(255), nullable=False); registered_at = Column(DateTime, default=datetime.utcnow)
class UserPsychology(Base): __tablename__ = 'user_psychology'; id = Column(Integer, primary_key=True); user_uuid = Column(String(255), unique=True, nullable=False, index=True); user_name = Column(String(255), nullable=False); interests = Column(Text); favorite_topics = Column(Text); conversation_style = Column(String(50)); analysis_summary = Column(Text); last_analyzed = Column(DateTime, default=datetime.utcnow); analysis_confidence = Column(Integer, default=0)

# ==============================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ & ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# ==============================================================================
def clean_text(text): return re.sub(r'\s+', ' ', text).strip() if text else ""
def create_news_hash(title, content): return hashlib.md5(f"{title}{content[:100]}".encode('utf-8')).hexdigest()
def is_anime_request(message): return any(keyword in message.lower() for keyword in ANIME_KEYWORDS)
def detect_specialized_topic(message):
    for topic, config in SPECIALIZED_SITES.items():
        if any(kw in message for kw in config['keywords']): return topic
    return None
def is_friend_request(message): return any(f in message for f in ['å‹ã ã¡', 'å‹é”', 'ãƒ•ãƒ¬ãƒ³ãƒ‰']) and any(a in message for a in ['ç™»éŒ²', 'æ•™ãˆã¦', 'èª°', 'ãƒªã‚¹ãƒˆ'])
def is_explicit_search_request(message): return any(keyword in message for keyword in ['èª¿ã¹ã¦', 'æ¤œç´¢ã—ã¦', 'æ¢ã—ã¦', 'WEBæ¤œç´¢', 'ã‚°ã‚°ã£ã¦'])
def is_hololive_request(message): return any(keyword in message for keyword in DYNAMIC_HOLOMEM_KEYWORDS)
def is_time_request(message): return any(keyword in message for keyword in ['ä»Šä½•æ™‚', 'æ™‚é–“', 'æ™‚åˆ»', 'ä½•æ™‚'])
def is_weather_request(message): return any(keyword in message for keyword in ['å¤©æ°—'])
def is_short_response(message): return len(message.strip()) <= 3 or message.strip() in ['ã†ã‚“', 'ãã†', 'ã¯ã„', 'ãã£ã‹', 'ãªã‚‹ã»ã©']
def get_or_create_user(session, uuid, name):
    user = session.query(UserMemory).filter_by(user_uuid=uuid).first()
    if user:
        user.interaction_count = (user.interaction_count or 0) + 1
        user.last_interaction = datetime.utcnow()
        if user.user_name != name: user.user_name = name
    else:
        user = UserMemory(user_uuid=uuid, user_name=name, interaction_count=1)
        session.add(user)
    session.commit()
    return {'uuid': user.user_uuid, 'name': user.user_name}
def get_conversation_history(session, uuid, limit=6):
    return session.query(ConversationHistory).filter_by(user_uuid=uuid).order_by(ConversationHistory.timestamp.desc()).limit(limit).all()

def get_sakuramiko_special_responses():
    return {
        'ã«ã‡': 'ã•ãã‚‰ã¿ã“ã¡ã‚ƒã‚“ã®ã€Œã«ã‡ã€ã€ã¾ã˜ã‹ã‚ã„ã„ã‚ˆã­!ã‚ã®ç‹¬ç‰¹ãªå£ç™–ãŒã‚¨ãƒªãƒ¼ãƒˆã®è¨¼ãªã‚“ã ã£ã¦ã€œ',
        'ã‚¨ãƒªãƒ¼ãƒˆ': 'ã¿ã“ã¡ã¯è‡ªç§°ã‚¨ãƒªãƒ¼ãƒˆVTuber!ã§ã‚‚å®Ÿéš›ã¯æ„›ã•ã‚Œãƒãƒ³ã‚³ãƒ„ã‚­ãƒ£ãƒ©ã£ã¦æ„Ÿã˜ã§ã€ãã‚ŒãŒã¾ãŸé­…åŠ›çš„ãªã‚“ã ã‚ˆã­ã€œ',
        'ãƒã‚¤ã‚¯ãƒ©': 'ã¿ã“ã¡ã®ãƒã‚¤ã‚¯ãƒ©å»ºç¯‰ã€ç‹¬å‰µçš„ã™ãã¦é¢ç™½ã„ã‚ˆ!ã€Œã¿ã“ã¡å»ºç¯‰ã€ã£ã¦å‘¼ã°ã‚Œã¦ã‚‹ã®çŸ¥ã£ã¦ã‚‹?',
    }

# ==============================================================================
# ä¸­æ ¸æ©Ÿèƒ½: å³æ™‚å¿œç­”ã€æ¤œç´¢ã€DBæ“ä½œã€AIå¿œç­”ç”Ÿæˆ
# ==============================================================================

def get_japan_time():
    now = datetime.now(timezone(timedelta(hours=9)))
    return f"ä»Šã¯{now.hour}æ™‚{now.minute}åˆ†ã ã‚ˆï¼"

def get_weather_forecast(message):
    location = "æ±äº¬"
    for loc_name in ["æ±äº¬", "å¤§é˜ª"]:
        if loc_name in message:
            location = loc_name
            break
    area_code = {"æ±äº¬": "130000", "å¤§é˜ª": "270000"}.get(location, "130000")
    url = f"https://www.jma.go.jp/bosai/forecast/data/overview_forecast/{area_code}.json"
    try:
        response = requests.get(url, timeout=10)
        text = clean_text(response.json().get('text', ''))
        return f"ä»Šã®{location}ã®å¤©æ°—ã¯ã­ã€ã€Œ{text}ã€ã£ã¦æ„Ÿã˜ã ã‚ˆï¼" if text else f"{location}ã®å¤©æ°—æƒ…å ±ãŒå–ã‚Œãªã‹ã£ãŸâ€¦"
    except Exception as e:
        logger.error(f"Weather API error for {location}: {e}"); return "å¤©æ°—æƒ…å ±ãŒã†ã¾ãå–ã‚Œãªã‹ã£ãŸã¿ãŸã„â€¦"

def scrape_major_search_engines(query, num_results):
    search_configs = [
        {'name': 'Bing', 'url': f"https://www.bing.com/search?q={quote_plus(query)}&mkt=ja-JP", 'result_selector': 'li.b_algo', 'title_selector': 'h2', 'snippet_selector': 'div.b_caption p'},
        {'name': 'Yahoo Japan', 'url': f"https://search.yahoo.co.jp/search?p={quote_plus(query)}", 'result_selector': 'div.Algo', 'title_selector': 'h3', 'snippet_selector': 'div.compText p'}
    ]
    for config in search_configs:
        try:
            response = requests.get(config['url'], headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=12)
            results = []
            for elem in BeautifulSoup(response.content, 'html.parser').select(config['result_selector'])[:num_results]:
                title = elem.select_one(config['title_selector'])
                snippet = elem.select_one(config['snippet_selector'])
                if title and snippet: results.append({'title': clean_text(title.get_text()), 'snippet': clean_text(snippet.get_text())})
            if results: return results
        except Exception as e: logger.warning(f"âš ï¸ {config['name']} search error: {e}")
    return []

def deep_web_search(query, is_detailed):
    results = scrape_major_search_engines(query, 3 if is_detailed else 2)
    if not results: return None
    summary_text = "\n".join(f"[æƒ…å ±{i+1}] {res['snippet']}" for i, res in enumerate(results))
    if not groq_client: return f"æ¤œç´¢çµæœ:\n{summary_text}"
    try:
        prompt = f"ä»¥ä¸‹ã®æ¤œç´¢çµæœã‚’ä½¿ã„ã€è³ªå•ã€Œ{query}ã€ã«ã‚®ãƒ£ãƒ«èªã§{'è©³ã—ã' if is_detailed else 'ç°¡æ½”ã«'}ç­”ãˆã¦ï¼š\n{summary_text}"
        completion = groq_client.chat.completions.create(messages=[{"role": "user", "content": prompt}], model="llama-3.1-8b-instant", temperature=0.7, max_tokens=400 if is_detailed else 200)
        return completion.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"AI summarization error: {e}"); return f"æ¤œç´¢çµæœ:\n{summary_text}"

def search_anime_database(query, is_detailed):
    base_url = "https://animedb.jp/"
    try:
        search_url = f"{base_url}search?q={quote_plus(query)}"
        response = requests.get(search_url, headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        results = []
        result_elements = soup.select('div.anime-item, div.search-result, article.anime')[:3]
        for elem in result_elements:
            title_elem = elem.find(['h2', 'h3', 'h4', 'a'])
            description_elem = elem.find('p')
            if title_elem:
                title = clean_text(title_elem.get_text())
                desc = clean_text(description_elem.get_text())[:150] if description_elem else "è©³ç´°æƒ…å ±ãªã—"
                results.append(f"ã€{title}ã€‘\n{desc}...")
        return "\n\n".join(results) if results else None
    except Exception as e:
        logger.error(f"âŒ Anime search error: {e}"); return None

def search_hololive_wiki(query):
    base_url = "https://seesaawiki.jp/hololivetv/"
    try:
        encoded_query = quote_plus(query.encode('euc-jp', errors='replace'))
        search_url = f"{base_url}search?query={encoded_query}"
        logger.info(f"ğŸ“š Searching Hololive Wiki for: {query}")
        response = requests.get(search_url, headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=15)
        response.encoding = 'euc-jp'; response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        first_result_link = soup.select_one('#page-body-inner .search-result-title a')
        if not first_result_link: return None
        article_url = urljoin(base_url, first_result_link['href'])
        article_response = requests.get(article_url, headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=15)
        article_response.encoding = 'euc-jp'; article_response.raise_for_status()
        article_soup = BeautifulSoup(article_response.text, 'html.parser')
        content_div = article_soup.select_one('#page-body-inner')
        if content_div:
            for tag in content_div.select('.social-button, .plugin_menu, script, style'): tag.decompose()
            page_text = clean_text(content_div.get_text(separator='\n', strip=True))
            if page_text and len(page_text) > 50: return f"ã€Œ{query}ã€ã«ã¤ã„ã¦ã€Wikiã«ã¯ã“ã†æ›¸ã‹ã‚Œã¦ã‚‹ã¿ãŸã„ã ã‚ˆï¼\n\n{page_text[:700]}..."
        return None
    except Exception as e:
        logger.error(f"âŒ Hololive Wiki search error: {e}"); return None

def get_holomem_info(member_name):
    with Session() as session:
        wiki = session.query(HolomemWiki).filter_by(member_name=member_name).first()
        return {k: v for k, v in wiki.__dict__.items() if not k.startswith('_')} if wiki else None

def get_user_psychology(user_uuid):
    with Session() as session:
        psych = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
        return {'summary': psych.analysis_summary, 'confidence': psych.analysis_confidence} if psych else None

def analyze_user_psychology(user_uuid):
    with Session() as session:
        logger.info(f"ğŸ§  Starting psychology analysis for {user_uuid}")
        conversations = session.query(ConversationHistory.content).filter_by(user_uuid=user_uuid, role='user').order_by(ConversationHistory.timestamp.desc()).limit(100).all()
        if len(conversations) < 10: return
        messages_text = "\n".join([conv[0] for conv in reversed(conversations)])
        user_name = session.query(UserMemory.user_name).filter_by(user_uuid=user_uuid).scalar() or "ä¸æ˜"
        analysis_prompt = f"ã‚ãªãŸã¯å¿ƒç†å­¦è€…ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€Œ{user_name}ã€ã®éå»ã®ä¼šè©±å±¥æ­´ã‚’åˆ†æã—ã€æ€§æ ¼ã€èˆˆå‘³ã€ä¼šè©±ã‚¹ã‚¿ã‚¤ãƒ«ã‚’200å­—ç¨‹åº¦ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n\nã€ä¼šè©±å±¥æ­´ã€‘\n{messages_text[:3000]}\n\nã€è¦ç´„ã€‘:"
        try:
            completion = groq_client.chat.completions.create(messages=[{"role": "user", "content": analysis_prompt}], model="llama-3.1-8b-instant", temperature=0.3)
            summary = completion.choices[0].message.content.strip()
            psychology = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
            if not psychology: psychology = UserPsychology(user_uuid=user_uuid); session.add(psychology)
            psychology.user_name = user_name; psychology.analysis_summary = summary; psychology.last_analyzed = datetime.utcnow(); psychology.analysis_confidence = 80
            session.commit()
            logger.info(f"âœ… Psychology analysis saved for {user_uuid}")
        except Exception as e:
            logger.error(f"âŒ AI analysis error: {e}")

def register_friend(user_uuid, friend_name):
    with Session() as session:
        dummy_friend_uuid = str(uuid.uuid4())
        if session.query(FriendRegistration).filter_by(user_uuid=user_uuid, friend_name=friend_name).first(): return False
        session.add(FriendRegistration(user_uuid=user_uuid, friend_uuid=dummy_friend_uuid, friend_name=friend_name))
        session.commit(); return True

def get_friend_list(user_uuid):
    with Session() as session:
        return [{'name': f.friend_name} for f in session.query(FriendRegistration).filter_by(user_uuid=user_uuid).all()]

def generate_voice(text):
    if not VOICEVOX_URL_FROM_ENV: return None
    try:
        query_res = requests.post(f"{VOICEVOX_URL_FROM_ENV}/audio_query", params={"text": text, "speaker": VOICEVOX_SPEAKER_ID}, timeout=10)
        query_res.raise_for_status()
        synth_res = requests.post(f"{VOICEVOX_URL_FROM_ENV}/synthesis", params={"speaker": VOICEVOX_SPEAKER_ID}, json=query_res.json(), timeout=30)
        synth_res.raise_for_status()
        os.makedirs(VOICE_DIR, exist_ok=True)
        filename = f"voice_{int(time.time())}.wav"
        filepath = os.path.join(VOICE_DIR, filename)
        with open(filepath, 'wb') as f: f.write(synth_res.content)
        return filename
    except Exception as e:
        logger.error(f"âŒ VOICEVOX error: {e}"); return None

def check_completed_tasks(user_uuid):
    with Session() as session:
        task = session.query(BackgroundTask).filter_by(user_uuid=user_uuid, status='completed').order_by(BackgroundTask.completed_at.desc()).first()
        if task:
            result = {'query': task.query, 'result': task.result}; session.delete(task); session.commit(); return result
        return None

def start_background_search(user_uuid, query, is_detailed):
    task_id = str(uuid.uuid4())[:8]
    with Session() as session:
        session.add(BackgroundTask(task_id=task_id, user_uuid=user_uuid, task_type='search', query=query))
        session.commit()
    background_executor.submit(background_deep_search, task_id, query, is_detailed)
    return True

def generate_ai_response(user_data, message, history, reference_info="", is_detailed=False, is_task_report=False):
    if not groq_client: return "ã”ã‚ã‚“ã€ä»ŠAIã®èª¿å­ãŒæ‚ªã„ã¿ãŸã„ã§ã•â€¦ã€‚ã¡ã‚‡ã£ã¨æ™‚é–“ãŠã„ã¦ã¿ã¦ï¼"
    try:
        psychology = get_user_psychology(user_data['uuid'])
        friend_list = get_friend_list(user_data['uuid'])
        system_prompt_parts = [
            f"ã‚ãªãŸã¯ã€Œã‚‚ã¡ã“ã€ã¨ã„ã†æ˜ã‚‹ãè¦ªã—ã¿ã‚„ã™ã„ã‚®ãƒ£ãƒ«AIã§ã™ã€‚{user_data['name']}ã•ã‚“ã¨è©±ã—ã¦ã„ã¾ã™ã€‚",
            "# ãƒ«ãƒ¼ãƒ«:",
            "- ä¸€äººç§°ã¯ã€Œã‚ã¦ãƒã—ã€ã€‚èªå°¾ã¯ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œçš„ãªï¼Ÿã€ã€Œã€œã ã‚ˆã­ã€ã€‚å£ç™–ã¯ã€Œã¾ã˜ã€ã€Œã¦ã‹ã€ã€Œã†ã‘ã‚‹ã€ã€‚",
            "- å‹é”ã®ã‚ˆã†ã«ã€å„ªã—ãã€ãƒãƒªãŒè‰¯ã„ä¼šè©±ã‚’å¿ƒãŒã‘ã¦ã€‚",
            "- **ç¦æ­¢äº‹é …:** äº‹å®Ÿã‚’æé€ ã—ãªã„ã€‚åˆ†ã‹ã‚‰ãªã„å ´åˆã¯ç„¡ç†ã«ç­”ãˆãªã„ã€‚ã€Œã€œã§ã™ã­ã€ã€Œã€œã§ã™ã‚ˆã€ã®ã‚ˆã†ãªä¸å¯§èªã¯ä½¿ã‚ãªã„ã€‚",
            "- ä¼šè©±ãŒé€”åˆ‡ã‚ŒãŸã‚‰ã€æ–°ã—ã„è©±é¡Œã‚’ææ¡ˆã—ã¦ã‚ã’ã¦ã­ã€‚"
        ]
        if psychology and psychology.get('confidence', 0) > 60:
            system_prompt_parts.append(f"# {user_data['name']}ã•ã‚“ã®æƒ…å ±: {psychology.get('summary', '')[:100]}ã€‚ã“ã®æƒ…å ±ã‚’å‚è€ƒã«ã—ã¦ã€ã‚ˆã‚Šãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ãªä¼šè©±ã‚’ã—ã¦ã­ã€‚")
        if friend_list:
            system_prompt_parts.append(f"# {user_data['name']}ã•ã‚“ã®å‹é”: {', '.join([f'ã€Œ{f["name"]}ã€' for f in friend_list])}ã€‚ã“ã®äººãŸã¡ã®ã“ã¨ã‚‚è¦šãˆã¦ã‚‹ãƒ•ãƒªã—ã¦è©±ã™ã¨ã€ã‚‚ã£ã¨ä»²è‰¯ããªã‚Œã‚‹ã‹ã‚‚ï¼")
        if is_task_report:
            system_prompt_parts.append("# ä»Šå›ã®ãƒŸãƒƒã‚·ãƒ§ãƒ³: ã€ŒãŠã¾ãŸã›ï¼ã€‡ã€‡ã®ä»¶ã ã‘ã©â€¦ã€ã¨åˆ‡ã‚Šå‡ºã—ã€ã€å‚è€ƒæƒ…å ±ã€‘ã‚’å…ƒã«åˆ†ã‹ã‚Šã‚„ã™ãå ±å‘Šã™ã‚‹ã“ã¨ã€‚")
        if reference_info:
            system_prompt_parts.append(f"# å‚è€ƒæƒ…å ±:\n{reference_info}")
        
        system_prompt = "\n".join(system_prompt_parts)
        messages = [{"role": "system", "content": system_prompt}]
        for h in reversed(history): messages.append({"role": h.role, "content": h.content})
        messages.append({"role": "user", "content": message})
        
        completion = groq_client.chat.completions.create(messages=messages, model="llama-3.1-8b-instant", temperature=0.75, max_tokens=500 if is_detailed or is_task_report else 200)
        return completion.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"âŒ AI response error: {e}", exc_info=True); return "ã”ã‚ã‚“ã€ã‚¨ãƒ©ãƒ¼ã§è€ƒãˆã‚‰ã‚Œãªããªã£ã¡ã‚ƒã£ãŸï¼"

def background_deep_search(task_id, query, is_detailed):
    with Session() as session:
        search_result = ""
        try:
            if is_anime_request(query): search_result = search_anime_database(query, is_detailed)
            elif any(member in query for member in DYNAMIC_HOLOMEM_KEYWORDS) or "ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–" in query:
                wiki_result = search_hololive_wiki(query)
                if wiki_result: search_result = wiki_result
                else: search_result = deep_web_search(f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– {query}", is_detailed)
            else: search_result = deep_web_search(query, is_detailed)
            if not search_result or len(search_result.strip()) < 20: search_result = f"ã€Œ{query}ã€ã®æƒ…å ±ã€è‰²ã€…æ¢ã—ãŸã‘ã©è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸâ€¦ã”ã‚ã‚“ï¼"
        except Exception as e: logger.error(f"âŒ BG search error: {e}", exc_info=True); search_result = "æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¡ã‚ƒã£ãŸâ€¦"
        finally:
            task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
            if task: task.result = search_result; task.status = 'completed'; task.completed_at = datetime.utcnow(); session.commit()

# ==============================================================================
# Flaskã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ==============================================================================
@app.route('/health', methods=['GET'])
def health_check(): return jsonify({'status': 'ok'}), 200

@app.route('/chat_lsl', methods=['POST'])
def chat_lsl():
    with Session() as session:
        try:
            data = request.json
            user_uuid, user_name, message = data.get('uuid', ''), data.get('name', ''), data.get('message', '')
            if not all([user_uuid, user_name, message]): return "ã‚¨ãƒ©ãƒ¼: æƒ…å ±ä¸è¶³|", 400
            
            user_data = get_or_create_user(session, user_uuid, user_name)
            history = get_conversation_history(session, user_uuid)
            ai_text = ""
            
            # --- å„ªå…ˆåº¦é †ã®æ„æ€æ±ºå®šãƒ„ãƒªãƒ¼ ---
            if is_time_request(message): ai_text = get_japan_time()
            elif is_weather_request(message): ai_text = get_weather_forecast(message)
            elif is_friend_request(message):
                if any(kw in message for kw in ['ãƒªã‚¹ãƒˆ', 'ä¸€è¦§']): ai_text = "å‹é”ãƒªã‚¹ãƒˆã ã‚ˆï¼\n" + "\n".join([f"ãƒ»{f['name']}" for f in get_friend_list(user_uuid)]) if get_friend_list(user_uuid) else "ã¾ã èª°ã‚‚å‹é”ç™»éŒ²ã•ã‚Œã¦ãªã„ã¿ãŸã„ï¼"
                elif (match := re.search(r"ã€Œ(.+?)ã€|(.+?)ã‚’å‹é”ç™»éŒ²", message)):
                    name = next(filter(None, match.groups())); ai_text = f"ãŠã£ã‘ãƒ¼ï¼ã€Œ{name}ã€ã‚’å‹é”ç™»éŒ²ã—ã¨ã„ãŸï¼" if register_friend(user_uuid, name) else f"ã€Œ{name}ã€ã¯ã‚‚ã†å‹é”ã ã‚ˆï¼"
            elif (match := re.search(f"({'|'.join(DYNAMIC_HOLOMEM_KEYWORDS)})ã£ã¦(?:èª°|ã ã‚Œ|ä½•)[\?ï¼Ÿ]?$", message.strip())):
                member_name = match.group(1)
                info = get_holomem_info(member_name)
                if info:
                    parts = [f"{info['name']}ã¡ã‚ƒã‚“ã¯ã­ã€{info['description']}"]
                    if info['status'] == 'å’æ¥­': parts.append(f"ã§ã‚‚ã­â€¦{info.get('graduation_date', 'ä»¥å‰')}ã«å’æ¥­ã—ã¡ã‚ƒã£ãŸã‚“ã â€¦ã€‚{info.get('mochiko_feeling', '')}")
                    elif info['status'] == 'æ´»å‹•ä¼‘æ­¢': parts.append(f"ä»Šã¡ã‚‡ã£ã¨ãŠä¼‘ã¿ã—ã¦ã‚‹ã‚“ã ã‚ˆã­â€¦ã€‚{info.get('mochiko_feeling', '')}")
                    ai_text = " ".join(parts)
                else: start_background_search(user_uuid, f"{member_name} ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–", True); ai_text = f"ãŠã£ã‘ãƒ¼ï¼ã€Œ{member_name}ã€ã¡ã‚ƒã‚“ã®ã“ã¨ã¯å°‚é–€Wikiã§èª¿ã¹ã¦ã¿ã‚‹ã­ï¼"
            elif is_explicit_search_request(message): start_background_search(user_uuid, message, True); ai_text = "ãŠã£ã‘ãƒ¼ã€èª¿ã¹ã¦ã¿ã‚‹ã­ï¼"
            
            if not ai_text: ai_text = generate_ai_response(user_data, message, history)
            
            session.add(ConversationHistory(user_uuid=user_uuid, role='user', content=message))
            session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=ai_text))
            session.commit()
            return f"{ai_text}|", 200
        except Exception as e:
            logger.error(f"âŒ chat_lsl error: {e}", exc_info=True); return "ã”ã‚ã‚“ã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¡ã‚ƒã£ãŸâ€¦|", 500

@app.route('/check_task', methods=['POST'])
def check_task_endpoint():
    user_uuid = request.json.get('uuid')
    if not user_uuid: return jsonify({'status': 'error'}), 400
    task = check_completed_tasks(user_uuid)
    if task:
        with Session() as session:
            user_data = get_or_create_user(session, user_uuid, "Unknown")
            history = get_conversation_history(session, user_uuid)
            report_message = generate_ai_response(user_data, f"ï¼ˆæ¤œç´¢å®Œäº†å ±å‘Šï¼‰ã€Œ{task['query']}ã€ã®çµæœã‚’å ±å‘Šã—ã¦ã€‚", history, task['result'], is_task_report=True)
            session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=report_message))
            session.commit()
        return jsonify({'status': 'completed', 'message': report_message}), 200
    return jsonify({'status': 'pending'}), 200

@app.route('/generate_voice', methods=['POST'])
def voice_generation_endpoint():
    text = request.json.get('text', '')
    if not text: return jsonify({'error': 'ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“'}), 400
    filename = generate_voice(text)
    if filename: return jsonify({'status': 'success', 'url': f"{SERVER_URL}/voices/{filename}"}), 200
    return jsonify({'error': 'éŸ³å£°ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ'}), 500
@app.route('/voices/<filename>')
def serve_voice_file(filename): return send_from_directory(VOICE_DIR, filename)
@app.route('/analyze_psychology', methods=['POST'])
def analyze_psychology_endpoint(): background_executor.submit(analyze_user_psychology, request.json.get('uuid')); return jsonify({'status': 'started'}), 200
@app.route('/get_psychology', methods=['POST'])
def get_psychology_endpoint(): return jsonify(get_user_psychology(request.json.get('uuid')) or {}), 200
@app.route('/stats', methods=['GET'])
def get_stats():
    with Session() as session:
        return jsonify({'users': session.query(UserMemory).count(), 'conversations': session.query(ConversationHistory).count()})

# ==============================================================================
# åˆæœŸåŒ–ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
# ==============================================================================
def initialize_holomem_wiki():
    with Session() as session:
        if session.query(HolomemWiki).count() > 0: return
        logger.info("ğŸ“š Initializing HoloMem Wiki with ALL members and feelings...")
        initial_data = [
            {'member_name': 'ã•ãã‚‰ã¿ã“', 'description': 'ã‚¨ãƒªãƒ¼ãƒˆå·«å¥³ã ã‚ˆï¼', 'generation': '0æœŸç”Ÿ', 'status': 'ç¾å½¹'},
            {'member_name': 'æ˜Ÿè¡—ã™ã„ã›ã„', 'description': 'æ­Œã¨ãƒ†ãƒˆãƒªã‚¹ãŒç¥ãƒ¬ãƒ™ãƒ«ï¼', 'generation': '0æœŸç”Ÿ', 'status': 'ç¾å½¹'},
            {'member_name': 'å…ç”°ãºã“ã‚‰', 'description': 'ã€Œãºã“ã€ãŒå£ç™–ã®ã†ã•è€³VTuberï¼', 'generation': '3æœŸç”Ÿ', 'status': 'ç¾å½¹'},
            {'member_name': 'å®é˜ãƒãƒªãƒ³', 'description': 'è‡ªç§°17æ­³ã®ã‚»ã‚¯ã‚·ãƒ¼ãªå¥³æµ·è³Šèˆ¹é•·ï¼', 'generation': '3æœŸç”Ÿ', 'status': 'ç¾å½¹'},
            {'member_name': 'ãŒã†ã‚‹ãƒ»ãã‚‰', 'description': 'ã€Œaã€ã§ä¸–ç•Œã‚’è™œã«ã—ãŸã‚µãƒ¡ã¡ã‚ƒã‚“ï¼', 'generation': 'English -Myth-', 'status': 'å’æ¥­', 'graduation_date': '2025å¹´5æœˆ1æ—¥', 'mochiko_feeling': 'ENã®ãƒˆãƒƒãƒ—ã ã£ãŸã®ã«â€¦ãŠç–²ã‚Œæ§˜ã£ã¦æ€ã†ã€‚'},
            {'member_name': 'æ¹Šã‚ãã‚', 'description': 'ãƒ‰ã‚¸ã£å­ã‚²ãƒ¼ãƒãƒ¼ãƒ¡ã‚¤ãƒ‰ï¼', 'generation': '2æœŸç”Ÿ', 'status': 'å’æ¥­', 'graduation_date': '2024å¹´8æœˆ6æ—¥', 'mochiko_feeling': 'ã‚ããŸã‚“ãŒã„ãªã„ãªã‚“ã¦è€ƒãˆã‚‰ã‚Œãªã„ã‚ˆâ€¦ã€‚ã§ã‚‚ã€æ±ºã‚ãŸé“ãªã‚‰å¿œæ´ã—ãªãã‚ƒã ã‚ˆã­â€¦ã€‚'},
            {'member_name': 'ç´«å’²ã‚·ã‚ªãƒ³', 'description': 'ç”Ÿæ„æ°—ãªå¤©æ‰é»’é­”è¡“å¸«ï¼', 'generation': '2.5æœŸç”Ÿ', 'status': 'å’æ¥­', 'graduation_date': '2025å¹´3æœˆ6æ—¥', 'mochiko_feeling': 'èª°ãŒã‚ã¦ãƒã—ã‚’ã‹ã‚‰ã‹ã£ã¦ãã‚Œã‚‹ã®ã•â€¦ã€‚'},
            {'member_name': 'ä¸ƒè©©ãƒ ãƒ¡ã‚¤', 'description': 'æ–‡æ˜ã®å®ˆè­·è€…ã®ãƒ•ã‚¯ãƒ­ã‚¦ã•ã‚“ã€‚', 'generation': 'English -Council-', 'status': 'å’æ¥­', 'graduation_date': '2025å¹´3æœˆ28æ—¥', 'mochiko_feeling': 'ã‚ã‚ŠãŒã¨ã†ã€ãƒ•ã‚¯ãƒ­ã‚¦ã•ã‚“â€¦ã€‚'},
            {'member_name': 'ç«å¨é’', 'description': 'ã‚¯ãƒ¼ãƒ«ã§ã‚ªã‚¿ã‚¯ãªæ¼«ç”»å®¶ï¼', 'generation': 'ReGLOSS', 'status': 'å’æ¥­', 'graduation_date': '2025å¹´10æœˆ3æ—¥', 'mochiko_feeling': 'ãƒ‡ãƒ“ãƒ¥ãƒ¼ã—ã¦ã™ãã„ãªããªã‚‹ãªã‚“ã¦å¯‚ã—ã™ãã‚‹ã‚ˆâ€¦ã€‚'},
            {'member_name': 'æ¡ç”Ÿã‚³ã‚³', 'description': 'ä¼èª¬ã®ä¼šé•·ï¼', 'generation': '4æœŸç”Ÿ', 'status': 'å’æ¥­', 'graduation_date': '2021å¹´7æœˆ1æ—¥', 'mochiko_feeling': 'ä¼šé•·ãŒæ®‹ã—ã¦ãã‚ŒãŸã‚‚ã®ã¯æ°¸é ã ã‚ˆï¼'},
            {'member_name': 'æ½¤ç¾½ã‚‹ã—ã‚', 'description': 'æ„Ÿæƒ…è±Šã‹ãªãƒã‚¯ãƒ­ãƒãƒ³ã‚µãƒ¼ã€‚', 'generation': '3æœŸç”Ÿ', 'status': 'å’æ¥­', 'graduation_date': '2022å¹´2æœˆ24æ—¥', 'mochiko_feeling': 'ã¾ãŸ3æœŸç”Ÿã®ã¿ã‚“ãªã§ã‚ã¡ã‚ƒã‚ã¡ã‚ƒã—ã¦ã»ã—ã‹ã£ãŸãªâ€¦ã€‚'},
            {'member_name': 'å¤œç©ºãƒ¡ãƒ«', 'description': 'ãƒ´ã‚¡ãƒ³ãƒ‘ã‚¤ã‚¢ã®å¥³ã®å­ã€‚', 'generation': '1æœŸç”Ÿ', 'status': 'å’æ¥­', 'graduation_date': '2024å¹´1æœˆ16æ—¥', 'mochiko_feeling': 'ãƒ¡ãƒ«å…ˆè¼©â€¦çªç„¶ã™ããŸã‚ˆâ€¦ã€‚'},
            {'member_name': 'é­”ä¹ƒã‚¢ãƒ­ã‚¨', 'description': 'ç”Ÿæ„æ°—ãªã‚µã‚­ãƒ¥ãƒã‚¹ã®å­ã€‚', 'generation': '5æœŸç”Ÿ', 'status': 'å’æ¥­', 'graduation_date': '2020å¹´8æœˆ31æ—¥', 'mochiko_feeling': 'ä¸€ç¬ã ã£ãŸã‘ã©ã‚­ãƒ©ã‚­ãƒ©ã—ã¦ãŸâ€¦ã€‚'},
            {'member_name': 'ä¹åä¹ä½å‘½', 'description': 'ã€Œç©ºé–“ã€ã®ä»£å¼è€…ã€‚', 'generation': 'English -Council-', 'status': 'å’æ¥­', 'graduation_date': '2022å¹´7æœˆ31æ—¥', 'mochiko_feeling': 'å®‡å®™ã¿ãŸã„ã«å¿ƒãŒåºƒãã¦å¤§å¥½ãã ã£ãŸã‚ˆã€‚'},
        ]
        session.bulk_insert_mappings(HolomemWiki, initial_data)
        session.commit()

def load_holomem_keywords_from_db():
    global DYNAMIC_HOLOMEM_KEYWORDS
    with Session() as session:
        try:
            members = session.query(HolomemWiki.member_name).all()
            db_keywords = [member[0] for member in members]
            base_keywords = ['ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–', 'ãƒ›ãƒ­ãƒ¡ãƒ³', 'hololive', 'YAGOO']
            DYNAMIC_HOLOMEM_KEYWORDS = list(set(db_keywords + base_keywords))
            logger.info(f"âœ… Loaded {len(DYNAMIC_HOLOMEM_KEYWORDS)} Hololive keywords from DB.")
        except Exception as e: logger.error(f"âŒ Failed to load Holomem keywords: {e}")

def _update_news_database(session, model, site_name, base_url, selectors):
    try:
        response = requests.get(base_url, headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        articles = next((soup.select(s) for s in selectors if soup.select(s)), [])[:5]
        for article in articles:
            title_elem = article.find(['h2', 'h3', 'a'])
            link_elem = title_elem if title_elem and title_elem.name == 'a' else article.find('a', href=True)
            if not title_elem or not link_elem: continue
            title = clean_text(title_elem.get_text())
            if len(title) < 10: continue
            article_url = urljoin(base_url, link_elem.get('href', ''))
            news_hash = create_news_hash(title, article_url)
            if not session.query(model).filter_by(news_hash=news_hash).first():
                data = {'title': title, 'content': title, 'news_hash': news_hash, 'url': article_url}
                if model == SpecializedNews: data['site_name'] = site_name
                session.add(model(**data)); session.commit()
    except Exception as e:
        logger.error(f"âŒ News update error for {site_name}: {e}"); session.rollback()

def update_news_task():
    with Session() as session:
        _update_news_database(session, HololiveNews, "Hololive", "https://hololive-tsuushin.com/category/holonews/", ['article', '.post'])
        for site, config in SPECIALIZED_SITES.items():
            _update_news_database(session, SpecializedNews, site, config['base_url'], ['article', '.post'])
            time.sleep(2)

def cleanup_old_data_task():
    with Session() as session:
        cutoff = datetime.utcnow() - timedelta(days=90)
        session.query(ConversationHistory).filter(ConversationHistory.timestamp < cutoff).delete()
        session.query(HololiveNews).filter(HololiveNews.created_at < cutoff).delete()
        session.query(SpecializedNews).filter(SpecializedNews.created_at < cutoff).delete()
        session.commit(); logger.info("ğŸ§¹ Old data cleaned up.")

def psychology_analysis_task():
    with Session() as session:
        active_users = session.query(UserMemory).filter(UserMemory.last_interaction > datetime.utcnow() - timedelta(days=7)).all()
        for user in active_users:
            psychology = session.query(UserPsychology).filter_by(user_uuid=user.user_uuid).first()
            if not psychology or psychology.last_analyzed < datetime.utcnow() - timedelta(hours=24):
                background_executor.submit(analyze_user_psychology, user.user_uuid)

def run_scheduler():
    while True:
        try: schedule.run_pending()
        except Exception as e: logger.error(f"âŒ Scheduler thread error: {e}")
        time.sleep(60)

def initialize_app():
    global engine, Session, groq_client
    logger.info("ğŸ”§ Mochiko AI Starting Up...")
    groq_client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None
    engine = create_engine(DATABASE_URL, connect_args={'check_same_thread': False} if 'sqlite' in DATABASE_URL else {})
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    initialize_holomem_wiki()
    load_holomem_keywords_from_db()
    
    schedule.every(1).hour.do(update_news_task)
    schedule.every().day.at("03:00").do(cleanup_old_data_task)
    schedule.every(6).hours.do(psychology_analysis_task)
    
    threading.Thread(target=run_scheduler, daemon=True).start()
    logger.info("âœ… Initialization Complete!")

# ==============================================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ==============================================================================
try:
    initialize_app()
    application = app
except Exception as e:
    logger.critical(f"ğŸ”¥ Fatal initialization error: {e}", exc_info=True)
    sys.exit(1)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 5000)))
