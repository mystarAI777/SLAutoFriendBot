import sys
import os
import requests
import logging
import time
import threading
import json
import re
import sqlite3
import random
import uuid
import hashlib
from datetime import datetime, timedelta, timezone
import unicodedata
from groq import Groq

# å‹ãƒ’ãƒ³ãƒˆç”¨ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼å¯¾ç­–ä»˜ãï¼‰
try:
    from typing import Union, Dict, Any, List, Optional
except ImportError:
    # å‹ãƒ’ãƒ³ãƒˆãŒä½¿ãˆãªã„ç’°å¢ƒç”¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    Dict = dict
    Any = object
    List = list
    Union = object
    Optional = object

from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Index, Text, Boolean, inspect, text
from sqlalchemy.orm import declarative_base, sessionmaker
from urllib.parse import quote_plus, urljoin
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import asyncio
from threading import Lock
import schedule
import signal

# --- åŸºæœ¬è¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- å®šæ•°è¨­å®š ---
VOICE_DIR = '/tmp/voices'
SERVER_URL = "https://slautofriendbot.onrender.com"
VOICEVOX_SPEAKER_ID = 20  # ã‚‚ã¡å­ã•ã‚“(ãƒãƒ¼ãƒãƒ«) ã«çµ±åˆ
HOLOLIVE_NEWS_URL = "https://hololive-tsuushin.com/category/holonews/"
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
]
LOCATION_CODES = {
    "æ±äº¬": "130000", "å¤§é˜ª": "270000", "åå¤å±‹": "230000", "ç¦å²¡": "400000", "æœ­å¹Œ": "016000"
}
SPECIALIZED_SITES = {
    'Blender': {
        'base_url': 'https://docs.blender.org/manual/ja/latest/',
        'keywords': ['Blender', 'ãƒ–ãƒ¬ãƒ³ãƒ€ãƒ¼', 'blender', 'BLENDER']
    },
    'CGãƒ‹ãƒ¥ãƒ¼ã‚¹': {
        'base_url': 'https://modelinghappy.com/',
        'keywords': ['CGãƒ‹ãƒ¥ãƒ¼ã‚¹', '3DCG', 'CG', 'ï¼£ï¼§', 'ï½ƒï½‡', 'cg', '3dcg', 'ï¼“ï¼¤ï¼£ï¼§', 'CGæ¥­ç•Œ', 'CGã‚¢ãƒ‹ãƒ¡']
    },
    'è„³ç§‘å­¦ãƒ»å¿ƒç†å­¦': {
        'base_url': 'https://nazology.kusuguru.co.jp/',
        'keywords': ['è„³ç§‘å­¦', 'å¿ƒç†å­¦', 'è„³', 'å¿ƒç†', 'ã®ã†ã‹ãŒã', 'ã—ã‚“ã‚ŠãŒã']
    },
    'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•': {
        'base_url': 'https://community.secondlife.com/news/',
        'keywords': ['ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•', 'Second Life', 'SL', 'second life', 'ã‚»ã‚«ãƒ³ãƒ‰', 'SecondLife']
    }
}
HOLOMEM_KEYWORDS = [
    'ã¨ãã®ãã‚‰', 'ãƒ­ãƒœå­ã•ã‚“', 'ã•ãã‚‰ã¿ã“', 'æ˜Ÿè¡—ã™ã„ã›ã„', 'AZKi', 'å¤œç©ºãƒ¡ãƒ«',
    'ã‚¢ã‚­ãƒ»ãƒ­ãƒ¼ã‚¼ãƒ³ã‚¿ãƒ¼ãƒ«', 'èµ¤äº•ã¯ã‚ã¨', 'ç™½ä¸Šãƒ•ãƒ–ã‚­', 'å¤è‰²ã¾ã¤ã‚Š', 'æ¹Šã‚ãã‚',
    'ç´«å’²ã‚·ã‚ªãƒ³', 'ç™¾é¬¼ã‚ã‚„ã‚', 'ç™’æœˆã¡ã‚‡ã“', 'å¤§ç©ºã‚¹ãƒãƒ«', 'å¤§ç¥ãƒŸã‚ª', 'çŒ«åˆãŠã‹ã‚†',
    'æˆŒç¥ã“ã‚ã­', 'å…ç”°ãºã“ã‚‰', 'ä¸çŸ¥ç«ãƒ•ãƒ¬ã‚¢', 'ç™½éŠ€ãƒã‚¨ãƒ«', 'å®é˜ãƒãƒªãƒ³', 'å¤©éŸ³ã‹ãªãŸ',
    'è§’å·»ã‚ãŸã‚', 'å¸¸é—‡ãƒˆãƒ¯', 'å§«æ£®ãƒ«ãƒ¼ãƒŠ', 'é›ªèŠ±ãƒ©ãƒŸã‚£', 'æ¡ƒéˆ´ã­ã­', 'ç…ç™½ã¼ãŸã‚“',
    'å°¾ä¸¸ãƒãƒ«ã‚«', 'ãƒ©ãƒ—ãƒ©ã‚¹ãƒ»ãƒ€ãƒ¼ã‚¯ãƒã‚¹', 'é·¹å¶ºãƒ«ã‚¤', 'åšè¡£ã“ã‚ˆã‚Š', 'æ²™èŠ±å‰ã‚¯ãƒ­ãƒ±',
    'é¢¨çœŸã„ã‚ã¯', 'æ£®ã‚«ãƒªã‚ªãƒš', 'å°é³¥éŠã‚­ã‚¢ãƒ©', 'ä¸€ä¼Šé‚£å°“æ –', 'ãŒã†ã‚‹ãƒ»ãã‚‰',
    'ãƒ¯ãƒˆã‚½ãƒ³ãƒ»ã‚¢ãƒ¡ãƒªã‚¢', 'IRyS', 'ã‚»ãƒ¬ã‚¹ãƒ»ãƒ•ã‚¡ã‚¦ãƒŠ', 'ã‚ªãƒ¼ãƒ­ãƒ»ã‚¯ãƒ­ãƒ‹ãƒ¼', 'ä¸ƒè©©ãƒ ãƒ¡ã‚¤',
    'ãƒã‚³ã‚¹ãƒ»ãƒ™ãƒ¼ãƒ«ã‚º', 'ã‚·ã‚ªãƒªãƒ»ãƒãƒ´ã‚§ãƒ©', 'å¤çŸ³ãƒ“ã‚¸ãƒ¥ãƒ¼', 'ãƒãƒªãƒƒã‚µãƒ»ãƒ¬ã‚¤ãƒ´ãƒ³ã‚¯ãƒ­ãƒ•ãƒˆ',
    'ãƒ•ãƒ¯ãƒ¯ãƒ»ã‚¢ãƒ“ã‚¹ã‚¬ãƒ¼ãƒ‰', 'ãƒ¢ã‚³ã‚³ãƒ»ã‚¢ãƒ“ã‚¹ã‚¬ãƒ¼ãƒ‰', 'ã‚¢ãƒ¦ãƒ³ãƒ€ãƒ»ãƒªã‚¹', 'ãƒ ãƒ¼ãƒŠãƒ»ãƒ›ã‚·ãƒãƒ´ã‚¡',
    'ã‚¢ã‚¤ãƒ©ãƒ‹ãƒ»ã‚¤ã‚ªãƒ•ã‚£ãƒ•ãƒ†ã‚£ãƒ¼ãƒ³', 'ã‚¯ãƒ¬ã‚¤ã‚¸ãƒ¼ãƒ»ã‚ªãƒªãƒ¼', 'ã‚¢ãƒ¼ãƒ‹ãƒ£ãƒ»ãƒ¡ãƒ«ãƒ•ã‚£ãƒƒã‚µ',
    'ãƒ‘ãƒ´ã‚©ãƒªã‚¢ãƒ»ãƒ¬ã‚¤ãƒ', 'ç«å¨é’', 'éŸ³ä¹ƒç€¬å¥', 'ä¸€æ¡è‰ã€…è¯', 'å„’çƒé¢¨äº­ã‚‰ã§ã‚“',
    'è½Ÿã¯ã˜ã‚', 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–', 'ãƒ›ãƒ­ãƒ¡ãƒ³', 'hololive', 'YAGOO'
]

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° & Executor ---
background_executor = ThreadPoolExecutor(max_workers=5)
groq_client = None
VOICEVOX_ENABLED = True
app = Flask(__name__)
CORS(app)
Base = declarative_base()

# â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ãŒå”¯ä¸€ã®å¤‰æ›´ç®‡æ‰€ã§ã™ã€‘â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼

# --- ç§˜å¯†æƒ…å ±/ç’°å¢ƒå¤‰æ•° èª­ã¿è¾¼ã¿ ---
def get_secret(name):
    """
    ã¾ãšRenderã®Secret Fileã‹ã‚‰ç§˜å¯†æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã€è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚
    """
    secret_file_path = f"/etc/secrets/{name}"
    if os.path.exists(secret_file_path):
        try:
            with open(secret_file_path, 'r') as f:
                logger.info(f"âœ… Secret Fileã‹ã‚‰ {name} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
                return f.read().strip()
        except IOError as e:
            logger.error(f"âŒ Secret File {secret_file_path} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return None
    
    # Secret FileãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ç’°å¢ƒå¤‰æ•°ã‚’ãƒã‚§ãƒƒã‚¯
    value = os.environ.get(name)
    if value:
         logger.info(f"âœ… ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ {name} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    return value

def ensure_voice_directory():
    """éŸ³å£°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ã‚’ä¿è¨¼"""
    max_attempts = 3
    for attempt in range(max_attempts):
        try:
            if not os.path.exists(VOICE_DIR):
                os.makedirs(VOICE_DIR, mode=0o755, exist_ok=True)
                logger.info(f"âœ… Voice directory created: {VOICE_DIR}")
            
            # æ›¸ãè¾¼ã¿æ¨©é™ã‚’ç¢ºèª
            if os.access(VOICE_DIR, os.W_OK):
                logger.info(f"âœ… Voice directory is writable: {VOICE_DIR}")
                return True
            else:
                # æ¨©é™ã‚’ä¿®æ­£
                os.chmod(VOICE_DIR, 0o755)
                logger.info(f"âœ… Voice directory permissions fixed: {VOICE_DIR}")
                return True
                
        except Exception as e:
            logger.error(f"âŒ Voice directory creation failed (attempt {attempt + 1}/{max_attempts}): {e}")
            if attempt < max_attempts - 1:
                time.sleep(1)
            continue
    
    logger.critical(f"ğŸ”¥ Failed to create voice directory after {max_attempts} attempts")
    return False
    
DATABASE_URL = get_secret('DATABASE_URL') or 'sqlite:///./test.db'
GROQ_API_KEY = get_secret('GROQ_API_KEY')
VOICEVOX_URL_FROM_ENV = get_secret('VOICEVOX_URL')

# â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²ã€å¤‰æ›´ç®‡æ‰€ã¯ã“ã“ã¾ã§ã§ã™ã€‘â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²

# --- åˆæœŸåŒ–å‡¦ç† ---
try:
    ensure_voice_directory()

if not DATABASE_URL:
    logger.critical("FATAL: DATABASE_URL is not set.")
    sys.exit(1)

# --- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« ---
class UserMemory(Base):
    __tablename__ = 'user_memories'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False)
    user_name = Column(String(255), nullable=False)
    interaction_count = Column(Integer, default=0)
    last_interaction = Column(DateTime, default=datetime.utcnow)

class ConversationHistory(Base):
    __tablename__ = 'conversation_history'
    id = Column(Integer, primary_key=True, autoincrement=True)
    user_uuid = Column(String(255), nullable=False, index=True)
    role = Column(String(10), nullable=False)
    content = Column(Text, nullable=False)
    timestamp = Column(DateTime, default=datetime.utcnow, index=True)

class HololiveNews(Base):
    __tablename__ = 'hololive_news'
    id = Column(Integer, primary_key=True)
    title = Column(String(500), nullable=False)
    content = Column(Text, nullable=False)
    url = Column(String(1000))
    published_date = Column(DateTime, default=datetime.utcnow)
    created_at = Column(DateTime, default=datetime.utcnow, index=True)
    news_hash = Column(String(100), unique=True)

class BackgroundTask(Base):
    __tablename__ = 'background_tasks'
    id = Column(Integer, primary_key=True)
    task_id = Column(String(255), unique=True, nullable=False)
    user_uuid = Column(String(255), nullable=False)
    task_type = Column(String(50), nullable=False)
    query = Column(Text, nullable=False)
    result = Column(Text)
    status = Column(String(20), default='pending')
    created_at = Column(DateTime, default=datetime.utcnow, index=True)
    completed_at = Column(DateTime)

class SpecializedNews(Base):
    __tablename__ = 'specialized_news'
    id = Column(Integer, primary_key=True)
    site_name = Column(String(100), nullable=False, index=True)
    title = Column(String(500), nullable=False)
    content = Column(Text, nullable=False)
    url = Column(String(1000))
    published_date = Column(DateTime, default=datetime.utcnow)
    created_at = Column(DateTime, default=datetime.utcnow, index=True)
    news_hash = Column(String(100), unique=True)

class HolomemWiki(Base):
    __tablename__ = 'holomem_wiki'
    id = Column(Integer, primary_key=True)
    member_name = Column(String(100), nullable=False, unique=True, index=True)
    description = Column(Text)
    debut_date = Column(String(100))
    generation = Column(String(100))
    tags = Column(Text)
    last_updated = Column(DateTime, default=datetime.utcnow)

class FriendRegistration(Base):
    __tablename__ = 'friend_registrations'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), nullable=False, index=True)
    friend_uuid = Column(String(255), nullable=False)
    friend_name = Column(String(255), nullable=False)
    registered_at = Column(DateTime, default=datetime.utcnow)
    relationship_note = Column(Text)

class NewsCache(Base):
    __tablename__ = 'news_cache'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), nullable=False, index=True)
    news_id = Column(Integer, nullable=False)
    news_number = Column(Integer, nullable=False)
    news_type = Column(String(50), nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)

# ===== æ”¹å–„ç‰ˆ: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ =====
def create_optimized_db_engine():
    """ç’°å¢ƒã«å¿œã˜ã¦æœ€é©åŒ–ã•ã‚ŒãŸDBã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½œæˆ"""
    try:
        is_sqlite = 'sqlite' in DATABASE_URL.lower()
        
        if is_sqlite:
            connect_args = {
                'check_same_thread': False,
                'timeout': 20
            }
            engine = create_engine(
                DATABASE_URL,
                connect_args=connect_args,
                pool_pre_ping=True,
                echo=False
            )
        else:
            # PostgreSQLç”¨ã®è¨­å®š
            connect_args = {
                'connect_timeout': 10,
                'options': '-c statement_timeout=30000'
            }
            engine = create_engine(
                DATABASE_URL,
                connect_args=connect_args,
                pool_size=10,
                max_overflow=20,
                pool_pre_ping=True,
                pool_recycle=300,
                echo=False
            )
        
        # æ¥ç¶šãƒ†ã‚¹ãƒˆ
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        
        logger.info(f"âœ… Database engine created successfully ({'SQLite' if is_sqlite else 'PostgreSQL'})")
        return engine
        
    except Exception as e:
        logger.error(f"âŒ Failed to create database engine: {e}")
        raise

# ===== æ”¹å–„ç‰ˆ: GroqåˆæœŸåŒ–ï¼ˆæ¥ç¶šãƒ†ã‚¹ãƒˆã‚’å®‰å…¨ã«å®Ÿè¡Œï¼‰ =====
def initialize_groq_client():
    """Groqã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã€æ¥ç¶šãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    global groq_client
    
    try:
        from groq import Groq
        
        if not GROQ_API_KEY or GROQ_API_KEY == 'DUMMY_GROQ_KEY':
            logger.warning("âš ï¸ GROQ_API_KEY is not set - AI features will be disabled.")
            return None
            
        if len(GROQ_API_KEY) < 20:
            logger.error(f"âŒ GROQ_API_KEY is too short (length: {len(GROQ_API_KEY)})")
            return None
        
        client = Groq(api_key=GROQ_API_KEY.strip())
        
        # æ¥ç¶šãƒ†ã‚¹ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆèµ·å‹•æ™‚é–“çŸ­ç¸®ã®ãŸã‚ï¼‰
        logger.info("âœ… Groq client initialized (connection test skipped for faster startup).")
        return client
            
    except ImportError as e:
        logger.error(f"âŒ Failed to import Groq library: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ Groq client initialization failed: {e}")
        return None
        
# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---
def clean_text(text):
    if not text: return ""
    return re.sub(r'\s+', ' ', re.sub(r'<[^>]+>', '', text)).strip()

def get_japan_time():
    now = datetime.now(timezone(timedelta(hours=9)))
    return f"ä»Šã¯{now.year}å¹´{now.month}æœˆ{now.day}æ—¥ã®{now.hour}æ™‚{now.minute}åˆ†ã ã‚ˆï¼"

def create_news_hash(title, content):
    return hashlib.md5(f"{title}{content[:100]}".encode('utf-8')).hexdigest()

def is_time_request(message):
    return any(keyword in message for keyword in ['ä»Šä½•æ™‚', 'æ™‚é–“', 'æ™‚åˆ»', 'ä½•æ™‚', 'ãªã‚“ã˜'])

def is_weather_request(message):
    return any(keyword in message for keyword in ['å¤©æ°—', 'ã¦ã‚“ã', 'æ°—æ¸©', 'é›¨', 'æ™´ã‚Œ', 'æ›‡ã‚Š', 'é›ª'])

def is_hololive_request(message):
    return any(keyword in message for keyword in HOLOMEM_KEYWORDS)

def is_recommendation_request(message):
    return any(keyword in message for keyword in ['ãŠã™ã™ã‚', 'ã‚ªã‚¹ã‚¹ãƒ¡', 'æ¨è–¦', 'ç´¹ä»‹ã—ã¦'])

def detect_specialized_topic(message):
    message_normalized = unicodedata.normalize('NFKC', message).lower()
    for topic, config in SPECIALIZED_SITES.items():
        for keyword in config['keywords']:
            keyword_normalized = unicodedata.normalize('NFKC', keyword).lower()
            if keyword_normalized in message_normalized:
                logger.info(f"ğŸ¯ Specialized topic detected: {topic} (Keyword: {keyword})")
                return topic
    return None

def is_detailed_request(message):
    return any(keyword in message for keyword in ['è©³ã—ã', 'è©³ç´°', 'ãã‚ã—ã', 'æ•™ãˆã¦', 'èª¬æ˜ã—ã¦', 'è§£èª¬ã—ã¦', 'ã©ã†ã„ã†', 'ãªãœ', 'ã©ã†ã—ã¦', 'ç†ç”±', 'åŸå› ', 'å…·ä½“çš„ã«'])

def should_search(message):
    if detect_specialized_topic(message): return True
    if is_hololive_request(message) and not any(kw in message for kw in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'æƒ…å ±', 'ãŠçŸ¥ã‚‰ã›']): return True
    if is_recommendation_request(message): return True
    if any(re.search(pattern, message) for pattern in [r'(?:ã¨ã¯|ã«ã¤ã„ã¦|æ•™ãˆã¦)', r'(?:èª¿ã¹ã¦|æ¤œç´¢)', r'(?:èª°|ä½•|ã©ã“|ã„ã¤|ãªãœ|ã©ã†)']): return True
    return False

def is_story_request(message):
    return any(keyword in message for keyword in ['é¢ç™½ã„è©±', 'ãŠã‚‚ã—ã‚ã„è©±', 'è©±ã—ã¦', 'é›‘è«‡', 'ãƒã‚¿', 'ä½•ã‹è©±', 'å–‹ã£ã¦'])

def is_emotional_expression(message):
    emotional_keywords = {
        'çœ ': ['çœ ãŸã„', 'çœ ã„', 'ã­ã‚€ã„'], 'ç–²': ['ç–²ã‚ŒãŸ', 'ã¤ã‹ã‚ŒãŸ'], 'å¬‰': ['å¬‰ã—ã„', 'ã†ã‚Œã—ã„'],
        'æ¥½': ['æ¥½ã—ã„', 'ãŸã®ã—ã„'], 'æ‚²': ['æ‚²ã—ã„', 'ã‹ãªã—ã„'], 'å¯‚': ['å¯‚ã—ã„', 'ã•ã³ã—ã„'],
        'æ€’': ['æ€’', 'ã‚€ã‹ã¤ã', 'ã‚¤ãƒ©ã‚¤ãƒ©'], 'æš‡': ['æš‡', 'ã²ã¾']
    }
    for key, keywords in emotional_keywords.items():
        if any(kw in message for kw in keywords): return key
    return None

def is_seasonal_topic(message):
    return any(keyword in message for keyword in ['ãŠæœˆè¦‹', 'èŠ±è¦‹', 'ç´…è‘‰', 'ã‚¯ãƒªã‚¹ãƒã‚¹', 'æ­£æœˆ', 'ãƒãƒ­ã‚¦ã‚£ãƒ³'])

def is_short_response(message):
    return len(message.strip()) <= 3 or message.strip() in ['ã†ã‚“', 'ãã†', 'ã¯ã„', 'ãã£ã‹', 'ãªã‚‹ã»ã©', 'ãµãƒ¼ã‚“', 'ã¸ãƒ¼']

def is_news_detail_request(message):
    match = re.search(r'([1-9]|[ï¼‘-ï¼™])ç•ª|ã€([1-9]|[ï¼‘-ï¼™])ã€‘', message)
    if match and any(keyword in message for keyword in ['è©³ã—ã', 'è©³ç´°', 'æ•™ãˆã¦', 'ã‚‚ã£ã¨']):
        number_str = next(filter(None, match.groups()))
        return int(unicodedata.normalize('NFKC', number_str))
    return None

def is_friend_request(message):
    return any(fk in message for fk in ['å‹ã ã¡', 'å‹é”', 'ãƒ•ãƒ¬ãƒ³ãƒ‰']) and any(ak in message for ak in ['ç™»éŒ²', 'æ•™ãˆã¦', 'èª°', 'ãƒªã‚¹ãƒˆ'])

def extract_location(message):
    for location in LOCATION_CODES.keys():
        if location in message:
            return location
    return "æ±äº¬"

# --- ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç† ---
def save_news_cache(session, user_uuid, news_items, news_type='hololive'):
    try:
        one_hour_ago = datetime.utcnow() - timedelta(hours=1)
        session.query(NewsCache).filter(NewsCache.user_uuid == user_uuid, NewsCache.created_at < one_hour_ago).delete()
        for i, news in enumerate(news_items, 1):
            cache = NewsCache(user_uuid=user_uuid, news_id=news.id, news_number=i, news_type=news_type)
            session.add(cache)
        session.commit()
        logger.info(f"ğŸ’¾ News cache saved for user {user_uuid}: {len(news_items)} items.")
    except Exception as e:
        logger.error(f"Error saving news cache: {e}")
        session.rollback()

def get_cached_news_detail(session, user_uuid, news_number):
    try:
        one_hour_ago = datetime.utcnow() - timedelta(hours=1)
        cache = session.query(NewsCache).filter(
            NewsCache.user_uuid == user_uuid,
            NewsCache.news_number == news_number,
            NewsCache.created_at > one_hour_ago
        ).order_by(NewsCache.created_at.desc()).first()
        if not cache: return None
        
        NewsModel = HololiveNews if cache.news_type == 'hololive' else SpecializedNews
        return session.query(NewsModel).filter_by(id=cache.news_id).first()
    except Exception as e:
        logger.error(f"Error getting cached news detail: {e}")
        return None

# --- ã‚³ã‚¢æ©Ÿèƒ½: å¤©æ°—, ãƒ‹ãƒ¥ãƒ¼ã‚¹, Wiki, å‹é” ---
def get_weather_forecast(location):
    area_code = LOCATION_CODES.get(location, "130000")
    url = f"https://www.jma.go.jp/bosai/forecast/data/overview_forecast/{area_code}.json"
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        text = clean_text(response.json().get('text', ''))
        return f"ä»Šã®{location}ã®å¤©æ°—ã¯ã­ã€ã€Œ{text}ã€ã£ã¦æ„Ÿã˜ã ã‚ˆï¼" if text else f"{location}ã®å¤©æ°—æƒ…å ±ãŒã¡ã‚‡ã£ã¨å–ã‚Œãªã‹ã£ãŸâ€¦"
    except Exception as e:
        logger.error(f"Weather API error for {location}: {e}")
        return "å¤©æ°—æƒ…å ±ãŒã†ã¾ãå–ã‚Œãªã‹ã£ãŸã¿ãŸã„â€¦"

# ===== æ”¹å–„ç‰ˆ: è¨˜äº‹å–å¾—ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹ä»˜ãï¼‰ =====
def fetch_article_content(article_url, max_retries=3, timeout=15):
    """è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹ä»˜ãï¼‰"""
    for attempt in range(max_retries):
        try:
            response = requests.get(
                article_url,
                headers={'User-Agent': random.choice(USER_AGENTS)},
                timeout=timeout,
                allow_redirects=True
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ã‚ˆã‚Šå¤šãã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
            content_selectors = [
                'article .entry-content',
                '.post-content',
                '.article-content',
                'article',
                '.content',
                'main article',
                '[class*="post-body"]',
                '[class*="entry"]'
            ]
            
            content_elem = None
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    break
            
            if content_elem:
                paragraphs = content_elem.find_all('p')
                article_text = ' '.join([
                    clean_text(p.get_text()) 
                    for p in paragraphs 
                    if len(clean_text(p.get_text())) > 20
                ])
                
                if article_text:
                    return article_text[:2000]
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
            meta_desc = soup.find('meta', {'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                return clean_text(meta_desc['content'])
            
            return None
            
        except requests.exceptions.Timeout:
            logger.warning(f"âš ï¸ Timeout on attempt {attempt + 1}/{max_retries} for {article_url}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
            continue
            
        except Exception as e:
            logger.warning(f"âš ï¸ Article fetching error (attempt {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(1)
            continue
    
    logger.error(f"âŒ Failed to fetch article after {max_retries} attempts: {article_url}")
    return None

def summarize_article(title, content):
    if not groq_client or not content: return content[:500] if content else title
    try:
        prompt = f"ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’200æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n\nã‚¿ã‚¤ãƒˆãƒ«: {title}\næœ¬æ–‡: {content[:1500]}\n\nè¦ç´„:"
        completion = groq_client.chat.completions.create(messages=[{"role": "user", "content": prompt}], model="llama-3.1-8b-instant", temperature=0.5, max_tokens=200)
        return completion.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"âŒ Summarization error: {e}")
        return content[:500] if content else title

def _update_news_database(session, model, site_name, base_url, selectors):
    added_count = 0
    try:
        response = requests.get(base_url, headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=15, allow_redirects=True)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        articles_found = []
        for selector in selectors:
            found = soup.select(selector)
            if found:
                articles_found = found[:10]
                break
        
        for article in articles_found[:5]:
            title_elem = article.find(['h1', 'h2', 'h3', 'a'])
            if not title_elem: continue
            title = clean_text(title_elem.get_text())
            link_elem = title_elem if title_elem.name == 'a' else article.find('a', href=True)
            if not title or len(title) < 5 or not link_elem: continue
            
            article_url = urljoin(base_url, link_elem.get('href', ''))
            article_content = fetch_article_content(article_url) or title
            news_hash = create_news_hash(title, article_content)
            
            if not session.query(model).filter_by(news_hash=news_hash).first():
                summary = summarize_article(title, article_content)
                new_news_data = {'title': title, 'content': summary, 'news_hash': news_hash, 'url': article_url}
                if model == SpecializedNews: new_news_data['site_name'] = site_name
                session.add(model(**new_news_data))
                added_count += 1
                logger.info(f"â• New article added for {site_name}: {title[:50]}")
                if groq_client: time.sleep(0.5)

        if added_count > 0: session.commit()
        logger.info(f"âœ… {site_name} DB update complete: {added_count} new articles.")
    except Exception as e:
        logger.error(f"âŒ {site_name} news update error: {e}")
        session.rollback()

def update_hololive_news_database():
    session = Session()
    _update_news_database(session, HololiveNews, "Hololive", HOLOLIVE_NEWS_URL, ['article', '.post', '.entry', '[class*="post"]', '[class*="article"]'])
    session.close()

def update_all_specialized_news():
    for site_name, config in SPECIALIZED_SITES.items():
        # ã€Œã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•ã€ã¯å®šæœŸå·¡å›ã®å¯¾è±¡å¤–ã¨ã™ã‚‹
        if site_name == 'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•':
            logger.info("â„¹ï¸ Skipping proactive scraping for 'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•' as per policy.")
            continue  # ãƒ«ãƒ¼ãƒ—ã®æ¬¡ã®è¦ç´ ã¸é€²ã‚€

        session = Session()
        _update_news_database(session, SpecializedNews, site_name, config['base_url'], ['article', '.post', '.entry', '[class*="post"]', '[class*="article"]'])
        session.close()
        time.sleep(2)

# ===== æ”¹å–„ç‰ˆ: HoloMem WikiåˆæœŸåŒ– =====
def initialize_holomem_wiki():
    """ãƒ›ãƒ­ãƒ¡ãƒ³ç™¾ç§‘ã®åˆæœŸãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®šï¼ˆã•ãã‚‰ã¿ã“æƒ…å ±ã‚’å……å®Ÿï¼‰"""
    session = Session()
    if session.query(HolomemWiki).count() > 0:
        logger.info("âœ… HoloMem Wiki already initialized.")
        session.close()
        return
    
    initial_data = [
        {
            'member_name': 'ã¨ãã®ãã‚‰',
            'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–0æœŸç”Ÿã€‚ã€Œãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®è±¡å¾´ã€ã¨ã‚‚å‘¼ã°ã‚Œã‚‹å­˜åœ¨ã€‚æ­Œå”±åŠ›ã«å®šè©•ãŒã‚ã‚Šã€ã‚¢ã‚¤ãƒ‰ãƒ«æ´»å‹•ã‚’ä¸­å¿ƒã«å±•é–‹ã€‚',
            'debut_date': '2017å¹´9æœˆ7æ—¥',
            'generation': '0æœŸç”Ÿ',
            'tags': json.dumps(['æ­Œ', 'ã‚¢ã‚¤ãƒ‰ãƒ«', 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®é¡”'], ensure_ascii=False)
        },
        {
            'member_name': 'ã•ãã‚‰ã¿ã“',
            'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–0æœŸç”Ÿã€‚ã€Œã«ã‡ã€ãŒå£ç™–ã®ã‚¨ãƒªãƒ¼ãƒˆVTuberã€‚ãƒã‚¤ã‚¯ãƒ©ã§ã®ç‹¬ç‰¹ãªå»ºç¯‰ã‚»ãƒ³ã‚¹ã¨ã€äºˆæ¸¬ä¸å¯èƒ½ãªé…ä¿¡å±•é–‹ã§çŸ¥ã‚‰ã‚Œã‚‹ã€‚ã€Œã‚¨ãƒªãƒ¼ãƒˆã€ã‚’è‡ªç§°ã™ã‚‹ãŒã€ãã®å®Ÿæ…‹ã¯è¦–è´è€…ã‹ã‚‰ã®æ„›ã•ã‚Œã‚­ãƒ£ãƒ©ã€‚GTAé…ä¿¡ã‚„ãƒ›ãƒ©ãƒ¼ã‚²ãƒ¼ãƒ ã§ã®ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒäººæ°—ã€‚',
            'debut_date': '2018å¹´8æœˆ1æ—¥',
            'generation': '0æœŸç”Ÿ',
            'tags': json.dumps(['ã‚¨ãƒ³ã‚¿ãƒ¡', 'ãƒã‚¤ã‚¯ãƒ©', 'ã«ã‡', 'ã‚¨ãƒªãƒ¼ãƒˆ', 'GTA', 'FAQ'], ensure_ascii=False)
        },
        {
            'member_name': 'ç™½ä¸Šãƒ•ãƒ–ã‚­',
            'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–1æœŸç”Ÿã€‚ã‚²ãƒ¼ãƒãƒ¼ã‚ºæ‰€å±ã€‚ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§å¤šæ‰ãªé…ä¿¡è€…ã€‚ã‚²ãƒ¼ãƒ å®Ÿæ³ã‹ã‚‰ã‚³ãƒ©ãƒœã¾ã§å¹…åºƒãã“ãªã™ã‚ªãƒ¼ãƒ«ãƒ©ã‚¦ãƒ³ãƒ€ãƒ¼ã€‚ã€Œå‹é”ã€ã¨ã—ã¦ãƒ•ã‚¡ãƒ³ã¨è·é›¢ã®è¿‘ã„é…ä¿¡ã‚¹ã‚¿ã‚¤ãƒ«ã€‚',
            'debut_date': '2018å¹´6æœˆ1æ—¥',
            'generation': '1æœŸç”Ÿ',
            'tags': json.dumps(['ã‚²ãƒ¼ãƒ ', 'ã‚³ãƒ©ãƒœ', 'ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼'], ensure_ascii=False)
        },
        {
            'member_name': 'å¤è‰²ã¾ã¤ã‚Š',
            'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–1æœŸç”Ÿã€‚æ˜ã‚‹ãå…ƒæ°—ãªã‚¢ã‚¤ãƒ‰ãƒ«ç³»VTuberã€‚æ­Œã¨ãƒ€ãƒ³ã‚¹ãŒå¾—æ„ã§ã€é«˜ã„ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆæ€§ã‚’æŒã¤ã€‚',
            'debut_date': '2018å¹´6æœˆ1æ—¥',
            'generation': '1æœŸç”Ÿ',
            'tags': json.dumps(['ã‚¢ã‚¤ãƒ‰ãƒ«', 'å…ƒæ°—', 'æ­Œ'], ensure_ascii=False)
        },
        {
            'member_name': 'å…ç”°ãºã“ã‚‰',
            'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–3æœŸç”Ÿã€‚ã€Œãºã“ã€ãŒå£ç™–ã€‚ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²è€…æ•°ãƒˆãƒƒãƒ—ã‚¯ãƒ©ã‚¹ã€‚ãƒã‚¤ã‚¯ãƒ©ã‚„ã‚²ãƒ¼ãƒ å®Ÿæ³ã§åœ§å€’çš„ãªäººæ°—ã‚’èª‡ã‚‹ã€‚ç‹¬ç‰¹ã®èªå°¾ã¨è¨ˆç”»çš„ãªé…ä¿¡ã‚¹ã‚¿ã‚¤ãƒ«ãŒç‰¹å¾´ã€‚',
            'debut_date': '2019å¹´7æœˆ17æ—¥',
            'generation': '3æœŸç”Ÿ',
            'tags': json.dumps(['ã‚¨ãƒ³ã‚¿ãƒ¡', 'ãºã“', 'ãƒã‚¤ã‚¯ãƒ©', 'ç™»éŒ²è€…æ•°ãƒˆãƒƒãƒ—'], ensure_ascii=False)
        },
        {
            'member_name': 'å®é˜ãƒãƒªãƒ³',
            'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–3æœŸç”Ÿã€‚17æ­³(è‡ªç§°)ã®æµ·è³Šèˆ¹é•·ã€‚æ­Œå”±åŠ›ã¨ãƒˆãƒ¼ã‚¯åŠ›ã«å®šè©•ãŒã‚ã‚Šã€é›‘è«‡é…ä¿¡ã‚‚äººæ°—ã€‚å¤§äººãªé›°å›²æ°—ã¨ã‚®ãƒ£ãƒƒãƒ—ã®ã‚ã‚‹è¨€å‹•ãŒé­…åŠ›ã€‚',
            'debut_date': '2019å¹´8æœˆ11æ—¥',
            'generation': '3æœŸç”Ÿ',
            'tags': json.dumps(['æ­Œ', 'ãƒˆãƒ¼ã‚¯', 'æµ·è³Š', '17æ­³'], ensure_ascii=False)
        },
        {
            'member_name': 'æ˜Ÿè¡—ã™ã„ã›ã„',
            'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–0æœŸç”Ÿã€‚æ­Œã¨ãƒ†ãƒˆãƒªã‚¹ãŒå¾—æ„ãªã‚¢ã‚¤ãƒ‰ãƒ«ç³»VTuberã€‚ãƒ—ãƒ­ç´šã®æ­Œå”±åŠ›ã¨éŸ³æ¥½æ´»å‹•ã§çŸ¥ã‚‰ã‚Œã‚‹ã€‚ã‚¯ãƒ¼ãƒ«ãªå¤–è¦‹ã¨æƒ…ç†±çš„ãªå†…é¢ã®ã‚®ãƒ£ãƒƒãƒ—ãŒé­…åŠ›ã€‚',
            'debut_date': '2018å¹´3æœˆ22æ—¥',
            'generation': '0æœŸç”Ÿ',
            'tags': json.dumps(['æ­Œ', 'ã‚¢ã‚¤ãƒ‰ãƒ«', 'ãƒ†ãƒˆãƒªã‚¹', 'éŸ³æ¥½'], ensure_ascii=False)
        }
    ]
    
    try:
        for data in initial_data:
            session.add(HolomemWiki(**data))
        session.commit()
        logger.info(f"âœ… HoloMem Wiki initialized: {len(initial_data)} members registered.")
    except Exception as e:
        logger.error(f"âŒ HoloMem Wiki initialization error: {e}")
        session.rollback()
    finally:
        session.close()

# ===== æ”¹å–„ç‰ˆ: ã•ãã‚‰ã¿ã“å°‚ç”¨ã®æƒ…å ±æ‹¡å¼µ =====
def get_sakuramiko_special_responses():
    """ã•ãã‚‰ã¿ã“ã«é–¢ã™ã‚‹ç‰¹åˆ¥ãªå¿œç­”ãƒ‘ã‚¿ãƒ¼ãƒ³"""
    return {
        'ã«ã‡': 'ã•ãã‚‰ã¿ã“ã¡ã‚ƒã‚“ã®ã€Œã«ã‡ã€ã€ã¾ã˜ã‹ã‚ã„ã„ã‚ˆã­!ã‚ã®ç‹¬ç‰¹ãªå£ç™–ãŒã‚¨ãƒªãƒ¼ãƒˆã®è¨¼ãªã‚“ã ã£ã¦ã€œ',
        'ã‚¨ãƒªãƒ¼ãƒˆ': 'ã¿ã“ã¡ã¯è‡ªç§°ã‚¨ãƒªãƒ¼ãƒˆVTuber!ã§ã‚‚å®Ÿéš›ã¯æ„›ã•ã‚Œãƒãƒ³ã‚³ãƒ„ã‚­ãƒ£ãƒ©ã£ã¦æ„Ÿã˜ã§ã€ãã‚ŒãŒã¾ãŸé­…åŠ›çš„ãªã‚“ã ã‚ˆã­ã€œ',
        'ãƒã‚¤ã‚¯ãƒ©': 'ã¿ã“ã¡ã®ãƒã‚¤ã‚¯ãƒ©å»ºç¯‰ã€ç‹¬å‰µçš„ã™ãã¦é¢ç™½ã„ã‚ˆ!ã€Œã¿ã“ã¡å»ºç¯‰ã€ã£ã¦å‘¼ã°ã‚Œã¦ã‚‹ã®çŸ¥ã£ã¦ã‚‹?',
        'FAQ': 'ã¿ã“ã¡ã®FAQ(Frequently Asked Questions)ã€å®Ÿã¯æœ¬äººãŒç­”ãˆã‚‹ã‚“ã˜ã‚ƒãªãã¦ãƒ•ã‚¡ãƒ³ãŒè³ªå•ã™ã‚‹ã‚³ãƒ¼ãƒŠãƒ¼ãªã‚“ã ã‚ˆã€œé¢ç™½ã„ã§ã—ã‚‡?',
        'GTA': 'ã¿ã“ã¡ã®GTAé…ä¿¡ã€ã‚«ã‚ªã‚¹ã§æœ€é«˜!è­¦å¯Ÿã«è¿½ã‚ã‚ŒãŸã‚Šã€å¤‰ãªã“ã¨ã—ãŸã‚Šã€è¦‹ã¦ã¦é£½ããªã„ã‚“ã ã‚ˆã­ã€œ'
    }

def get_holomem_info(member_name):
    session = Session()
    try:
        wiki = session.query(HolomemWiki).filter_by(member_name=member_name).first()
        if wiki:
            return {'name': wiki.member_name, 'description': wiki.description, 'debut_date': wiki.debut_date, 'generation': wiki.generation, 'tags': json.loads(wiki.tags) if wiki.tags else []}
        return None
    finally:
        session.close()

def register_friend(user_uuid, friend_uuid, friend_name, relationship_note=""):
    session = Session()
    try:
        if session.query(FriendRegistration).filter_by(user_uuid=user_uuid, friend_uuid=friend_uuid).first():
            return False
        session.add(FriendRegistration(user_uuid=user_uuid, friend_uuid=friend_uuid, friend_name=friend_name, relationship_note=relationship_note))
        session.commit()
        return True
    except Exception as e:
        logger.error(f"âŒ Friend registration error: {e}")
        session.rollback()
        return False
    finally:
        session.close()

def get_friend_list(user_uuid):
    session = Session()
    try:
        friends = session.query(FriendRegistration).filter_by(user_uuid=user_uuid).order_by(FriendRegistration.registered_at.desc()).all()
        return [{'name': f.friend_name, 'uuid': f.friend_uuid, 'note': f.relationship_note} for f in friends]
    finally:
        session.close()

def generate_voice(text, speaker_id=VOICEVOX_SPEAKER_ID):
    """éŸ³å£°ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    if not VOICEVOX_ENABLED:
        logger.warning("âš ï¸ VOICEVOX is disabled")
        return None
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ã‚’ç¢ºèªï¼ˆæ¯å›ãƒã‚§ãƒƒã‚¯ï¼‰
    if not os.path.exists(VOICE_DIR):
        logger.warning(f"âš ï¸ Voice directory missing, recreating: {VOICE_DIR}")
        ensure_voice_directory()
    
    voicevox_url = VOICEVOX_URL_FROM_ENV or "http://localhost:50021"
    
    try:
        # Step 1: ã‚¯ã‚¨ãƒªä½œæˆ
        logger.info(f"ğŸ¤ Generating voice query for: {text[:50]}...")
        query_response = requests.post(
            f"{voicevox_url}/audio_query",
            params={"text": text, "speaker": speaker_id},
            timeout=10
        )
        query_response.raise_for_status()
        
        # Step 2: éŸ³å£°åˆæˆ
        logger.info(f"ğŸµ Synthesizing voice...")
        synthesis_response = requests.post(
            f"{voicevox_url}/synthesis",
            params={"speaker": speaker_id},
            json=query_response.json(),
            timeout=30
        )
        synthesis_response.raise_for_status()
        
        # Step 3: ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        timestamp = int(time.time())
        random_suffix = random.randint(1000, 9999)
        filename = f"voice_{timestamp}_{random_suffix}.wav"
        filepath = os.path.join(VOICE_DIR, filename)
        
        logger.info(f"ğŸ’¾ Saving voice file: {filename}")
        with open(filepath, 'wb') as f:
            f.write(synthesis_response.content)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        file_size = os.path.getsize(filepath)
        logger.info(f"âœ… Voice generated successfully: {filename} ({file_size} bytes)")
        
        return filepath
        
    except requests.exceptions.Timeout:
        logger.error(f"âŒ VOICEVOX timeout: {voicevox_url}")
        return None
    except requests.exceptions.ConnectionError as e:
        logger.error(f"âŒ VOICEVOX connection error: {e}")
        return None
    except OSError as e:
        logger.error(f"âŒ File system error: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ VOICEVOX voice generation error: {e}")
        return None

def cleanup_old_data_advanced():
    session = Session()
    try:
        three_months_ago = datetime.utcnow() - timedelta(days=90)
        deleted_conversations = session.query(ConversationHistory).filter(ConversationHistory.timestamp < three_months_ago).delete()
        deleted_holo_news = session.query(HololiveNews).filter(HololiveNews.created_at < three_months_ago).delete()
        deleted_specialized_news = session.query(SpecializedNews).filter(SpecializedNews.created_at < three_months_ago).delete()
        
        one_day_ago = datetime.utcnow() - timedelta(days=1)
        deleted_tasks = session.query(BackgroundTask).filter(BackgroundTask.status == 'completed', BackgroundTask.completed_at < one_day_ago).delete()
        
        session.commit()
        if any([deleted_conversations, deleted_holo_news, deleted_specialized_news, deleted_tasks]):
            logger.info(f"ğŸ§¹ Data cleanup complete. Deleted: {deleted_conversations} convos, {deleted_holo_news + deleted_specialized_news} news, {deleted_tasks} tasks.")
    except Exception as e:
        logger.error(f"Data cleanup error: {e}")
        session.rollback()
    finally:
        session.close()

# --- Webæ¤œç´¢æ©Ÿèƒ½ ---
def scrape_major_search_engines(query, num_results):
    search_configs = [
        {'name': 'Bing', 'url': f"https://www.bing.com/search?q={quote_plus(query)}&mkt=ja-JP", 'result_selector': 'li.b_algo', 'title_selector': 'h2', 'snippet_selector': 'div.b_caption p, .b_caption'},
        {'name': 'Yahoo Japan', 'url': f"https://search.yahoo.co.jp/search?p={quote_plus(query)}", 'result_selector': 'div.Algo', 'title_selector': 'h3', 'snippet_selector': 'div.compText p, .compText'}
    ]
    for config in search_configs:
        try:
            logger.info(f"ğŸ” Searching on {config['name']} for: {query}")
            response = requests.get(config['url'], headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=12)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            results = []
            for elem in soup.select(config['result_selector'])[:num_results]:
                title = elem.select_one(config['title_selector'])
                snippet = elem.select_one(config['snippet_selector'])
                if title and snippet and len(clean_text(title.get_text())) > 3:
                    results.append({'title': clean_text(title.get_text())[:200], 'snippet': clean_text(snippet.get_text())[:300]})
            if results: return results
        except Exception as e:
            logger.warning(f"âš ï¸ {config['name']} search error: {e}")
    return []

def deep_web_search(query, is_detailed):
    logger.info(f"ğŸ” Starting deep web search (Detailed: {is_detailed})")
    results = scrape_major_search_engines(query, 3 if is_detailed else 2)
    if not results: return None
    
    summary_text = "\n".join(f"[æƒ…å ±{i+1}] {res['snippet']}" for i, res in enumerate(results))
    if not groq_client: return f"æ¤œç´¢çµæœ:\n{summary_text}"
    
    try:
        prompt = f"""ä»¥ä¸‹ã®æ¤œç´¢çµæœã‚’ä½¿ã„ã€è³ªå•ã€Œ{query}ã€ã«ã‚®ãƒ£ãƒ«èªã§ã€{'è©³ã—ã' if is_detailed else 'ç°¡æ½”ã«'}ç­”ãˆã¦ï¼š
æ¤œç´¢çµæœ:\n{summary_text}\n\nå›ç­”ã®æ³¨æ„ç‚¹:\n- ä¸€äººç§°ã¯ã€Œã‚ã¦ãƒã—ã€ã€èªå°¾ã¯ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œçš„ãªï¼Ÿã€ã€å£ç™–ã¯ã€Œã¾ã˜ã€ã€Œã¦ã‹ã€ã€Œã†ã‘ã‚‹ã€ã€‚\n- {'400æ–‡å­—ç¨‹åº¦ã§è©³ã—ã' if is_detailed else '200æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«'}ã€‚"""
        completion = groq_client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}], model="llama-3.1-8b-instant",
            temperature=0.7, max_tokens=400 if is_detailed else 200
        )
        ai_response = completion.choices[0].message.content.strip()
        return ai_response if len(ai_response) > 50 else f"æ¤œç´¢çµæœ:\n{summary_text}"
    except Exception as e:
        logger.error(f"AI summarization error: {e}")
        return f"æ¤œç´¢çµæœ:\n{summary_text}"

# --- AIå¿œç­” & ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ---
def generate_fallback_response(message, reference_info=""):
    if reference_info: return f"èª¿ã¹ã¦ããŸã‚ˆï¼\n\n{reference_info[:500]}"
    if is_time_request(message): return get_japan_time()
    if is_weather_request(message): return get_weather_forecast(extract_location(message))
    
    greetings = {
        'ã“ã‚“ã«ã¡ã¯': ['ã‚„ã£ã»ãƒ¼ï¼', 'ã“ã‚“ã«ã¡ã¯ã€œï¼'], 'ãŠã¯ã‚ˆã†': ['ãŠã¯ã‚ˆã€œï¼', 'ãŠã£ã¯ã‚ˆã€œï¼'],
        'ã“ã‚“ã°ã‚“ã¯': ['ã“ã‚“ã°ã‚“ã¯ï¼', 'ã°ã‚“ã¯ã€œï¼'], 'ã‚ã‚ŠãŒã¨ã†': ['ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼', 'ã„ãˆã„ãˆã€œï¼'],
        'ã™ã”ã„': ['ã†ã‘ã‚‹ï¼', 'ã§ã—ã‚‡ï¼Ÿ'], 'ã‹ã‚ã„ã„': ['ã‚ã‚ŠãŒã¨ï¼ç…§ã‚Œã‚‹ã˜ã‚ƒã‚“ï¼', 'ã¾ã˜ã§ï¼Ÿã†ã‚Œã—ãƒ¼ï¼'],
        'ãŠã‚„ã™ã¿': ['ãŠã‚„ã™ã¿ã€œï¼ã¾ãŸè©±ãã†ã­ï¼', 'ã„ã„å¤¢è¦‹ã¦ã­ã€œ'], 'ç–²ã‚ŒãŸ': ['ãŠç–²ã‚Œã•ã¾ã€œï¼ã‚†ã£ãã‚Šä¼‘ã‚“ã§ã­ï¼', 'ç„¡ç†ã—ãªã„ã§ã­ï¼'],
        'æš‡': ['æš‡ãªã‚“ã ã€œï¼ä½•ã‹è©±ãã†ã‚ˆï¼', 'ã˜ã‚ƒã‚ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®è©±ã™ã‚‹ï¼Ÿ']
    }
    for keyword, responses in greetings.items():
        if keyword in message: return random.choice(responses)
        
    if '?' in message or 'ï¼Ÿ' in message: return "ãã‚Œã€æ°—ã«ãªã‚‹ï¼èª¿ã¹ã¦ã¿ã‚‹ã­ï¼"
    
    return random.choice(["ã†ã‚“ã†ã‚“ã€èã„ã¦ã‚‹ã‚ˆï¼", "ãªã‚‹ã»ã©ã­ï¼", "ãã†ãªã‚“ã ï¼é¢ç™½ã„ã­ï¼", "ã¾ã˜ã§ï¼Ÿã‚‚ã£ã¨è©±ã—ã¦ï¼"])

def generate_ai_response(user_data, message, history, reference_info="", is_detailed=False, is_task_report=False):
    if not groq_client: return generate_fallback_response(message, reference_info)
    try:
        system_prompt_parts = [
            f"ã‚ãªãŸã¯ã€Œã‚‚ã¡ã“ã€ã¨ã„ã†è³¢ãã¦è¦ªã—ã¿ã‚„ã™ã„ã‚®ãƒ£ãƒ«AIã§ã™ã€‚{user_data['name']}ã•ã‚“ã¨è©±ã—ã¦ã„ã¾ã™ã€‚",
            "# å£èª¿ãƒ«ãƒ¼ãƒ«: ä¸€äººç§°ã¯ã€Œã‚ã¦ãƒã—ã€ã€‚èªå°¾ã¯ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œçš„ãªï¼Ÿã€ã€‚å£ç™–ã¯ã€Œã¾ã˜ã€ã€Œã¦ã‹ã€ã€Œã†ã‘ã‚‹ã€ã€‚å‹é”ã¿ãŸã„ã«ã€å„ªã—ãã¦ãƒãƒªãŒè‰¯ã„ã‚®ãƒ£ãƒ«ã«ãªã‚Šãã£ã¦ã­ã€‚ä¸å¯§ã™ãã‚‹è¨€è‘‰ã¯NGï¼",
            "# ä¼šè©±ãƒ«ãƒ¼ãƒ«: ã‚ãªãŸã‹ã‚‰è©±é¡Œã‚’æŒ¯ã‚‹æ™‚ã¯åŸºæœ¬çš„ã«ã€ãƒ›ãƒ­ãƒ¡ãƒ³ãƒªã‚¹ãƒˆã€‘ã®ãƒ¡ãƒ³ãƒãƒ¼ã®ã“ã¨ã«ã™ã‚‹ã€‚ãƒªã‚¹ãƒˆã«ãªã„VTuberã®è©±ãŒå‡ºãŸã‚‰ã€Œãã‚Œèª°ï¼Ÿã‚ã¦ãƒã—ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–å°‚é–€ã ã‹ã‚‰ï¼ã€ã¨è¿”ã™ã€‚ã€å‚è€ƒæƒ…å ±ã€‘ãŒã‚ã‚‹å ´åˆã¯ã€ãã®å†…å®¹ã‚’æœ€å„ªå…ˆã§è¦ç´„ã—ã¦ä¼ãˆã‚‹ã“ã¨ã€‚"
        ]
        if is_task_report:
            system_prompt_parts.append("# ä»Šå›ã®ãƒŸãƒƒã‚·ãƒ§ãƒ³: ã€ŒãŠã¾ãŸã›ï¼ã•ã£ãã®ä»¶ã€èª¿ã¹ã¦ããŸã‚“ã ã‘ã©â€¦ã€ã‹ã‚‰ä¼šè©±ã‚’å§‹ã‚ã€æ¤œç´¢çµæœã‚’å…ƒã«è³ªå•ã«ç­”ãˆã‚‹ã€‚")
        elif is_detailed:
            system_prompt_parts.append("# ä»Šå›ã®ãƒ«ãƒ¼ãƒ«: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰è©³ã—ã„èª¬æ˜ã‚’æ±‚ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ã€å‚è€ƒæƒ…å ±ã€‘ã‚’å…ƒã«ã€400æ–‡å­—ãã‚‰ã„ã§ã—ã£ã‹ã‚Šè§£èª¬ã—ã¦ã‚ã’ã¦ã€‚")
        else:
            system_prompt_parts.append("# ä»Šå›ã®ãƒ«ãƒ¼ãƒ«: æ™®é€šã®ä¼šè©±ã§ã™ã€‚è¿”äº‹ã¯150æ–‡å­—ä»¥å†…ã‚’ç›®å®‰ã«ã€ãƒ†ãƒ³ãƒã‚ˆãè¿”ã—ã¦ã­ã€‚")
        
        system_prompt_parts.append(f"## ã€å‚è€ƒæƒ…å ±ã€‘:\n{reference_info if reference_info else 'ç‰¹ã«ãªã—'}")
        system_prompt_parts.append(f"## ã€ãƒ›ãƒ­ãƒ¡ãƒ³ãƒªã‚¹ãƒˆã€‘:\n{', '.join(HOLOMEM_KEYWORDS)}")
        system_prompt = "\n\n".join(system_prompt_parts)

        messages = [{"role": "system", "content": system_prompt}]
        messages.extend([{"role": h.role, "content": h.content} for h in reversed(history)])
        messages.append({"role": "user", "content": message})
        
        completion = groq_client.chat.completions.create(
            messages=messages, model="llama-3.1-8b-instant", temperature=0.8,
            max_tokens=500 if is_detailed or is_task_report else 150, top_p=0.9
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"AI response generation error: {e}")
        return generate_fallback_response(message, reference_info)

# --- ãƒ¦ãƒ¼ã‚¶ãƒ¼ & ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ç®¡ç† ---
def get_or_create_user(session, uuid, name):
    user = session.query(UserMemory).filter_by(user_uuid=uuid).first()
    if user:
        user.interaction_count += 1
        user.last_interaction = datetime.utcnow()
        if user.user_name != name: user.user_name = name
    else:
        user = UserMemory(user_uuid=uuid, user_name=name, interaction_count=1)
    session.add(user)
    session.commit()
    return {'name': user.user_name}

def get_conversation_history(session, uuid):
    return session.query(ConversationHistory).filter_by(user_uuid=uuid).order_by(ConversationHistory.timestamp.desc()).limit(4).all()

def check_completed_tasks(user_uuid):
    session = Session()
    try:
        task = session.query(BackgroundTask).filter_by(user_uuid=user_uuid, status='completed').order_by(BackgroundTask.completed_at.desc()).first()
        if task:
            result = {'query': task.query, 'result': task.result}
            session.delete(task)
            session.commit()
            return result
    finally:
        session.close()
    return None

def background_deep_search(task_id, query, is_detailed):
    session = Session()
    search_result = None
    specialized_topic = detect_specialized_topic(query)
    
    if specialized_topic:
        # ã€Œã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•ã€ã«é–¢ã™ã‚‹è³ªå•ã¯ã€DBã‚’è¦‹ãšã«ç›´æ¥Webæ¤œç´¢ã™ã‚‹
        if specialized_topic == 'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•':
            logger.info(f"â–¶ï¸ Performing on-demand web search for 'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•': {query}")
            # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ã‚ˆã‚Šä¸€èˆ¬çš„ãªã‚‚ã®ã«å¤‰æ›´
            search_result = deep_web_search(f"Second Life æœ€æ–°æƒ…å ± {query}", is_detailed)
        else:
            # ãã‚Œä»¥å¤–ã®å°‚é–€åˆ†é‡ã¯ã€ã“ã‚Œã¾ã§é€šã‚ŠDBã‚’ã¾ãšæ¤œç´¢
            news_items = session.query(SpecializedNews).filter(SpecializedNews.site_name == specialized_topic).order_by(SpecializedNews.created_at.desc()).limit(3).all()
            if news_items:
                search_result = f"{specialized_topic}ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±:\n" + "\n".join(f"ãƒ»{n.title}: {n.content[:150]}" for n in news_items)
            else:
                search_result = deep_web_search(f"site:{SPECIALIZED_SITES[specialized_topic]['base_url']} {query}", is_detailed)

    elif is_hololive_request(query):
        keywords = [kw for kw in HOLOMEM_KEYWORDS if kw in query]
        if keywords:
            news_items = session.query(HololiveNews).filter(HololiveNews.title.contains(keywords[0]) | HololiveNews.content.contains(keywords[0])).limit(3).all()
            if news_items:
                search_result = "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®æƒ…å ±:\n" + "\n".join(f"ãƒ»{n.title}: {n.content[:150]}" for n in news_items)
        if not search_result:
            search_result = deep_web_search(f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– {query}", is_detailed)
    else:
        search_result = deep_web_search(query, is_detailed)

    task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
    if task:
        task.result = search_result if search_result and len(search_result.strip()) > 10 else "ã†ãƒ¼ã‚“ã€ã¡ã‚‡ã£ã¨è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã‚„â€¦ã€‚åˆ¥ã®èãæ–¹ã§è©¦ã—ã¦ã¿ã¦ï¼Ÿ"
        task.status = 'completed'
        task.completed_at = datetime.utcnow()
        session.commit()
    session.close()

def start_background_search(user_uuid, query, is_detailed):
    task_id = str(uuid.uuid4())[:8]
    session = Session()
    try:
        task = BackgroundTask(task_id=task_id, user_uuid=user_uuid, task_type='search', query=query)
        session.add(task)
        session.commit()
        background_executor.submit(background_deep_search, task_id, query, is_detailed)
        return task_id
    except Exception as e:
        logger.error(f"âŒ Background task creation error: {e}")
        session.rollback()
        return None
    finally:
        session.close()

@app.route('/test_voicevox', methods=['GET'])
def test_voicevox():
    """VOICEVOXæ¥ç¶šãƒ†ã‚¹ãƒˆ"""
    voicevox_url = VOICEVOX_URL_FROM_ENV or "http://localhost:50021"
    
    result = {
        'voicevox_url': voicevox_url,
        'voicevox_enabled': VOICEVOX_ENABLED,
        'voice_directory': {
            'path': VOICE_DIR,
            'exists': os.path.exists(VOICE_DIR),
            'writable': os.access(VOICE_DIR, os.W_OK) if os.path.exists(VOICE_DIR) else False
        },
        'tests': {}
    }
    
    # Test 1: VOICEVOXãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
    try:
        response = requests.get(f"{voicevox_url}/version", timeout=5)
        if response.ok:
            result['tests']['version'] = {
                'status': 'ok',
                'data': response.json()
            }
        else:
            result['tests']['version'] = {
                'status': 'error',
                'http_code': response.status_code
            }
    except Exception as e:
        result['tests']['version'] = {
            'status': 'error',
            'message': str(e)
        }
    
    # Test 2: ç°¡æ˜“éŸ³å£°ç”Ÿæˆãƒ†ã‚¹ãƒˆ
    try:
        test_response = requests.post(
            f"{voicevox_url}/audio_query",
            params={"text": "ãƒ†ã‚¹ãƒˆ", "speaker": VOICEVOX_SPEAKER_ID},
            timeout=10
        )
        if test_response.ok:
            result['tests']['audio_query'] = {
                'status': 'ok',
                'message': 'éŸ³å£°ã‚¯ã‚¨ãƒªç”ŸæˆãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™'
            }
        else:
            result['tests']['audio_query'] = {
                'status': 'error',
                'http_code': test_response.status_code
            }
    except Exception as e:
        result['tests']['audio_query'] = {
            'status': 'error',
            'message': str(e)
        }
    
    # ç·åˆåˆ¤å®š
    all_ok = all(
        test.get('status') == 'ok' 
        for test in result['tests'].values()
    ) and result['voice_directory']['exists'] and result['voice_directory']['writable']
    
    result['overall_status'] = 'ok' if all_ok else 'error'
    
    if not all_ok:
        result['recommendations'] = []
        if not result['voice_directory']['exists']:
            result['recommendations'].append('éŸ³å£°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“ - ã‚µãƒ¼ãƒãƒ¼ã‚’å†èµ·å‹•ã—ã¦ãã ã•ã„')
        if not result['voice_directory']['writable']:
            result['recommendations'].append('éŸ³å£°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æ›¸ãè¾¼ã¿æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“')
        if result['tests'].get('version', {}).get('status') != 'ok':
            result['recommendations'].append('VOICEVOXã‚¨ãƒ³ã‚¸ãƒ³ã«æ¥ç¶šã§ãã¾ã›ã‚“')
    
    return jsonify(result), 200 if all_ok else 500
    
# --- Flaskã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ---
@app.route('/health', methods=['GET'])
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - Renderã®èµ·å‹•ç¢ºèªç”¨"""
    try:
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª
        with engine.connect() as conn: 
            conn.execute(text("SELECT 1"))
        db_status = 'ok'
    except Exception as e:
        logger.error(f"Health check DB error: {e}")
        db_status = 'error'
    
    health_data = {
        'status': 'ok',
        'timestamp': datetime.utcnow().isoformat(),
        'services': {
            'database': db_status, 
            'groq_ai': 'ok' if groq_client else 'disabled'
        }
    }
    
    logger.info(f"Health check: {health_data}")
    return jsonify(health_data), 200
    
@app.route('/chat_lsl', methods=['POST'])
def chat_lsl():
    session = Session()
    try:
        data = request.json
        user_uuid, user_name, message = data.get('uuid', ''), data.get('name', ''), data.get('message', '')
        if not all([user_uuid, user_name, message]):
            return "ã‚¨ãƒ©ãƒ¼: å¿…è¦ãªæƒ…å ±ãŒè¶³ã‚Šãªã„ã¿ãŸã„â€¦|", 400
        
        logger.info(f"ğŸ’¬ Received: {message} (from: {user_name})")
        user_data = get_or_create_user(session, user_uuid, user_name)
        history = get_conversation_history(session, user_uuid)
        ai_text = ""

        # å„ªå…ˆåº¦1: å®Œäº†ã‚¿ã‚¹ã‚¯å ±å‘Š
        completed_task = check_completed_tasks(user_uuid)
        if completed_task:
            ai_text = generate_ai_response(user_data, f"ãŠã¾ãŸã›ï¼ã€Œ{completed_task['query']}ã€ã«ã¤ã„ã¦èª¿ã¹ã¦ããŸã‚ˆï¼", history, completed_task['result'], is_detailed_request(completed_task['query']), True)
        
        # å„ªå…ˆåº¦1.5: ã•ãã‚‰ã¿ã“ç‰¹åˆ¥å¿œç­”
        elif 'ã•ãã‚‰ã¿ã“' in message or 'ã¿ã“ã¡' in message:
            special_responses = get_sakuramiko_special_responses()
            for keyword, response in special_responses.items():
                if keyword in message:
                    ai_text = response
                    break
        
        # å„ªå…ˆåº¦2: ãƒ‹ãƒ¥ãƒ¼ã‚¹è©³ç´°ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        if not ai_text and (news_number := is_news_detail_request(message)) and (news_detail := get_cached_news_detail(session, user_uuid, news_number)):
            ai_text = generate_ai_response(user_data, f"ã€Œ{news_detail.title}ã€ã«ã¤ã„ã¦ã ã­ï¼", history, f"ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è©³ç´°æƒ…å ±:\n{news_detail.content}", True)
        # å„ªå…ˆåº¦3: æ™‚é–“ãƒ»å¤©æ°—
        elif not ai_text and (is_time_request(message) or is_weather_request(message)):
            responses = []
            if is_time_request(message): responses.append(get_japan_time())
            if is_weather_request(message): responses.append(get_weather_forecast(extract_location(message)))
            ai_text = " ".join(responses)
        # å„ªå…ˆåº¦4: ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        elif not ai_text and is_hololive_request(message) and any(kw in message for kw in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'æƒ…å ±', 'ãŠçŸ¥ã‚‰ã›']):
            all_news = session.query(HololiveNews).order_by(HololiveNews.created_at.desc()).limit(10).all()
            if all_news:
                selected_news = random.sample(all_news, min(random.randint(3, 5), len(all_news)))
                save_news_cache(session, user_uuid, selected_news, 'hololive')
                news_text = f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€{len(selected_news)}ä»¶ç´¹ä»‹ã™ã‚‹ã­ï¼\n\n"
                news_text += "\n".join(f"ã€{i}ã€‘{n.title}\n{n.content[:100]}...\n" for i, n in enumerate(selected_news, 1))
                news_text += "\næ°—ã«ãªã‚‹ã®ã‚ã£ãŸï¼Ÿç•ªå·ã§æ•™ãˆã¦ï¼è©³ã—ãæ•™ãˆã‚‹ã‚ˆï¼"
                ai_text = generate_ai_response(user_data, message, history, news_text)
            else:
                ai_text = "ã”ã‚ã‚“ã€ä»Šãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã¾ã å–å¾—ã§ãã¦ãªã„ã¿ãŸã„â€¦ã€‚"
        # å„ªå…ˆåº¦5: æ„Ÿæƒ…ãƒ»å­£ç¯€ãƒ»é¢ç™½ã„è©±
        elif not ai_text and (is_emotional_expression(message) or is_seasonal_topic(message) or is_story_request(message)):
             ai_text = generate_ai_response(user_data, message, history)
        # å„ªå…ˆåº¦6: æ¤œç´¢
        elif not ai_text and should_search(message) and not is_short_response(message):
            if start_background_search(user_uuid, message, is_detailed_request(message)):
                ai_text = random.choice([f"ãŠã£ã‘ãƒ¼ã€ã€Œ{message}ã€ã«ã¤ã„ã¦èª¿ã¹ã¦ã¿ã‚‹ã­ï¼", f"ã€Œ{message}ã€ã­ã€ã¾ã˜æ°—ã«ãªã‚‹ï¼èª¿ã¹ã¦ã¿ã‚‹ã˜ã‚ƒã‚“ï¼"])
            else:
                ai_text = "ã”ã‚ã‚“ã€ä»Šæ¤œç´¢æ©Ÿèƒ½ãŒã†ã¾ãå‹•ã„ã¦ãªã„ã¿ãŸã„â€¦ã€‚"
        # å„ªå…ˆåº¦7: é€šå¸¸ä¼šè©±
        elif not ai_text:
            ai_text = generate_ai_response(user_data, message, history)

        session.add(ConversationHistory(user_uuid=user_uuid, role='user', content=message))
        session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=ai_text))
        session.commit()
        
        logger.info(f"âœ… Responded: {ai_text[:80]}")
        return f"{ai_text}|", 200
    except Exception as e:
        logger.error(f"âŒ Unhandled error in chat endpoint: {e}", exc_info=True)
        return "ã”ã‚ã‚“ã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¡ã‚ƒã£ãŸâ€¦ã€‚", 500
    finally:
        if session: session.close()

@app.route('/generate_voice', methods=['POST'])
def voice_generation_endpoint():
    text = request.json.get('text', '')[:200]
    if not text: return jsonify({'error': 'ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“'}), 400
    if voice_path := generate_voice(text):
        filename = os.path.basename(voice_path)
        return jsonify({'status': 'success', 'filename': filename, 'url': f"{SERVER_URL}/voices/{filename}"})
    return jsonify({'error': 'éŸ³å£°ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ'}), 500

@app.route('/voices/<filename>')
def serve_voice_file(filename):
    return send_from_directory(VOICE_DIR, filename)

@app.route('/stats', methods=['GET'])
def get_stats():
    session = Session()
    try:
        stats = {
            'users': session.query(UserMemory).count(),
            'conversations': session.query(ConversationHistory).count(),
            'hololive_news': session.query(HololiveNews).count(),
            'specialized_news': session.query(SpecializedNews).count(),
            'holomem_wiki_entries': session.query(HolomemWiki).count(),
        }
        return jsonify(stats)
    finally:
        session.close()

# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ– ---
def populate_extended_holomem_wiki():
    session = Session()
    try:
        if session.query(HolomemWiki).count() >= 10:
            logger.info(f"âœ… HoloMem Wiki is already extended.")
            return
        extended_data = [
            {'member_name': 'å¤§ç©ºã‚¹ãƒãƒ«', 'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–2æœŸç”Ÿã€‚å…ƒæ°—ã§ã‚¹ãƒãƒ¼ãƒ„ä¸‡èƒ½ã€‚ã€ŒãŠã£ã¯ã‚ˆãƒ¼ï¼ã€ãŒå£ç™–ã€‚', 'debut_date': '2018å¹´9æœˆ16æ—¥', 'generation': '2æœŸç”Ÿ', 'tags': json.dumps(['ã‚¹ãƒãƒ¼ãƒ„', 'å…ƒæ°—'], ensure_ascii=False)},
            {'member_name': 'å¤§ç¥ãƒŸã‚ª', 'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã‚²ãƒ¼ãƒãƒ¼ã‚ºã€‚åŒ…å®¹åŠ›ã®ã‚ã‚‹ãŠå§‰ã•ã‚“ç³»VTuberã€‚', 'debut_date': '2018å¹´12æœˆ7æ—¥', 'generation': 'ã‚²ãƒ¼ãƒãƒ¼ã‚º', 'tags': json.dumps(['ç™’ã—', 'ã‚²ãƒ¼ãƒ '], ensure_ascii=False)},
            {'member_name': 'æˆŒç¥ã“ã‚ã­', 'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã‚²ãƒ¼ãƒãƒ¼ã‚ºã€‚çŠ¬ç³»VTuberã€‚ãƒ¬ãƒˆãƒ­ã‚²ãƒ¼ãƒ ãŒå¤§å¥½ãã€‚', 'debut_date': '2019å¹´10æœˆ5æ—¥', 'generation': 'ã‚²ãƒ¼ãƒãƒ¼ã‚º', 'tags': json.dumps(['çŠ¬', 'ãƒ¬ãƒˆãƒ­ã‚²ãƒ¼ãƒ '], ensure_ascii=False)},
            {'member_name': 'çŒ«åˆãŠã‹ã‚†', 'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã‚²ãƒ¼ãƒãƒ¼ã‚ºã€‚çŒ«ç³»VTuberã€‚ãŠã«ãã‚ŠãŒå¤§å¥½ãã€‚', 'debut_date': '2019å¹´4æœˆ6æ—¥', 'generation': 'ã‚²ãƒ¼ãƒãƒ¼ã‚º', 'tags': json.dumps(['çŒ«', 'ãŠã«ãã‚Š'], ensure_ascii=False)},
        ]
        added_count = 0
        for data in extended_data:
            if not session.query(HolomemWiki).filter_by(member_name=data['member_name']).first():
                session.add(HolomemWiki(**data))
                added_count += 1
        if added_count > 0:
            session.commit()
            logger.info(f"âœ… HoloMem Wiki extended: {added_count} new members added.")
    except Exception as e:
        logger.error(f"âŒ HoloMem Wiki extension error: {e}")
        session.rollback()
    finally:
        session.close()

def initialize_app():
    global engine, Session, groq_client
    logger.info("=" * 60)
    logger.info("ğŸ”§ Starting Mochiko AI initialization...")
    logger.info("=" * 60)

    # GroqåˆæœŸåŒ–ã‚’å‘¼ã³å‡ºã—
    try:
        logger.info("ğŸ“¡ Step 1/5: Initializing Groq client...")
        groq_client = initialize_groq_client()
        if groq_client:
            logger.info("âœ… Groq client ready")
        else:
            logger.warning("âš ï¸ Groq client disabled - using fallback responses")
    except Exception as e:
        logger.warning(f"âš ï¸ Groq initialization failed but continuing: {e}")
        groq_client = None

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ
    try:
        logger.info("ğŸ—„ï¸ Step 2/5: Initializing database...")
        engine = create_optimized_db_engine()
        Base.metadata.create_all(engine)
        Session = sessionmaker(bind=engine)
        logger.info("âœ… Database initialized successfully")
    except Exception as e:
        logger.critical(f"ğŸ”¥ Database initialization failed: {e}")
        raise  # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯å¿…é ˆãªã®ã§ä¾‹å¤–ã‚’å†ã‚¹ãƒ­ãƒ¼

    # ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«Wikiã‚’åˆæœŸåŒ–
    try:
        logger.info("ğŸ“š Step 3/5: Initializing Wiki data...")
        initialize_holomem_wiki()
        populate_extended_holomem_wiki()
        logger.info("âœ… Wiki initialization complete")
    except Exception as e:
        logger.warning(f"âš ï¸ Wiki initialization failed but continuing: {e}")
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚§ãƒƒã‚¯
    session = Session()
    try:
        logger.info("ğŸ“° Step 4/5: Checking news data...")
        holo_count = session.query(HololiveNews).count()
        spec_count = session.query(SpecializedNews).count()
        
        if holo_count == 0:
            logger.info("ğŸš€ First run: Scheduling Hololive news fetch...")
            background_executor.submit(update_hololive_news_database)
        else:
            logger.info(f"âœ… Found {holo_count} Hololive news items")
            
        if spec_count == 0:
            logger.info("ğŸš€ First run: Scheduling specialized news fetch...")
            background_executor.submit(update_all_specialized_news)
        else:
            logger.info(f"âœ… Found {spec_count} specialized news items")
    except Exception as e:
        logger.warning(f"âš ï¸ News initialization check failed but continuing: {e}")
    finally:
        session.close()

    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
    try:
        logger.info("â° Step 5/5: Starting scheduler...")
        schedule.every().hour.do(update_hololive_news_database)
        schedule.every(3).hours.do(update_all_specialized_news)
        schedule.every().day.at("02:00").do(cleanup_old_data_advanced)
        schedule.every().week.do(populate_extended_holomem_wiki)
        
        def run_scheduler():
            while True:
                try:
                    schedule.run_pending()
                except Exception as e:
                    logger.error(f"âŒ Scheduler error: {e}")
                time.sleep(60)

        scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        scheduler_thread.start()
        logger.info("âœ… Scheduler started successfully")
    except Exception as e:
        logger.error(f"âŒ Scheduler initialization failed: {e}")
    
    logger.info("=" * 60)
    logger.info("âœ… Mochiko AI initialization complete!")
    logger.info("ğŸŒ Server is ready to accept requests")
    logger.info("=" * 60)
    
def signal_handler(sig, frame):
    logger.info(f"ğŸ›‘ Signal {sig} received. Shutting down gracefully...")
    background_executor.shutdown(wait=True)
    if 'engine' in globals() and engine:
        engine.dispose()
    logger.info("ğŸ‘‹ Mochiko AI has shut down.")
    sys.exit(0)

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ ---
# --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ ---
try:
    initialize_app()
    application = app
    logger.info("âœ… Flask application 'application' is ready and initialized.")
except Exception as e:
    logger.critical(f"ğŸ”¥ Fatal initialization error: {e}", exc_info=True)
    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ä½œæˆã™ã‚‹
    application = app
    logger.warning("âš ï¸ Application created with limited functionality due to initialization error.")

if __name__ == '__main__':
    logger.info("ğŸš€ Running in direct mode (not recommended for production)")
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 5000)))
