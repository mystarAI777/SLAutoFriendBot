#
# Mochiko AI - Version 3.1 (Ê§úÁ¥¢Ê©üËÉΩÂÆåÂÖ®ÂÆüË£Ö + ÊñáÂ≠óÂåñ„ÅëÂØæÁ≠ñ)
#

import sys
import os
import requests
import logging
import time
import threading
import json
import re
import random
import uuid
import hashlib
from datetime import datetime, timedelta, timezone
import unicodedata

from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Text, text
from sqlalchemy.orm import declarative_base, sessionmaker
from urllib.parse import quote_plus, urljoin
from bs4 import BeautifulSoup
from groq import Groq
import google.generativeai as genai
from concurrent.futures import ThreadPoolExecutor

# --- Âü∫Êú¨Ë®≠ÂÆö ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Flask „Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥ÂàùÊúüÂåñ ---
app = Flask(__name__)
CORS(app)
app.config['JSON_AS_ASCII'] = False
app.config['JSONIFY_MIMETYPE'] = 'application/json; charset=utf-8'

# --- ÂÆöÊï∞ ---
VOICE_DIR = '/tmp/voices'
SERVER_URL = "https://slautofriendbot.onrender.com"
HOLOMEM_KEYWORDS = [
    '„Å®„Åç„ÅÆ„Åù„Çâ', '„É≠„ÉúÂ≠ê„Åï„Çì', '„Åï„Åè„Çâ„Åø„Åì', 'ÊòüË°ó„Åô„ÅÑ„Åõ„ÅÑ', 'AZKi', 'Â§úÁ©∫„É°„É´',
    '„Ç¢„Ç≠„Éª„É≠„Éº„Çº„É≥„Çø„Éº„É´', 'Ëµ§‰∫ï„ÅØ„ÅÇ„Å®', 'ÁôΩ‰∏ä„Éï„Éñ„Ç≠', 'Â§èËâ≤„Åæ„Å§„Çä', 'Êπä„ÅÇ„Åè„ÅÇ',
    'Á¥´Âí≤„Ç∑„Ç™„É≥', 'ÁôæÈ¨º„ÅÇ„ÇÑ„ÇÅ', 'ÁôíÊúà„Å°„Çá„Åì', 'Â§ßÁ©∫„Çπ„Éê„É´', 'Â§ßÁ•û„Éü„Ç™', 'Áå´Âèà„Åä„Åã„ÇÜ',
    'ÊàåÁ•û„Åì„Çç„Å≠', 'ÂÖéÁî∞„Å∫„Åì„Çâ', '‰∏çÁü•ÁÅ´„Éï„É¨„Ç¢', 'ÁôΩÈäÄ„Éé„Ç®„É´', 'ÂÆùÈêò„Éû„É™„É≥', 'Â§©Èü≥„Åã„Å™„Åü',
    'ËßíÂ∑ª„Çè„Åü„ÇÅ', 'Â∏∏Èóá„Éà„ÉØ', 'Âß´Ê£Æ„É´„Éº„Éä', 'Èõ™Ëä±„É©„Éü„Ç£', 'Ê°ÉÈà¥„Å≠„Å≠', 'ÁçÖÁôΩ„Åº„Åü„Çì',
    'Â∞æ‰∏∏„Éù„É´„Ç´', '„É©„Éó„É©„Çπ„Éª„ÉÄ„Éº„ÇØ„Éç„Çπ', 'È∑πÂ∂∫„É´„Ç§', 'ÂçöË°£„Åì„Çà„Çä', 'Ê≤ôËä±Âèâ„ÇØ„É≠„É±',
    'È¢®Áúü„ÅÑ„Çç„ÅØ', 'Ê£Æ„Ç´„É™„Ç™„Éö', 'Â∞èÈ≥•ÈÅä„Ç≠„Ç¢„É©', '‰∏Ä‰ºäÈÇ£Â∞ìÊ†ñ', '„Åå„ÅÜ„Çã„Éª„Åê„Çâ',
    '„ÉØ„Éà„ÇΩ„É≥„Éª„Ç¢„É°„É™„Ç¢', 'IRyS', '„Çª„É¨„Çπ„Éª„Éï„Ç°„Ç¶„Éä', '„Ç™„Éº„É≠„Éª„ÇØ„É≠„Éã„Éº', '‰∏ÉË©©„É†„É°„Ç§',
    '„Éè„Ç≥„Çπ„Éª„Éô„Éº„É´„Ç∫', '„Ç∑„Ç™„É™„Éª„Éé„É¥„Çß„É©', 'Âè§Áü≥„Éì„Ç∏„É•„Éº', '„Éç„É™„ÉÉ„Çµ„Éª„É¨„Ç§„É¥„É≥„ÇØ„É≠„Éï„Éà',
    '„Éï„ÉØ„ÉØ„Éª„Ç¢„Éì„Çπ„Ç¨„Éº„Éâ', '„É¢„Ç≥„Ç≥„Éª„Ç¢„Éì„Çπ„Ç¨„Éº„Éâ', '„Ç¢„É¶„É≥„ÉÄ„Éª„É™„Çπ', '„É†„Éº„Éä„Éª„Éõ„Ç∑„Éé„É¥„Ç°',
    '„Ç¢„Ç§„É©„Éã„Éª„Ç§„Ç™„Éï„Ç£„Éï„ÉÜ„Ç£„Éº„É≥', '„ÇØ„É¨„Ç§„Ç∏„Éº„Éª„Ç™„É™„Éº', '„Ç¢„Éº„Éã„É£„Éª„É°„É´„Éï„Ç£„ÉÉ„Çµ',
    '„Éë„É¥„Ç©„É™„Ç¢„Éª„É¨„Ç§„Éç', 'ÁÅ´Â®ÅÈùí', 'Èü≥‰πÉÁÄ¨Â•è', '‰∏ÄÊù°Ëéâ„ÄÖËèØ', 'ÂÑíÁÉèÈ¢®‰∫≠„Çâ„Åß„Çì',
    'ËΩü„ÅØ„Åò„ÇÅ', '„Éõ„É≠„É©„Ç§„Éñ', '„Éõ„É≠„É°„É≥', 'hololive', 'YAGOO', 'ÊΩ§ÁæΩ„Çã„Åó„ÅÇ', 'Ê°êÁîü„Ç≥„Ç≥', 'È≠î‰πÉ„Ç¢„É≠„Ç®', '‰πùÂçÅ‰πù‰ΩêÂëΩ'
]
SPECIALIZED_SITES = {
    'Blender': {'base_url': 'https://docs.blender.org/manual/ja/latest/', 'keywords': ['Blender', '„Éñ„É¨„É≥„ÉÄ„Éº']},
    'CG„Éã„É•„Éº„Çπ': {'base_url': 'https://modelinghappy.com/', 'keywords': ['CG„Éã„É•„Éº„Çπ', '3DCG', 'CG']},
    'ËÑ≥ÁßëÂ≠¶„ÉªÂøÉÁêÜÂ≠¶': {'base_url': 'https://nazology.kusuguru.co.jp/', 'keywords': ['ËÑ≥ÁßëÂ≠¶', 'ÂøÉÁêÜÂ≠¶']},
    '„Çª„Ç´„É≥„Éâ„É©„Ç§„Éï': {'base_url': 'https://community.secondlife.com/news/', 'keywords': ['„Çª„Ç´„É≥„Éâ„É©„Ç§„Éï', 'SL']},
    '„Ç¢„Éã„É°': {'base_url': 'https://animedb.jp/', 'keywords': ['„Ç¢„Éã„É°', 'anime']}
}

# --- „Ç∞„É≠„Éº„Éê„É´Â§âÊï∞ & Executor ---
background_executor = ThreadPoolExecutor(max_workers=5)
groq_client = None
gemini_model = None
Base = declarative_base()

# --- ÁßòÂØÜÊÉÖÂ†±/Áí∞Â¢ÉÂ§âÊï∞ Ë™≠„ÅøËæº„Åø ---
def get_secret(name):
    secret_file_path = f"/etc/secrets/{name}"
    if os.path.exists(secret_file_path):
        with open(secret_file_path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    return os.environ.get(name)

DATABASE_URL = get_secret('DATABASE_URL') or 'sqlite:///./test.db'
GROQ_API_KEY = get_secret('GROQ_API_KEY')
GEMINI_API_KEY = get_secret('GEMINI_API_KEY')

# --- „Éá„Éº„Çø„Éô„Éº„Çπ„É¢„Éá„É´ ---
class UserMemory(Base):
    __tablename__ = 'user_memories'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False, index=True)
    user_name = Column(String(255), nullable=False)

class ConversationHistory(Base):
    __tablename__ = 'conversation_history'
    id = Column(Integer, primary_key=True, autoincrement=True)
    user_uuid = Column(String(255), nullable=False, index=True)
    role = Column(String(10), nullable=False)
    content = Column(Text, nullable=False)
    timestamp = Column(DateTime, default=datetime.utcnow)

class BackgroundTask(Base):
    __tablename__ = 'background_tasks'
    id = Column(Integer, primary_key=True)
    task_id = Column(String(255), unique=True, nullable=False)
    user_uuid = Column(String(255), nullable=False, index=True)
    query = Column(Text, nullable=False)
    result = Column(Text)
    status = Column(String(20), default='pending')
    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime)

class UserPsychology(Base):
    __tablename__ = 'user_psychology'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False, index=True)
    user_name = Column(String(255), nullable=False)
    conversation_style = Column(String(100))
    emotional_tendency = Column(String(100))
    favorite_topics = Column(Text)
    confidence = Column(Integer, default=0)

# --- AI„ÇØ„É©„Ç§„Ç¢„É≥„ÉàÂàùÊúüÂåñ ---
def initialize_gemini_client():
    global gemini_model
    try:
        if GEMINI_API_KEY and len(GEMINI_API_KEY) > 20:
            genai.configure(api_key=GEMINI_API_KEY)
            gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')
            logger.info("‚úÖ Gemini 2.0 Flash Exp client initialized.")
        else:
            logger.warning("‚ö†Ô∏è GEMINI_API_KEY not set or invalid. Gemini disabled.")
    except Exception as e:
        logger.error(f"‚ùå Gemini client initialization failed: {e}")

def initialize_groq_client():
    global groq_client
    try:
        if GROQ_API_KEY and len(GROQ_API_KEY) > 20:
            groq_client = Groq(api_key=GROQ_API_KEY)
            logger.info("‚úÖ Llama 3.3 70B (Groq) client initialized.")
        else:
            logger.warning("‚ö†Ô∏è GROQ_API_KEY not set or invalid. Llama disabled.")
    except Exception as e:
        logger.error(f"‚ùå Groq client initialization failed: {e}")

# --- AI„É¢„Éá„É´Âëº„Å≥Âá∫„Åó ---
def call_gemini(prompt, history=None, system_context=""):
    if not gemini_model:
        return None
    try:
        full_prompt = f"{system_context}\n\n[PAST CONVERSATION]\n{history or ''}\n\n[CURRENT PROMPT]\n{prompt}"
        response = gemini_model.generate_content(full_prompt)
        return response.text.strip()
    except Exception as e:
        logger.error(f"‚ùå Gemini API error: {e}")
        return None

def call_llama_advanced(prompt, history=None, system_prompt=None):
    if not groq_client:
        return None
    try:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        if history:
            messages.extend([
                {"role": "user" if msg.role == "user" else "assistant", "content": msg.content}
                for msg in history[-5:]
            ])
        messages.append({"role": "user", "content": prompt})
        
        completion = groq_client.chat.completions.create(
            messages=messages,
            model="llama-3.3-70b-versatile",
            temperature=0.7,
            max_tokens=800
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"‚ùå Llama API error: {e}")
        return None

# --- „É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£Èñ¢Êï∞ ---
def clean_text(text):
    if not text:
        return ""
    return re.sub(r'\s+', ' ', text).strip()

def is_short_response(message):
    return len(message.strip()) <= 5

def is_explicit_search_request(message):
    return any(keyword in message for keyword in ['Ë™ø„Åπ„Å¶', 'Ê§úÁ¥¢„Åó„Å¶'])

def detect_specialized_topic(message):
    for topic, config in SPECIALIZED_SITES.items():
        if any(keyword.lower() in message.lower() for keyword in config['keywords']):
            return topic
    return None

def should_search(message):
    if is_short_response(message):
        return False
    if is_explicit_search_request(message):
        return True
    if detect_specialized_topic(message) or is_hololive_request(message):
        return True
    search_patterns = [r'„Å®„ÅØ', r'„Å´„Å§„ÅÑ„Å¶', r'Êïô„Åà„Å¶', r'Ë™∞', r'‰Ωï', r'„Å™„Åú', r'Ë©≥„Åó„Åè']
    return any(re.search(pattern, message) for pattern in search_patterns)

def is_hololive_request(message):
    return any(keyword in message for keyword in HOLOMEM_KEYWORDS)

def get_user_psychology(user_uuid):
    session = Session()
    try:
        psychology = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
        if not psychology:
            return None
        return {
            'conversation_style': psychology.conversation_style,
            'emotional_tendency': psychology.emotional_tendency,
            'favorite_topics': json.loads(psychology.favorite_topics or '[]'),
            'confidence': psychology.confidence
        }
    finally:
        session.close()

# --- üîç Ê§úÁ¥¢Ê©üËÉΩ„ÅÆÂÆåÂÖ®ÂÆüË£Ö ---
def scrape_google_search(query, max_results=5):
    """GoogleÊ§úÁ¥¢„ÅÆ„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞"""
    results = []
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        url = f"https://www.google.com/search?q={quote_plus(query)}&hl=ja"
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            for g in soup.find_all('div', class_='g')[:max_results]:
                title_elem = g.find('h3')
                snippet_elem = g.find('div', class_=['VwiC3b', 'yXK7lf'])
                
                if title_elem and snippet_elem:
                    results.append({
                        'title': clean_text(title_elem.get_text()),
                        'snippet': clean_text(snippet_elem.get_text())
                    })
        
        logger.info(f"üîç Google search found {len(results)} results for: {query}")
    except Exception as e:
        logger.error(f"‚ùå Google search error: {e}")
    
    return results

def scrape_specialized_site(topic, query):
    """Â∞ÇÈñÄ„Çµ„Ç§„Éà„ÅÆ„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞"""
    config = SPECIALIZED_SITES.get(topic)
    if not config:
        return []
    
    results = []
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(config['base_url'], headers=headers, timeout=10)
        response.encoding = 'utf-8'
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # „Çµ„Ç§„Éà„Åî„Å®„ÅÆÊßãÈÄ†„Å´Âøú„Åò„Å¶„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÔºàÁ∞°ÊòìÁâàÔºâ
            articles = soup.find_all(['article', 'div'], class_=re.compile(r'post|article|entry'))[:3]
            
            for article in articles:
                title_elem = article.find(['h1', 'h2', 'h3', 'a'])
                text_elem = article.find(['p', 'div'], class_=re.compile(r'content|excerpt|summary'))
                
                if title_elem:
                    results.append({
                        'title': clean_text(title_elem.get_text()),
                        'snippet': clean_text(text_elem.get_text()) if text_elem else ''
                    })
        
        logger.info(f"üéØ Specialized site ({topic}) found {len(results)} results")
    except Exception as e:
        logger.error(f"‚ùå Specialized site scraping error: {e}")
    
    return results

def perform_web_search(query, specialized_topic=None):
    """Áµ±ÂêàÊ§úÁ¥¢ÂÆüË°å"""
    all_results = []
    
    # Â∞ÇÈñÄ„Çµ„Ç§„ÉàÂÑ™ÂÖà
    if specialized_topic:
        all_results.extend(scrape_specialized_site(specialized_topic, query))
    
    # GoogleÊ§úÁ¥¢ÔºàÂ∞ÇÈñÄ„Çµ„Ç§„Éà„ÅßÁµêÊûú„ÅåÂ∞ë„Å™„ÅÑÂ†¥Âêà„ÅØËøΩÂä†Ôºâ
    if len(all_results) < 3:
        all_results.extend(scrape_google_search(query, max_results=5))
    
    return all_results[:5]  # ÊúÄÂ§ß5‰ª∂

# --- „Ç≥„Ç¢„É≠„Ç∏„ÉÉ„ÇØ ---
def generate_ai_response(user_data, message, history, reference_info=""):
    use_llama = len(reference_info) > 100 or any(keyword in message for keyword in ['ÂàÜÊûê', 'Ë©≥„Åó„Åè', 'Ë™¨Êòé'])
    
    psychology = get_user_psychology(user_data['uuid'])
    system_prompt_parts = [
        f"„ÅÇ„Å™„Åü„ÅØ„Äå„ÇÇ„Å°„Åì„Äç„Å®„ÅÑ„ÅÜ„ÇÆ„É£„É´AI„Åß„Åô„ÄÇ{user_data['name']}„Åï„Çì„Å®Ë©±„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ",
        "‰∏Ä‰∫∫Áß∞„ÅØ„Äå„ÅÇ„Å¶„ÅÉ„Åó„Äç„ÄÅË™ûÂ∞æ„ÅØ„Äå„Äú„Åò„ÇÉ„Çì„Äç„Äå„ÄúÁöÑ„Å™Ôºü„Äç„ÄÅÂè£Áôñ„ÅØ„Äå„Åæ„Åò„Äç„Äå„Å¶„Åã„Äç„Äå„ÅÜ„Åë„Çã„Äç„ÄÇ"
    ]
    if psychology and psychology.get('confidence', 0) > 50:
        system_prompt_parts.append(f"Áõ∏Êâã„ÅØ{psychology['conversation_style']}„Å™‰ºöË©±„ÇíÂ•Ω„Åø„Åæ„Åô„ÄÇ")
    if reference_info:
        system_prompt_parts.append(f"\n„ÄêÂèÇËÄÉÊÉÖÂ†±„Äë\n{reference_info}")
    
    system_prompt = "\n".join(system_prompt_parts)

    response = None
    if use_llama:
        logger.info("üß† Using Llama 3.3 70B for detailed response.")
        response = call_llama_advanced(message, history, system_prompt)
    
    if not response:
        logger.info("üöÄ Using Gemini 2.0 Flash for fast response.")
        history_text = "\n".join([f"{h.role}: {h.content}" for h in history])
        response = call_gemini(message, history_text, system_prompt)

    return response or "„Åî„ÇÅ„Çì„ÄÅ„Å°„Çá„Å£„Å®ËÄÉ„Åà„Åå„Åæ„Å®„Åæ„Çâ„Å™„ÅÑ„ÇÑ‚Ä¶ÔºÅ"

def background_deep_search(task_id, query, history):
    """„Éê„ÉÉ„ÇØ„Ç∞„É©„Ç¶„É≥„ÉâÊ§úÁ¥¢„ÅÆÂÆüË°åÔºà‰øÆÊ≠£ÁâàÔºâ"""
    session = Session()
    try:
        # ‚ë† ‰ºöË©±Â±•Ê≠¥„Çí‰Ωø„Å£„Å¶ÊñáËÑàÁêÜËß£
        contextual_query = query
        if history and any(kw in query for kw in ['„Åù„Çå', '„ÅÇ„Çå', '„Åù„ÅÆ‰∏≠„Åß', '„ÇÇ„Å£„Å®Ë©≥„Åó„Åè', 'Ë©≥„Åó„Åè']):
            logger.info("üß† Generating contextual search query from history...")
            history_text = "\n".join([
                f"{'„É¶„Éº„Ç∂„Éº' if h.role=='user' else '„ÇÇ„Å°„Åì'}: {h.content}"
                for h in history[-5:]
            ])
            prompt = f'''‰ª•‰∏ã„ÅÆ‰ºöË©±Â±•Ê≠¥„ÇíÂèÇËÄÉ„Å´„ÄÅÊúÄÂæå„ÅÆË≥™Âïè„ÇíËá™Â∑±ÂÆåÁµê„Åó„ÅüGoogleÊ§úÁ¥¢„ÇØ„Ç®„É™„Å´Â§âÊèõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
Ê§úÁ¥¢„ÇØ„Ç®„É™„Å†„Åë„ÇíËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ‰ΩôË®à„Å™ÊñáÁ´†„ÅØ‰∏çË¶Å„Åß„Åô„ÄÇ

[‰ºöË©±Â±•Ê≠¥]
{history_text}

[ÊúÄÂæå„ÅÆË≥™Âïè]
"{query}"

[Â§âÊèõÂæå„ÅÆÊ§úÁ¥¢„ÇØ„Ç®„É™]:'''
            
            generated_query = call_gemini(prompt)
            if generated_query:
                contextual_query = clean_text(generated_query).replace('"', '').replace('„Äå', '').replace('„Äç', '')
                logger.info(f"‚úÖ Contextual query generated: '{contextual_query}'")

        # ‚ë° Â∞ÇÈñÄ„Éà„Éî„ÉÉ„ÇØÊ§úÂá∫
        specialized_topic = detect_specialized_topic(contextual_query)
        if specialized_topic:
            logger.info(f"üéØ Detected specialized topic: {specialized_topic}")

        # ‚ë¢ WebÊ§úÁ¥¢ÂÆüË°å
        search_results = perform_web_search(contextual_query, specialized_topic)

        # ‚ë£ Ê§úÁ¥¢ÁµêÊûú„ÅÆÂá¶ÁêÜ
        if not search_results:
            search_result = f"„Äå{contextual_query}„Äç„Å´„Å§„ÅÑ„Å¶Ë™ø„Åπ„Åü„Åë„Å©„ÄÅÊÉÖÂ†±„ÅåË¶ã„Å§„Åã„Çâ„Å™„Åã„Å£„Åü„Çà‚Ä¶ÔºÅ„ÇÇ„ÅÜÂ∞ë„ÅóÂÖ∑‰ΩìÁöÑ„Å´ËÅû„ÅÑ„Å¶„Åè„Çå„ÇãÔºü"
        else:
            summary_text = "\n\n".join([
                f"„Äê{res['title']}„Äë\n{res['snippet']}"
                for res in search_results
            ])
            
            # ‚ë§ AIË¶ÅÁ¥ÑÁîüÊàêÔºàLlama‰ΩøÁî®Ôºâ
            logger.info("üß† Generating AI summary with Llama...")
            search_result = call_llama_advanced(
                f"„Äå{contextual_query}„Äç„Å´„Å§„ÅÑ„Å¶Ë™ø„Åπ„Å¶„Åç„Åü„ÇàÔºÅ‰ª•‰∏ã„ÅÆÊÉÖÂ†±„Çí„ÇÇ„Å°„ÅìÈ¢®„Å´„Åæ„Å®„ÇÅ„Å¶Êïô„Åà„Å¶„ÄÇ",
                history,
                f"„ÅÇ„Å™„Åü„ÅØ„Äå„ÇÇ„Å°„Åì„Äç„Åß„Åô„ÄÇ‰ª•‰∏ã„ÅÆÊ§úÁ¥¢ÁµêÊûú„ÇíÂü∫„Å´„ÄÅ„Çè„Åã„Çä„ÇÑ„Åô„ÅèË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n{summary_text}"
            )
            
            if not search_result:
                search_result = f"Ë™ø„Åπ„Å¶„Åç„Åü„ÇàÔºÅ\n\n{summary_text[:500]}...\n\n„Å£„Å¶ÊÑü„Åò„Åò„ÇÉ„ÇìÔºÅ"

        # ‚ë• „Çø„Çπ„ÇØÂÆå‰∫Ü„Çí‰øùÂ≠ò
        task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
        if task:
            task.result = search_result
            task.status = 'completed'
            task.completed_at = datetime.utcnow()
            session.commit()
            logger.info(f"‚úÖ Task {task_id} completed successfully")

    except Exception as e:
        logger.error(f"‚ùå Background search failed for task {task_id}: {e}", exc_info=True)
        try:
            task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
            if task:
                task.result = "„Åî„ÇÅ„Çì„ÄÅÊ§úÁ¥¢‰∏≠„Å´„Ç®„É©„Éº„ÅåËµ∑„Åç„Å°„ÇÉ„Å£„Åü‚Ä¶„ÇÇ„ÅÜ‰∏ÄÂõûË©¶„Åó„Å¶„Åø„Å¶ÔºÅ"
                task.status = 'failed'
                task.completed_at = datetime.utcnow()
                session.commit()
        except:
            pass
    finally:
        session.close()

def start_background_search(user_uuid, query, history):
    """„Éê„ÉÉ„ÇØ„Ç∞„É©„Ç¶„É≥„ÉâÊ§úÁ¥¢„ÅÆÈñãÂßã"""
    session = Session()
    try:
        task_id = str(uuid.uuid4())
        task = BackgroundTask(task_id=task_id, user_uuid=user_uuid, query=query)
        session.add(task)
        session.commit()
        
        # Â±•Ê≠¥„ÇíÊ∏°„Åó„Å¶„Éê„ÉÉ„ÇØ„Ç∞„É©„Ç¶„É≥„ÉâÂÆüË°å
        background_executor.submit(background_deep_search, task_id, query, history)
        logger.info(f"üöÄ Background search started: {task_id}")
        return task_id
    except Exception as e:
        logger.error(f"‚ùå Failed to start background search: {e}")
        session.rollback()
        return None
    finally:
        session.close()

# --- Flask „Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà ---
@app.route('/chat_lsl', methods=['POST'])
def chat_lsl():
    session = Session()
    try:
        data = request.json
        user_uuid = data.get('uuid')
        user_name = data.get('name')
        message = data.get('message')

        if not user_uuid or not message:
            return jsonify({"type": "text", "message": "ÂøÖË¶Å„Å™ÊÉÖÂ†±„Åå‰∏çË∂≥„Åó„Å¶„Çã„ÇàÔºÅ"}), 400

        user_data = {'uuid': user_uuid, 'name': user_name or '„É¶„Éº„Ç∂„Éº'}
        history = session.query(ConversationHistory)\
            .filter_by(user_uuid=user_uuid)\
            .order_by(ConversationHistory.timestamp.desc())\
            .limit(10).all()
        history.reverse()  # Âè§„ÅÑÈ†Ü„Å´‰∏¶„ÅπÊõø„Åà

        response_data = {}
        
        # Ê§úÁ¥¢„ÅåÂøÖË¶Å„ÅãÂà§ÂÆö
        if should_search(message):
            logger.info(f"üîç Search triggered for: {message}")
            task_id = start_background_search(user_uuid, message, history)
            if task_id:
                response_data = {
                    "type": "search_started",
                    "task_id": task_id,
                    "message": "„Åä„Å£„Åë„Éº„ÄÅË™ø„Åπ„Å¶„Åø„Çã„Å≠ÔºÅ„Å°„Çá„Å£„Å®ÂæÖ„Å£„Å¶„Å¶ÔºÅ"
                }
            else:
                response_data = {
                    "type": "text",
                    "message": "„Åî„ÇÅ„Çì„ÄÅ‰ªäÊ§úÁ¥¢Ê©üËÉΩ„Åå„ÅÜ„Åæ„ÅèÂãï„ÅÑ„Å¶„Å™„ÅÑ„Åø„Åü„ÅÑ‚Ä¶ÊôÆÈÄö„Å´Á≠î„Åà„Çã„Å≠ÔºÅ"
                }
                ai_text = generate_ai_response(user_data, message, history)
                response_data["message"] = ai_text
        else:
            # ÈÄöÂ∏∏„ÅÆ‰ºöË©±
            ai_text = generate_ai_response(user_data, message, history)
            response_data = {"type": "text", "message": ai_text}

        # ‰ºöË©±Â±•Ê≠¥„Çí‰øùÂ≠ò
        session.add(ConversationHistory(user_uuid=user_uuid, role='user', content=message))
        if response_data.get("message"):
            session.add(ConversationHistory(
                user_uuid=user_uuid,
                role='assistant',
                content=response_data["message"]
            ))
        session.commit()

        return jsonify(response_data)

    except Exception as e:
        logger.error(f"‚ùå Error in /chat_lsl: {e}", exc_info=True)
        return jsonify({
            "type": "text",
            "message": "„Åî„ÇÅ„Çì„ÄÅ„Çµ„Éº„Éê„Éº„Åß„Ç®„É©„Éº„ÅåËµ∑„Åç„Å°„ÇÉ„Å£„Åü‚Ä¶"
        }), 500
    finally:
        session.close()

@app.route('/check_task', methods=['POST'])
def check_task():
    """„Çø„Çπ„ÇØ„Çπ„ÉÜ„Éº„Çø„ÇπÁ¢∫Ë™ç„Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà"""
    session = Session()
    try:
        data = request.json
        task_id = data.get('task_id')
        
        if not task_id:
            return jsonify({'status': 'error', 'message': 'task_id„ÅåÂøÖË¶Å„Åß„Åô'}), 400

        task = session.query(BackgroundTask).filter_by(task_id=task_id).first()

        if not task:
            return jsonify({'status': 'not_found'}), 404

        if task.status == 'completed':
            result = task.result
            # ÂÆå‰∫Ü„Åó„Åü„Çø„Çπ„ÇØ„ÇíÂâäÈô§
            session.delete(task)
            session.commit()
            return jsonify({'status': 'completed', 'message': result})
        elif task.status == 'failed':
            result = task.result
            session.delete(task)
            session.commit()
            return jsonify({'status': 'completed', 'message': result})
        else:
            return jsonify({'status': 'pending'})

    except Exception as e:
        logger.error(f"‚ùå Error in /check_task: {e}", exc_info=True)
        return jsonify({'status': 'error', 'message': '„Çµ„Éº„Éê„Éº„Ç®„É©„Éº'}), 500
    finally:
        session.close()

@app.route('/health', methods=['GET'])
def health():
    """„Éò„É´„Çπ„ÉÅ„Çß„ÉÉ„ÇØ"""
    return jsonify({
        'status': 'healthy',
        'gemini': gemini_model is not None,
        'llama': groq_client is not None
    })

# --- „Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥ÂàùÊúüÂåñ ---
def initialize_app():
    global engine, Session
    logger.info("=" * 30 + " INITIALIZING MOCHIKO AI " + "=" * 30)
    
    initialize_gemini_client()
    initialize_groq_client()
    
    try:
        engine = create_engine(
            DATABASE_URL,
            connect_args={'check_same_thread': False} if 'sqlite' in DATABASE_URL else {},
            echo=False
        )
        Base.metadata.create_all(engine)
        Session = sessionmaker(bind=engine)
        logger.info("‚úÖ Database initialized successfully.")
    except Exception as e:
        logger.critical(f"üî• Database initialization failed: {e}")
        sys.exit(1)
    
    logger.info("=" * 30 + " INITIALIZATION COMPLETE " + "=" * 30)

# --- „É°„Ç§„É≥ÂÆüË°å ---
if __name__ == '__main__':
    initialize_app()
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 8080)), debug=False)
else:
    # Production server (gunicorn)
    initialize_app()
