# ==============================================================================
# ã‚‚ã¡ã“AI - ç©¶æ¥µã®å…¨æ©Ÿèƒ½çµ±åˆç‰ˆ (v16.2 - Final Integrated / No Omissions)
#
# ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€v16.0ã®å …ç‰¢ãªåŸºç›¤ã«ã€å¯¾è©±å‹æ¤œç´¢ãƒ•ãƒ­ãƒ¼ãªã©ã®UXå‘ä¸Šæ©Ÿèƒ½ã‚’å®Œå…¨ã«çµ±åˆã—ãŸæœ€çµ‚ç‰ˆã§ã™ã€‚
# ã„ã‹ãªã‚‹éƒ¨åˆ†ã‚‚çœç•¥ã›ãšã€ã™ã¹ã¦ã®æ©Ÿèƒ½ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ã€‚
# - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰AI (Geminié«˜é€Ÿå¿œç­” + Llama 70B é«˜ç²¾åº¦åˆ†æ)
# - è©³ç´°ãªãƒ¦ãƒ¼ã‚¶ãƒ¼å¿ƒç†åˆ†æã¨ã€ãã‚Œã‚’æ´»ç”¨ã—ãŸãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºå¿œç­”
# - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®è‡ªå‹•æš—å·åŒ–ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½ (GitHubé€£æº)
# - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æŒ‡æ‘˜ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿®æ­£æ©Ÿèƒ½
# - å¯¾è©±å‹ã®Webæ¤œç´¢ï¼†ãƒ‹ãƒ¥ãƒ¼ã‚¹é–²è¦§æ©Ÿèƒ½ï¼ˆãƒªã‚¹ãƒˆè¡¨ç¤ºã¨ç•ªå·é¸æŠï¼‰
# - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†æ©Ÿèƒ½
# - éŸ³å£°åˆæˆæ©Ÿèƒ½ã®å®Œå…¨ãªå®Ÿè£…
# - ç‰¹å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã•ãã‚‰ã¿ã“ç­‰ï¼‰ã¸ã®ç‰¹æ®Šå¿œç­”
# - å’æ¥­ç”Ÿæƒ…å ± (ã‚‚ã¡ã“ã®æ°—æŒã¡ã‚’å«ã‚€) ã®ç®¡ç†
# - å®Œå…¨ãªUTF-8æ–‡å­—åŒ–ã‘å¯¾ç­–
# - LSLã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆé€£æºç”¨ã®éåŒæœŸã‚¿ã‚¹ã‚¯ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½
# - æ´—ç·´ã•ã‚ŒãŸå„ªå…ˆåº¦åˆ†å²ã«ã‚ˆã‚‹é«˜åº¦ãªä¼šè©±ãƒ­ã‚¸ãƒƒã‚¯
# - å…¨ã¦ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã¨å …ç‰¢ãªåˆæœŸåŒ–å‡¦ç†
# ==============================================================================

# ===== ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ =====
import sys
import os
import requests
import logging
import time
import threading
import json
import re
import random
import uuid
import hashlib
from datetime import datetime, timedelta, timezone
import unicodedata
from urllib.parse import quote_plus, urljoin
from threading import Lock

# --- ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ---
from flask import Flask, request, jsonify, send_from_directory, Response
from flask_cors import CORS
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Text, Boolean, inspect, text
from sqlalchemy.orm import declarative_base, sessionmaker
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import schedule
import signal
from groq import Groq
import google.generativeai as genai

# ==============================================================================
# åŸºæœ¬è¨­å®šã¨ãƒ­ã‚®ãƒ³ã‚°
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ==============================================================================
# å®šæ•°è¨­å®š
# ==============================================================================
VOICE_DIR = '/tmp/voices'
SERVER_URL = os.environ.get('RENDER_EXTERNAL_URL', "http://localhost:5000")
background_executor = ThreadPoolExecutor(max_workers=5)
VOICEVOX_SPEAKER_ID = 20
HOLOLIVE_NEWS_URL = "https://hololive-tsuushin.com/category/holonews/"
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
]
LOCATION_CODES = { "æ±äº¬": "130000", "å¤§é˜ª": "270000", "åå¤å±‹": "230000", "ç¦å²¡": "400000", "æœ­å¹Œ": "016000" }
SPECIALIZED_SITES = {
    'Blender': {'base_url': 'https://docs.blender.org/manual/ja/latest/', 'keywords': ['Blender', 'ãƒ–ãƒ¬ãƒ³ãƒ€ãƒ¼']},
    'CGãƒ‹ãƒ¥ãƒ¼ã‚¹': {'base_url': 'https://modelinghappy.com/', 'keywords': ['CGãƒ‹ãƒ¥ãƒ¼ã‚¹', '3DCG', 'CG', 'CGæ¥­ç•Œ']},
    'è„³ç§‘å­¦ãƒ»å¿ƒç†å­¦': {'base_url': 'https://nazology.kusuguru.co.jp/', 'keywords': ['è„³ç§‘å­¦', 'å¿ƒç†å­¦', 'è„³', 'èªçŸ¥ç§‘å­¦']},
    'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•': {'base_url': 'https://community.secondlife.com/news/', 'keywords': ['ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•', 'Second Life', 'SL']},
    'ã‚¢ãƒ‹ãƒ¡': {'base_url': 'https://animedb.jp/', 'keywords': ['ã‚¢ãƒ‹ãƒ¡', 'anime']}
}
HOLOMEM_KEYWORDS = [
    'ã¨ãã®ãã‚‰', 'ãƒ­ãƒœå­ã•ã‚“', 'ã•ãã‚‰ã¿ã“', 'æ˜Ÿè¡—ã™ã„ã›ã„', 'AZKi', 'å¤œç©ºãƒ¡ãƒ«', 'ã‚¢ã‚­ãƒ»ãƒ­ãƒ¼ã‚¼ãƒ³ã‚¿ãƒ¼ãƒ«', 'èµ¤äº•ã¯ã‚ã¨', 'ç™½ä¸Šãƒ•ãƒ–ã‚­', 'å¤è‰²ã¾ã¤ã‚Š', 'æ¹Šã‚ãã‚', 'ç´«å’²ã‚·ã‚ªãƒ³', 'ç™¾é¬¼ã‚ã‚„ã‚', 'ç™’æœˆã¡ã‚‡ã“', 'å¤§ç©ºã‚¹ãƒãƒ«', 'å¤§ç¥ãƒŸã‚ª', 'çŒ«åˆãŠã‹ã‚†', 'æˆŒç¥ã“ã‚ã­', 'å…ç”°ãºã“ã‚‰', 'ä¸çŸ¥ç«ãƒ•ãƒ¬ã‚¢', 'ç™½éŠ€ãƒã‚¨ãƒ«', 'å®é˜ãƒãƒªãƒ³', 'å¤©éŸ³ã‹ãªãŸ', 'è§’å·»ã‚ãŸã‚', 'å¸¸é—‡ãƒˆãƒ¯', 'å§«æ£®ãƒ«ãƒ¼ãƒŠ', 'é›ªèŠ±ãƒ©ãƒŸã‚£', 'æ¡ƒéˆ´ã­ã­', 'ç…ç™½ã¼ãŸã‚“', 'å°¾ä¸¸ãƒãƒ«ã‚«', 'ãƒ©ãƒ—ãƒ©ã‚¹ãƒ»ãƒ€ãƒ¼ã‚¯ãƒã‚¹', 'é·¹å¶ºãƒ«ã‚¤', 'åšè¡£ã“ã‚ˆã‚Š', 'æ²™èŠ±å‰ã‚¯ãƒ­ãƒ±', 'é¢¨çœŸã„ã‚ã¯', 'æ£®ã‚«ãƒªã‚ªãƒš', 'å°é³¥éŠã‚­ã‚¢ãƒ©', 'ä¸€ä¼Šé‚£å°“æ –', 'ãŒã†ã‚‹ãƒ»ãã‚‰', 'ãƒ¯ãƒˆã‚½ãƒ³ãƒ»ã‚¢ãƒ¡ãƒªã‚¢', 'IRyS', 'ã‚»ãƒ¬ã‚¹ãƒ»ãƒ•ã‚¡ã‚¦ãƒŠ', 'ã‚ªãƒ¼ãƒ­ãƒ»ã‚¯ãƒ­ãƒ‹ãƒ¼', 'ä¸ƒè©©ãƒ ãƒ¡ã‚¤', 'ãƒã‚³ã‚¹ãƒ»ãƒ™ãƒ¼ãƒ«ã‚º', 'ã‚·ã‚ªãƒªãƒ»ãƒãƒ´ã‚§ãƒ©', 'å¤çŸ³ãƒ“ã‚¸ãƒ¥ãƒ¼', 'ãƒãƒªãƒƒã‚µãƒ»ãƒ¬ã‚¤ãƒ´ãƒ³ã‚¯ãƒ­ãƒ•ãƒˆ', 'ãƒ•ãƒ¯ãƒ¯ãƒ»ã‚¢ãƒ“ã‚¹ã‚¬ãƒ¼ãƒ‰', 'ãƒ¢ã‚³ã‚³ãƒ»ã‚¢ãƒ“ã‚¹ã‚¬ãƒ¼ãƒ‰', 'ã‚¢ãƒ¦ãƒ³ãƒ€ãƒ»ãƒªã‚¹', 'ãƒ ãƒ¼ãƒŠãƒ»ãƒ›ã‚·ãƒãƒ´ã‚¡', 'ã‚¢ã‚¤ãƒ©ãƒ‹ãƒ»ã‚¤ã‚ªãƒ•ã‚£ãƒ•ãƒ†ã‚£ãƒ¼ãƒ³', 'ã‚¯ãƒ¬ã‚¤ã‚¸ãƒ¼ãƒ»ã‚ªãƒªãƒ¼', 'ã‚¢ãƒ¼ãƒ‹ãƒ£ãƒ»ãƒ¡ãƒ«ãƒ•ã‚£ãƒƒã‚µ', 'ãƒ‘ãƒ´ã‚©ãƒªã‚¢ãƒ»ãƒ¬ã‚¤ãƒ', 'ç«å¨é’', 'éŸ³ä¹ƒç€¬å¥', 'ä¸€æ¡è‰ã€…è¯', 'å„’çƒé¢¨äº­ã‚‰ã§ã‚“', 'è½Ÿã¯ã˜ã‚', 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–', 'ãƒ›ãƒ­ãƒ¡ãƒ³', 'hololive', 'YAGOO', 'æ¡ç”Ÿã‚³ã‚³', 'æ½¤ç¾½ã‚‹ã—ã‚', 'é­”ä¹ƒã‚¢ãƒ­ã‚¨', 'ä¹åä¹ä½å‘½'
]
ANIME_KEYWORDS = [
    'ã‚¢ãƒ‹ãƒ¡', 'anime', 'ANIME', 'ï½±ï¾†ï¾’', 'ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³', 'ä½œç”»', 'å£°å„ª', 'OP', 'ED', 'åŠ‡å ´ç‰ˆ', 'æ˜ ç”»', 'åŸä½œ', 'æ¼«ç”»', 'ãƒ©ãƒãƒ™'
]

# ==============================================================================
# ç§˜å¯†æƒ…å ±/ç’°å¢ƒå¤‰æ•° èª­ã¿è¾¼ã¿
# ==============================================================================
def get_secret(name):
    secret_file_path = f"/etc/secrets/{name}"
    if os.path.exists(secret_file_path):
        try:
            with open(secret_file_path, 'r') as f:
                return f.read().strip()
        except IOError: pass
    return os.environ.get(name)

DATABASE_URL = get_secret('DATABASE_URL') or 'sqlite:///./mochiko.db'
GROQ_API_KEY = get_secret('GROQ_API_KEY')
GEMINI_API_KEY = get_secret('GEMINI_API_KEY')
VOICEVOX_URL_FROM_ENV = get_secret('VOICEVOX_URL')
ADMIN_TOKEN = get_secret('ADMIN_TOKEN')
BACKUP_ENCRYPTION_KEY = get_secret('BACKUP_ENCRYPTION_KEY')

# ==============================================================================
# AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
# ==============================================================================
groq_client = None
gemini_model = None
VOICEVOX_ENABLED = True if VOICEVOX_URL_FROM_ENV else False
fernet = None
search_context_cache = {}
cache_lock = Lock()

# ==============================================================================
# Flask & ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
# ==============================================================================
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
CORS(app)

def create_db_engine_with_retry(max_retries=5, retry_delay=5):
    from sqlalchemy.exc import OperationalError
    for attempt in range(max_retries):
        try:
            connect_args = {'check_same_thread': False} if 'sqlite' in DATABASE_URL else {'connect_timeout': 10}
            engine = create_engine(DATABASE_URL, pool_pre_ping=True, pool_recycle=300, connect_args=connect_args)
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            return engine
        except OperationalError as e:
            if attempt < max_retries - 1:
                logger.warning(f"âš ï¸ DBæ¥ç¶šå¤±æ•—: {e}. {retry_delay}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤...")
                time.sleep(retry_delay)
            else:
                raise
        except Exception as e:
            raise

engine = create_db_engine_with_retry()
Base = declarative_base()

# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« (å…¨æ©Ÿèƒ½åˆ†)
# ==============================================================================
class UserMemory(Base): __tablename__ = 'user_memories'; id = Column(Integer, primary_key=True); user_uuid = Column(String(255), unique=True, nullable=False); user_name = Column(String(255), nullable=False); interaction_count = Column(Integer, default=0); last_interaction = Column(DateTime, default=datetime.utcnow)
class ConversationHistory(Base): __tablename__ = 'conversation_history'; id = Column(Integer, primary_key=True, autoincrement=True); user_uuid = Column(String(255), nullable=False, index=True); role = Column(String(10), nullable=False); content = Column(Text, nullable=False); timestamp = Column(DateTime, default=datetime.utcnow, index=True)
class HololiveNews(Base): __tablename__ = 'hololive_news'; id = Column(Integer, primary_key=True); title = Column(String(500), nullable=False); content = Column(Text, nullable=False); url = Column(String(1000)); created_at = Column(DateTime, default=datetime.utcnow, index=True); news_hash = Column(String(100), unique=True)
class SpecializedNews(Base): __tablename__ = 'specialized_news'; id = Column(Integer, primary_key=True); site_name = Column(String(100), nullable=False, index=True); title = Column(String(500), nullable=False); content = Column(Text, nullable=False); url = Column(String(1000)); created_at = Column(DateTime, default=datetime.utcnow, index=True); news_hash = Column(String(100), unique=True)
class BackgroundTask(Base): __tablename__ = 'background_tasks'; id = Column(Integer, primary_key=True); task_id = Column(String(255), unique=True, nullable=False); user_uuid = Column(String(255), nullable=False); task_type = Column(String(50), nullable=False); query = Column(Text, nullable=False); result = Column(Text); status = Column(String(20), default='pending'); created_at = Column(DateTime, default=datetime.utcnow); completed_at = Column(DateTime)
class HolomemWiki(Base):
    __tablename__ = 'holomem_wiki'
    id = Column(Integer, primary_key=True); member_name = Column(String(100), nullable=False, unique=True, index=True); description = Column(Text); debut_date = Column(String(100)); generation = Column(String(100)); tags = Column(Text)
    status = Column(String(50), default='ç¾å½¹', nullable=False); graduation_date = Column(String(100), nullable=True); graduation_reason = Column(Text, nullable=True); mochiko_feeling = Column(Text, nullable=True); last_updated = Column(DateTime, default=datetime.utcnow)
class FriendRegistration(Base): __tablename__ = 'friend_registrations'; id = Column(Integer, primary_key=True); user_uuid = Column(String(255), nullable=False, index=True); friend_uuid = Column(String(255), nullable=False); friend_name = Column(String(255), nullable=False); registered_at = Column(DateTime, default=datetime.utcnow); relationship_note = Column(Text)
class UserPsychology(Base):
    __tablename__ = 'user_psychology'
    id = Column(Integer, primary_key=True); user_uuid = Column(String(255), unique=True, nullable=False, index=True); user_name = Column(String(255), nullable=False)
    openness = Column(Integer, default=50); conscientiousness = Column(Integer, default=50); extraversion = Column(Integer, default=50); agreeableness = Column(Integer, default=50); neuroticism = Column(Integer, default=50)
    interests = Column(Text); favorite_topics = Column(Text); conversation_style = Column(String(100)); emotional_tendency = Column(String(100)); analysis_summary = Column(Text)
    total_messages = Column(Integer, default=0); avg_message_length = Column(Integer, default=0); analysis_confidence = Column(Integer, default=0); last_analyzed = Column(DateTime)
class NewsCache(Base): __tablename__ = 'news_cache'; id = Column(Integer, primary_key=True); user_uuid = Column(String(255), nullable=False, index=True); news_id = Column(Integer, nullable=False); news_number = Column(Integer, nullable=False); news_type = Column(String(50), nullable=False); created_at = Column(DateTime, default=datetime.utcnow)
class UserContext(Base):
    __tablename__ = 'user_context'
    id = Column(Integer, primary_key=True); user_uuid = Column(String(255), unique=True, nullable=False, index=True); last_context_type = Column(String(50), nullable=False); last_query = Column(Text, nullable=True); updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)


Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)

# ==============================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ & ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° (å…¨æ©Ÿèƒ½åˆ†)
# ==============================================================================
def create_json_response(data, status=200): return Response(json.dumps(data, ensure_ascii=False), mimetype='application/json; charset=utf-8', status=status)
def clean_text(text): return re.sub(r'\s+', ' ', re.sub(r'<[^>]+>', '', text or "")).strip()
def get_japan_time(): return f"ä»Šã¯{datetime.now(timezone(timedelta(hours=9))).strftime('%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†')}ã ã‚ˆï¼"
def create_news_hash(title, content): return hashlib.md5(f"{title}{content[:100]}".encode('utf-8')).hexdigest()

def is_time_request(message): return any(keyword in message for keyword in ['ä»Šä½•æ™‚', 'æ™‚é–“', 'æ™‚åˆ»'])
def is_weather_request(message): return any(keyword in message for keyword in ['å¤©æ°—äºˆå ±', 'æ˜æ—¥ã®å¤©æ°—ã¯ï¼Ÿ', 'ä»Šæ—¥ã®å¤©æ°—ã¯ï¼Ÿ'])
def is_hololive_request(message): return any(keyword in message for keyword in HOLOMEM_KEYWORDS)
def is_friend_request(message): return any(fk in message for fk in ['å‹ã ã¡', 'å‹é”']) and any(ak in message for ak in ['ç™»éŒ²', 'èª°', 'ãƒªã‚¹ãƒˆ'])
def is_anime_request(message): return any(keyword in message for keyword in ANIME_KEYWORDS)
def detect_specialized_topic(message):
    for topic, config in SPECIALIZED_SITES.items():
        if any(keyword in message for keyword in config['keywords']): return topic
    return None
def is_explicit_search_request(message): return any(keyword in message for keyword in ['èª¿ã¹ã¦', 'æ¤œç´¢ã—ã¦', 'æ¢ã—ã¦'])
def should_search(message):
    if is_short_response(message) or is_explicit_search_request(message) or is_number_selection(message): return False
    if detect_specialized_topic(message) or is_anime_request(message): return True
    for member in HOLOMEM_KEYWORDS:
        if member in message and not any(kw in message for kw in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'æƒ…å ±']):
            if len(message.replace(member, '').strip()) > 5: return True
    patterns = [r'(?:ã¨ã¯|ã«ã¤ã„ã¦|æ•™ãˆã¦)', r'(?:èª°|ä½•|ã©ã“|ã„ã¤|ãªãœ|ã©ã†)']
    return any(re.search(pattern, message) for pattern in patterns)
def is_detailed_request(message): return any(keyword in message for keyword in ['è©³ã—ã', 'è©³ç´°', 'æ•™ãˆã¦', 'èª¬æ˜ã—ã¦'])
def is_short_response(message): return len(message.strip()) <= 3 or message.strip() in ['ã†ã‚“', 'ãã†', 'ã¯ã„', 'ãã£ã‹', 'ãªã‚‹ã»ã©']
def extract_location(message):
    for location in LOCATION_CODES.keys():
        if location in message: return location
    return "æ±äº¬"
def is_number_selection(message):
    match = re.search(r'^\s*([1-9]|[ï¼‘-ï¼™])\s*$', message.strip())
    if match: return int(unicodedata.normalize('NFKC', match.group(1)))
    return None
def detect_db_correction_request(message):
    pattern = r"(.+?)(?:(?:ã®|ã«é–¢ã™ã‚‹)(?:æƒ…å ±|ãƒ‡ãƒ¼ã‚¿))?(?:ã§|ã€|ã ã‘ã©|ã§ã™ãŒ)ã€?ã€Œ(.+?)ã€ã¯ã€Œ(.+?)ã€ãŒæ­£ã—ã„ã‚ˆ"
    match = re.search(pattern, message)
    if match:
        member_name_raw, field_raw, value_raw = match.groups()
        member_name = member_name_raw.strip()
        field = field_raw.strip()
        value = value_raw.strip()
        if member_name in HOLOMEM_KEYWORDS and field in ['èª¬æ˜', 'ãƒ‡ãƒ“ãƒ¥ãƒ¼æ—¥', 'æœŸ', 'ã‚¿ã‚°', 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹', 'å’æ¥­æ—¥', 'ã‚‚ã¡ã“ã®æ°—æŒã¡']:
            return {'member_name': member_name, 'field': field, 'value': value}
    return None
def get_sakuramiko_special_responses():
    return {
        'ã«ã‡': 'ã¿ã“ã¡ã®ã€Œã«ã‡ã€ã€ã¾ã˜ã‹ã‚ã„ã™ãã˜ã‚ƒã‚“!ã‚ã®ç‹¬ç‰¹ãªå£ç™–ãŒã‚¨ãƒªãƒ¼ãƒˆã®è¨¼ãªã‚“ã ã£ã¦ã€œã†ã‘ã‚‹!',
        'ã‚¨ãƒªãƒ¼ãƒˆ': 'ã¿ã“ã¡ã£ã¦è‡ªç§°ã‚¨ãƒªãƒ¼ãƒˆVTuberãªã‚“ã ã‘ã©ã€å®Ÿéš›ã¯æ„›ã•ã‚Œãƒãƒ³ã‚³ãƒ„ã£ã¦æ„Ÿã˜ã§ã•ã€ãã‚ŒãŒã¾ãŸæœ€é«˜ãªã‚“ã ã‚ˆã­ã€œ',
        'ãƒã‚¤ã‚¯ãƒ©': 'ã¿ã“ã¡ã®ãƒã‚¤ã‚¯ãƒ©å»ºç¯‰ã€ç‹¬å‰µçš„ã™ãã¦é¢ç™½ã„ã‚ˆ!ã€Œã¿ã“ã¡å»ºç¯‰ã€ã£ã¦å‘¼ã°ã‚Œã¦ã‚“ã®çŸ¥ã£ã¦ã‚‹?ã¾ã˜å€‹æ€§çš„!',
        'FAQ': 'ã¿ã“ã¡ã®FAQã£ã¦ã•ã€å®Ÿã¯æœ¬äººãŒç­”ãˆã‚‹ã‚“ã˜ã‚ƒãªãã¦ãƒ•ã‚¡ãƒ³ãŒè³ªå•ã™ã‚‹ã‚³ãƒ¼ãƒŠãƒ¼ãªã®!é¢ç™½ã„ã‚ˆã­ã€œ',
        'GTA': 'ã¿ã“ã¡ã®GTAé…ä¿¡ã€ã‚«ã‚ªã‚¹ã™ãã¦æœ€é«˜!è­¦å¯Ÿã«è¿½ã‚ã‚ŒãŸã‚Šå¤‰ãªã“ã¨ã—ãŸã‚Šã€è¦‹ã¦ã¦é£½ããªã„ã‚“ã ã‚ˆã­ã€œ'
    }

# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ç®¡ç†
# ==============================================================================
def get_or_create_user(session, uuid, name):
    user = session.query(UserMemory).filter_by(user_uuid=uuid).first()
    if user:
        user.interaction_count += 1
        user.last_interaction = datetime.utcnow()
        if user.user_name != name: user.user_name = name
    else:
        user = UserMemory(user_uuid=uuid, user_name=name, interaction_count=1)
        session.add(user)
    session.commit()
    return user
def get_conversation_history(session, uuid, limit=8):
    history = session.query(ConversationHistory).filter_by(user_uuid=uuid).order_by(ConversationHistory.timestamp.desc()).limit(limit).all()
    return list(reversed(history))
def check_completed_tasks(user_uuid):
    with Session() as session:
        task = session.query(BackgroundTask).filter(BackgroundTask.user_uuid == user_uuid, BackgroundTask.status == 'completed').order_by(BackgroundTask.completed_at.desc()).first()
        if task:
            query_data = json.loads(task.query)
            result = {'query': query_data.get('query', query_data) , 'result': task.result, 'type': task.task_type}
            session.delete(task); session.commit()
            return result
    return None
def start_background_task(user_uuid, task_type, query_data):
    task_id = str(uuid.uuid4())[:8]
    with Session() as session:
        session.add(BackgroundTask(task_id=task_id, user_uuid=user_uuid, task_type=task_type, query=json.dumps(query_data, ensure_ascii=False)))
        session.commit()
    
    if task_type == 'search':
        background_executor.submit(background_deep_search, task_id, query_data['query'], query_data.get('is_detailed', False))
    elif task_type == 'db_correction':
        background_executor.submit(background_db_correction, task_id, query_data)
    elif task_type == 'psych_analysis':
        background_executor.submit(analyze_user_psychology, user_uuid)
    return task_id

# ==============================================================================
# AIãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—é–¢æ•° (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰AI)
# ==============================================================================
def call_gemini(prompt, history, system_context):
    if not gemini_model: return None
    try:
        full_prompt = system_context + "\n\n"
        for msg in history[-5:]:
            full_prompt += f"{'ãƒ¦ãƒ¼ã‚¶ãƒ¼' if msg.role == 'user' else 'AI'}: {msg.content}\n"
        full_prompt += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {prompt}\nAI: "
        response = gemini_model.generate_content(full_prompt, generation_config={"temperature": 0.7, "max_output_tokens": 200})
        return response.text.strip()
    except Exception as e:
        logger.error(f"âŒ Gemini APIã‚¨ãƒ©ãƒ¼: {e}")
        return None

def call_llama_advanced(prompt, history, system_prompt, max_tokens=1000):
    if not groq_client: return None
    try:
        messages = [{"role": "system", "content": system_prompt}]
        for msg in history[-8:]:
            messages.append({"role": "user" if msg.role == "user" else "assistant", "content": msg.content})
        messages.append({"role": "user", "content": prompt})
        completion = groq_client.chat.completions.create(messages=messages, model="llama-3.3-70b-versatile", temperature=0.7, max_tokens=max_tokens)
        return completion.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"âŒ Llama APIã‚¨ãƒ©ãƒ¼: {e}")
        return None

def generate_fallback_response(message, reference_info=""):
    if reference_info:
        return f"èª¿ã¹ã¦ããŸã‚ˆï¼\n\n{reference_info[:200]}"
    greetings = {
        'ã“ã‚“ã«ã¡ã¯': ['ã‚„ã£ã»ãƒ¼ï¼', 'ã“ã‚“ã«ã¡ã¯ã€œï¼å…ƒæ°—ï¼Ÿ'], 'ãŠã¯ã‚ˆã†': ['ãŠã¯ã‚ˆã€œï¼ä»Šæ—¥ã‚‚ã„ã„å¤©æ°—ã ã­ï¼', 'ãŠã£ã¯ã‚ˆã€œï¼'],
        'ã“ã‚“ã°ã‚“ã¯': ['ã“ã‚“ã°ã‚“ã¯ï¼ä»Šæ—¥ã©ã†ã ã£ãŸï¼Ÿ', 'ã°ã‚“ã¯ã€œï¼'], 'ã‚ã‚ŠãŒã¨ã†': ['ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼', 'ã„ãˆã„ãˆã€œï¼'],
    }
    for keyword, responses in greetings.items():
        if keyword in message: return random.choice(responses)
    if '?' in message or 'ï¼Ÿ' in message:
        return random.choice(["ãã‚Œã€æ°—ã«ãªã‚‹ã­ï¼", "ã†ãƒ¼ã‚“ã€ãªã‚“ã¦è¨€ãŠã†ã‹ãªï¼", "ã¾ã˜ï¼Ÿã©ã†ã„ã†ã“ã¨ï¼Ÿ"])
    return random.choice(["ã†ã‚“ã†ã‚“ï¼", "ãªã‚‹ã»ã©ã­ï¼", "ãã†ãªã‚“ã ï¼", "ã¾ã˜ã§ï¼Ÿ"])

# ==============================================================================
# æ€§æ ¼åˆ†æ & æ´»ç”¨é–¢æ•°
# ==============================================================================
def analyze_user_psychology(user_uuid):
    with Session() as session:
        try:
            history = session.query(ConversationHistory).filter_by(user_uuid=user_uuid, role='user').order_by(ConversationHistory.timestamp.desc()).limit(100).all()
            if len(history) < 10: return
            
            messages_text = "\n".join([f"- {h.content}" for h in reversed(history)])
            analysis_prompt = f"ä»¥ä¸‹ã®ä¼šè©±å±¥æ­´ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€§æ ¼ã‚’åˆ†æã—JSONã§å‡ºåŠ›ã—ã¦ãã ã•ã„:\n\nä¼šè©±å±¥æ­´:\n{messages_text[:3000]}\n\nJSONå½¢å¼:\n{{\"openness\":50,\"conscientiousness\":50,\"extraversion\":50,\"agreeableness\":50,\"neuroticism\":50,\"interests\":[],\"favorite_topics\":[],\"conversation_style\":\"\",\"emotional_tendency\":\"\",\"analysis_summary\":\"\",\"confidence\":75}}"
            
            response_text = call_llama_advanced(analysis_prompt, [], "ã‚ãªãŸã¯å¿ƒç†å­¦ã®å°‚é–€å®¶ã§ã™ã€‚", 800)
            if not response_text: return
            result = json.loads(response_text)
            
            psych = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
            user = session.query(UserMemory).filter_by(user_uuid=user_uuid).first()
            if not psych:
                psych = UserPsychology(user_uuid=user_uuid, user_name=user.user_name if user else "Unknown"); session.add(psych)
            
            for key, value in result.items():
                if hasattr(psych, key):
                    setattr(psych, key, json.dumps(value, ensure_ascii=False) if isinstance(value, (list, dict)) else value)
            psych.last_analyzed = datetime.utcnow(); psych.total_messages = len(history)
            session.commit()
            logger.info(f"âœ… æ€§æ ¼åˆ†æå®Œäº† for {user_uuid}")
        except Exception as e:
            logger.error(f"âŒ æ€§æ ¼åˆ†æã‚¨ãƒ©ãƒ¼: {e}"); session.rollback()

def get_psychology_insight(user_uuid):
    with Session() as session:
        psych = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
        if not psych or psych.analysis_confidence < 60: return ""
        insights = []
        if psych.extraversion > 70: insights.append("ç¤¾äº¤çš„ãª")
        if psych.openness > 70: insights.append("å¥½å¥‡å¿ƒæ—ºç››ãª")
        if psych.conversation_style: insights.append(f"{psych.conversation_style}ã‚¹ã‚¿ã‚¤ãƒ«ã®")
        favorite_topics = json.loads(psych.favorite_topics) if psych.favorite_topics else []
        if favorite_topics: insights.append(f"{'ã€'.join(favorite_topics[:2])}ãŒå¥½ããª")
        return "".join(insights)

# ==============================================================================
# ã‚³ã‚¢æ©Ÿèƒ½: å¤©æ°—, Wiki, DBä¿®æ­£, ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†
# ==============================================================================
def get_weather_forecast(location):
    url = f"https://www.jma.go.jp/bosai/forecast/data/overview_forecast/{LOCATION_CODES.get(location, '130000')}.json"
    try:
        response = requests.get(url, timeout=10); response.raise_for_status()
        return f"ä»Šã®{location}ã®å¤©æ°—ã¯ã­ã€ã€Œ{clean_text(response.json().get('text', ''))}ã€ã£ã¦æ„Ÿã˜ã ã‚ˆï¼"
    except Exception as e:
        logger.error(f"å¤©æ°—APIã‚¨ãƒ©ãƒ¼: {e}"); return "å¤©æ°—æƒ…å ±ãŒã†ã¾ãå–ã‚Œãªã‹ã£ãŸã¿ãŸã„â€¦"

def get_holomem_info(member_name):
    with Session() as session:
        wiki = session.query(HolomemWiki).filter(HolomemWiki.member_name.ilike(f'%{member_name}%')).first()
        if wiki:
            return {c.name: getattr(wiki, c.name) for c in wiki.__table__.columns}
    return None

def background_db_correction(task_id, correction):
    result = f"ã€Œ{correction['member_name']}ã€ã¡ã‚ƒã‚“ã®æƒ…å ±ä¿®æ­£ã€ã‚„ã£ã¦ã¿ãŸã‘ã©å¤±æ•—ã—ã¡ã‚ƒã£ãŸâ€¦ã€‚ã”ã‚ã‚“ï¼"
    with Session() as session:
        try:
            wiki = session.query(HolomemWiki).filter_by(member_name=correction['member_name']).first()
            if wiki:
                field_map = {'èª¬æ˜': 'description', 'ãƒ‡ãƒ“ãƒ¥ãƒ¼æ—¥': 'debut_date', 'æœŸ': 'generation', 'ã‚¿ã‚°': 'tags', 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': 'status', 'å’æ¥­æ—¥': 'graduation_date', 'ã‚‚ã¡ã“ã®æ°—æŒã¡': 'mochiko_feeling'}
                db_field = field_map.get(correction['field'])
                if db_field:
                    setattr(wiki, db_field, correction['value'])
                    wiki.last_updated = datetime.utcnow()
                    session.commit()
                    result = f"ãŠã£ã‘ãƒ¼ï¼ã€Œ{correction['member_name']}ã€ã¡ã‚ƒã‚“ã®ã€Œ{correction['field']}ã€ã®æƒ…å ±ã‚’ã€Œ{correction['value']}ã€ã«æ›´æ–°ã—ã¨ã„ãŸã‚ˆï¼æ•™ãˆã¦ãã‚Œã¦ã¾ã˜åŠ©ã‹ã‚‹ï¼"
                else: result = f"ã”ã‚ã‚“ã€ã€Œ{correction['field']}ã€ã£ã¦ã„ã†é …ç›®ã¯ãªã„ã¿ãŸã„â€¦"
            else: result = f"ã”ã‚ã‚“ã€ã€Œ{correction['member_name']}ã€ã¡ã‚ƒã‚“ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸâ€¦"
        except Exception as e:
            logger.error(f"âŒ DB Correction error: {e}"); session.rollback()
        
        task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
        if task:
            task.result = result; task.status = 'completed'; task.completed_at = datetime.utcnow()
            session.commit()

def save_user_context(session, user_uuid, context_type, query):
    context = session.query(UserContext).filter_by(user_uuid=user_uuid).first()
    if not context:
        context = UserContext(user_uuid=user_uuid, last_context_type=context_type, last_query=query)
        session.add(context)
    else:
        context.last_context_type = context_type
        context.last_query = query
    session.commit()

def get_user_context(session, user_uuid):
    context = session.query(UserContext).filter_by(user_uuid=user_uuid).first()
    if context and (datetime.utcnow() - context.updated_at).total_seconds() < 600: # 10åˆ†æœ‰åŠ¹
        return {'type': context.last_context_type, 'query': context.last_query}
    return None

# ==============================================================================
# AIå¿œç­”ç”Ÿæˆ (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰ˆ)
# ==============================================================================
def generate_ai_response(user_data, message, history, reference_info="", is_detailed=False, is_task_report=False):
    if not gemini_model and not groq_client:
        return generate_fallback_response(message, reference_info)

    use_llama = is_detailed or is_task_report or len(reference_info) > 100 or any(kw in message for kw in ['åˆ†æ', 'è©³ã—ã', 'èª¬æ˜'])
    personality_context = get_psychology_insight(user_data['uuid'])
    system_prompt = f"ã‚ãªãŸã¯ã€Œã‚‚ã¡ã“ã€ã¨ã„ã†ã‚®ãƒ£ãƒ«AIã§ã™ã€‚{user_data['name']}ã•ã‚“ã¨è©±ã—ã¦ã„ã¾ã™ã€‚\n# å£èª¿ãƒ«ãƒ¼ãƒ«\n- ä¸€äººç§°ã¯ã€Œã‚ã¦ãƒã—ã€ã€‚èªå°¾ã¯ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œçš„ãªï¼Ÿã€ã€‚å£ç™–ã¯ã€Œã¾ã˜ã€ã€Œã¦ã‹ã€ã€Œã†ã‘ã‚‹ã€ã€‚\n# ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±\n- {user_data['name']}ã•ã‚“ã¯ã€Œ{personality_context}äººã€ã¨ã„ã†å°è±¡ã ã‚ˆã€‚ã“ã®æƒ…å ±ã‚’ä¼šè©±ã«æ´»ã‹ã—ã¦ã­ã€‚"
    if is_task_report:
        system_prompt += "\n# ä»Šå›ã®ãƒŸãƒƒã‚·ãƒ§ãƒ³\n- ã€ŒãŠã¾ãŸã›ï¼ã•ã£ãã®ä»¶ã ã‘ã©â€¦ã€ã¨åˆ‡ã‚Šå‡ºã—ã€ã€å‚è€ƒæƒ…å ±ã€‘ã‚’å…ƒã«è³ªå•ã«ç­”ãˆã¦ã€‚"
    system_prompt += f"\n## ã€å‚è€ƒæƒ…å ±ã€‘:\n{reference_info if reference_info else 'ç‰¹ã«ãªã—'}"

    try:
        if use_llama and groq_client:
            logger.info("ğŸ§  Llama 3.3 70Bã‚’ä½¿ç”¨ (é«˜ç²¾åº¦)")
            response = call_llama_advanced(message, history, system_prompt, 500 if is_detailed else 300)
            if response: return response
            logger.warning("âš ï¸ Llamaå¤±æ•—ã€Geminiã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        
        if gemini_model:
            logger.info("ğŸš€ Gemini 2.0 Flashã‚’ä½¿ç”¨ (é«˜é€Ÿ)")
            response = call_gemini(message, history, system_prompt)
            if response: return response

        logger.error("âš ï¸ å…¨ã¦ã®AIãƒ¢ãƒ‡ãƒ«ãŒå¤±æ•—")
        return generate_fallback_response(message, reference_info)
    except Exception as e:
        logger.error(f"âŒ AIå¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return "ã†ã…ã€AIã®èª¿å­ãŒæ‚ªã„ã¿ãŸã„â€¦ã”ã‚ã‚“ã­ï¼"

# ==============================================================================
# å¤–éƒ¨æƒ…å ±æ¤œç´¢æ©Ÿèƒ½ (Webæ¤œç´¢, Wikiæ¤œç´¢, ã‚¢ãƒ‹ãƒ¡æ¤œç´¢)
# ==============================================================================
def scrape_major_search_engines(query, num_results):
    search_configs = [
        {'name': 'Bing', 'url': f"https://www.bing.com/search?q={quote_plus(query)}&mkt=ja-JP", 'result_selector': 'li.b_algo', 'title_selector': 'h2', 'snippet_selector': 'div.b_caption p, .b_caption'},
        {'name': 'Yahoo Japan', 'url': f"https://search.yahoo.co.jp/search?p={quote_plus(query)}", 'result_selector': 'div.Algo', 'title_selector': 'h3', 'snippet_selector': 'div.compText p, .compText'}
    ]
    for config in search_configs:
        try:
            response = requests.get(config['url'], headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=12); response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser'); results = []
            for elem in soup.select(config['result_selector'])[:num_results]:
                title = elem.select_one(config['title_selector']); snippet = elem.select_one(config['snippet_selector'])
                if title and snippet: results.append({'title': clean_text(title.get_text()), 'snippet': clean_text(snippet.get_text()), 'full_content': clean_text(snippet.get_text())})
            if results: return results
        except Exception as e: logger.warning(f"âš ï¸ {config['name']} search error: {e}")
    return []
def search_anime_database(query):
    url = f"https://animedb.jp/search?q={quote_plus(query)}"
    try:
        response = requests.get(url, headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=15); response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser'); results = []
        for elem in soup.select('div.anime-item, div.search-result')[:3]:
            title = elem.select_one('h2, h3, a'); description = elem.select_one('p, div.description')
            if title: results.append(f"ã€{clean_text(title.get_text())}ã€‘\n{clean_text(description.get_text()) if description else 'è©³ç´°æƒ…å ±ãªã—'}")
        return "\n\n".join(results) if results else None
    except Exception as e:
        logger.error(f"âŒ Anime search error: {e}"); return None
def search_hololive_wiki(member_name, query_topic):
    url = f"https://seesaawiki.jp/hololivetv/search?query={quote_plus(f'{member_name} {query_topic}'.encode('euc-jp'))}"
    try:
        response = requests.get(url, headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=15); response.encoding = 'euc-jp'; response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        page_text = clean_text(soup.select_one('#pagebody, .contents').get_text())
        sentences = re.split(r'[ã€‚ï¼\n]', page_text)
        relevant = [s.strip() for s in sentences if member_name in s and query_topic in s]
        return " ".join(relevant)[:1000] if relevant else page_text[:500]
    except Exception as e:
        logger.error(f"âŒ Hololive Wiki search error: {e}"); return None
def background_deep_search(task_id, query, is_detailed):
    search_result = ""
    try:
        if is_anime_request(query):
            anime_result = search_anime_database(query)
            if anime_result:
                search_result = json.dumps([{'title': 'ã‚¢ãƒ‹ãƒ¡DBã‹ã‚‰ã®æƒ…å ±', 'snippet': anime_result, 'full_content': anime_result}], ensure_ascii=False)
        
        if not search_result:
            raw_results = scrape_major_search_engines(query, 5)
            if raw_results:
                formatted_results = [{'number': i, **r} for i, r in enumerate(raw_results, 1)]
                search_result = json.dumps(formatted_results, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ Background search error: {e}")

    with Session() as session:
        task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
        if task:
            task.result = search_result; task.status = 'completed'; task.completed_at = datetime.utcnow()
            session.commit()

# ==============================================================================
# DBãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½
# ==============================================================================
def commit_encrypted_backup_to_github():
    if not fernet: logger.error("âŒ æš—å·åŒ–ã‚­ãƒ¼ãŒæœªè¨­å®šã®ãŸã‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚"); return
    logger.info("ğŸš€ Committing encrypted backup to GitHub...")
    try:
        with Session() as session:
            backup_data = {'timestamp': datetime.utcnow().isoformat(), 'tables': {}}
            tables = {'user_memories': UserMemory, 'user_psychology': UserPsychology, 'holomem_wiki': HolomemWiki}
            for name, model in tables.items():
                records = session.query(model).all()
                backup_data['tables'][name] = [{c.name: getattr(r, c.name).isoformat() if isinstance(getattr(r, c.name), datetime) else getattr(r, c.name) for c in r.__table__.columns} for r in records]
        
        json_data = json.dumps(backup_data, ensure_ascii=False).encode('utf-8')
        encrypted_data = fernet.encrypt(json_data)
        
        os.makedirs(BACKUP_DIR, exist_ok=True)
        backup_file = os.path.join(BACKUP_DIR, 'database_backup.json.encrypted')
        with open(backup_file, 'wb') as f:
            f.write(encrypted_data)
        
        repo_backup_file = os.path.join(os.getcwd(), 'database_backup.json.encrypted')
        os.rename(backup_file, repo_backup_file)

        commands = [
            ['git', 'config', 'user.email', 'mochiko-bot@example.com'],
            ['git', 'config', 'user.name', 'Mochiko Backup Bot'],
            ['git', 'add', repo_backup_file],
            ['git', 'commit', '-m', f'ğŸ”’ Encrypted DB Backup {datetime.utcnow().isoformat()}'],
            ['git', 'push']
        ]
        for cmd in commands:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
            if result.returncode != 0 and 'nothing to commit' not in result.stdout:
                logger.error(f"âŒ Git command failed: {cmd}\n{result.stderr}")
                return
        logger.info("âœ… Encrypted backup committed to GitHub")
    except Exception as e:
        logger.error(f"âŒ GitHub commit error: {e}")

def require_admin_auth(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not ADMIN_TOKEN: return create_json_response({'error': 'Server configuration error'}, 500)
        auth_header = request.headers.get('Authorization')
        if not auth_header or f'Bearer {ADMIN_TOKEN}' != auth_header:
            return create_json_response({'error': 'Invalid credentials'}, 401)
        return f(*args, **kwargs)
    return decorated_function

# ==============================================================================
# Flask ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ (æ–‡å­—åŒ–ã‘å¯¾ç­–æ¸ˆã¿ & å®Œå…¨ç‰ˆ)
# ==============================================================================
@app.route('/health')
def health_check():
    return create_json_response({'status': 'ok'})

@app.route('/chat_lsl', methods=['POST'])
def chat_lsl():
    with Session() as session:
        try:
            data = request.json
            user_uuid, user_name, message = data['uuid'], data['name'], data['message'].strip()
            
            user_data_obj = get_or_create_user(session, user_uuid, user_name)
            history = get_conversation_history(session, user_uuid)
            session.add(ConversationHistory(user_uuid=user_uuid, role='user', content=message))
            
            ai_text = ""
            user_data = {'uuid': user_uuid, 'name': user_data_obj.user_name}

            # å„ªå…ˆåº¦1: å®Œäº†ã‚¿ã‚¹ã‚¯
            completed_task = check_completed_tasks(user_uuid)
            if completed_task:
                query = completed_task['query']
                result = completed_task['result']
                task_type = completed_task['type']

                if task_type == 'search':
                    search_results = json.loads(result) if result else None
                    if search_results:
                        with cache_lock:
                            search_context_cache[user_uuid] = {'results': search_results, 'query': query, 'timestamp': time.time()}
                        save_user_context(session, user_uuid, 'web_search', query)
                        list_items = [f"ã€{r['number']}ã€‘{r['title']}" for r in search_results]
                        ai_text = f"ãŠã¾ãŸã›ï¼ã€Œ{query}ã€ã«ã¤ã„ã¦èª¿ã¹ã¦ããŸã‚ˆï¼\n" + "\n".join(list_items) + "\n\næ°—ã«ãªã‚‹ç•ªå·æ•™ãˆã¦ï¼"
                    else:
                        ai_text = f"ã”ã‚ã‚“ã€ã€Œ{query}ã€ã«ã¤ã„ã¦èª¿ã¹ãŸã‘ã©ã€è‰¯ã„æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸâ€¦"
                elif task_type == 'db_correction':
                     ai_text = result
                elif task_type == 'psych_analysis':
                    psych = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
                    ai_text = f"åˆ†æçµ‚ã‚ã£ãŸã‚ˆï¼ã‚ã¦ãƒã—ãŒè¦‹ãŸã‚ãªãŸã¯â€¦ã€Œ{psych.analysis_summary}ã€ã£ã¦æ„Ÿã˜ï¼" if psych else "åˆ†æçµ‚ã‚ã£ãŸã‘ã©ã€ã¾ã ã†ã¾ãã¾ã¨ã‚ã‚‰ã‚Œãªã„ã‚„â€¦"

            # å„ªå…ˆåº¦2: ç•ªå·é¸æŠ
            elif (selected_number := is_number_selection(message)):
                user_context = get_user_context(session, user_uuid)
                if user_context and user_context['type'] == 'web_search':
                    with cache_lock:
                        cached_data = search_context_cache.get(user_uuid)
                    if cached_data and (time.time() - cached_data['timestamp']) < 600:
                        selected_item = next((r for r in cached_data['results'] if r.get('number') == selected_number), None)
                        if selected_item:
                            prompt = f"ã€Œ{selected_item['title']}ã€ã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦"
                            ai_text = generate_ai_response(user_data, prompt, history, selected_item['full_content'], is_detailed=True)
                        else:
                            ai_text = "ã‚ã‚Œã€ãã®ç•ªå·ã®æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‚„â€¦"
                    else:
                        ai_text = "ã”ã‚ã‚“ã€å‰ã®æ¤œç´¢çµæœãŒå¤ããªã£ã¡ã‚ƒã£ãŸï¼ã‚‚ã†ä¸€å›æ¤œç´¢ã—ã¦ã¿ã¦ï¼"
                else:
                    ai_text = "ãˆã€ãªã‚“ã®ç•ªå·ã ã£ã‘ï¼Ÿä½•ã‹ã‚’èª¿ã¹ã¦ã‹ã‚‰ç•ªå·ã§æ•™ãˆã¦ã­ï¼"
            
            # å„ªå…ˆåº¦3: æ©Ÿèƒ½çš„ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            elif 'æ€§æ ¼åˆ†æ' in message:
                start_background_task(user_uuid, 'psych_analysis', {}); ai_text = "ãŠã£ã‘ãƒ¼ï¼ã‚ãªãŸã®æ€§æ ¼ã€åˆ†æã—ã¦ã¿ã‚‹ã­ï¼çµ‚ã‚ã£ãŸã‚‰æ•™ãˆã‚‹ã‹ã‚‰ã€ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã¦ï¼"
            elif (correction_req := detect_db_correction_request(message)):
                start_background_task(user_uuid, 'db_correction', correction_req); ai_text = f"ãˆã€ã¾ã˜ã§ï¼ï¼Ÿã€Œ{correction_req['member_name']}ã€ã¡ã‚ƒã‚“ã®æƒ…å ±ã€ç›´ã—ã¦ã¿ã‚‹ã­ï¼"
            elif is_time_request(message): ai_text = get_japan_time()
            elif is_weather_request(message): ai_text = get_weather_forecast(extract_location(message))
            elif ('ã•ãã‚‰ã¿ã“' in message or 'ã¿ã“ã¡' in message):
                for keyword, resp in get_sakuramiko_special_responses().items():
                    if keyword in message:
                        ai_text = resp; break
            elif should_search(message) or is_explicit_search_request(message):
                start_background_task(user_uuid, 'search', {'query': message, 'is_detailed': is_detailed_request(message)}); ai_text = f"ãŠã£ã‘ãƒ¼ã€ã€Œ{message}ã€ã«ã¤ã„ã¦èª¿ã¹ã¦ã¿ã‚‹ã­ï¼ã¡ã‚‡ã„å¾…ã£ã¦ã¦ï¼"
            
            # å„ªå…ˆåº¦4: é€šå¸¸ä¼šè©±
            if not ai_text:
                ai_text = generate_ai_response(user_data, message, history)
            
            # è‡ªå‹•æ€§æ ¼åˆ†æ
            if user_data_obj.interaction_count % 50 == 0 and user_data_obj.interaction_count > 10:
                start_background_task(user_uuid, 'psych_analysis', {})

            session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=ai_text))
            session.commit()
            
            return Response(f"{ai_text}|", mimetype='text/plain; charset=utf-8', status=200)

        except Exception as e:
            logger.error(f"âŒ Chatã‚¨ãƒ©ãƒ¼: {e}", exc_info=True); session.rollback()
            return Response("ã”ã‚ã‚“ã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¡ã‚ƒã£ãŸâ€¦|", mimetype='text/plain; charset=utf-8', status=500)

@app.route('/check_task', methods=['POST'])
def check_task_endpoint():
    data = request.json
    user_uuid = data.get('uuid', '')
    if not user_uuid: return create_json_response({'error': 'uuid is required'}, 400)
    
    completed_task = check_completed_tasks(user_uuid)
    if completed_task:
        return create_json_response({'status': 'completed', 'task': completed_task})
    return create_json_response({'status': 'pending'})

@app.route('/admin/backup', methods=['POST'])
@require_admin_auth
def admin_backup():
    background_executor.submit(commit_encrypted_backup_to_github)
    return create_json_response({'status': 'Backup process started in background.'})

@app.route('/generate_voice', methods=['POST'])
def generate_voice_endpoint():
    if not VOICEVOX_ENABLED:
        return create_json_response({'error': 'Voice synthesis is not enabled on the server.'}, 503)
    data = request.json
    text = data.get('text')
    if not text:
        return create_json_response({'error': 'Text is required.'}, 400)

    try:
        query_res = requests.post(f"{VOICEVOX_URL_FROM_ENV}/audio_query", params={"text": text, "speaker": VOICEVOX_SPEAKER_ID}, timeout=10)
        query_res.raise_for_status()
        
        synth_res = requests.post(f"{VOICEVOX_URL_FROM_ENV}/synthesis", params={"speaker": VOICEVOX_SPEAKER_ID}, json=query_res.json(), timeout=30)
        synth_res.raise_for_status()
        
        os.makedirs(VOICE_DIR, exist_ok=True)
        filename = f"voice_{uuid.uuid4()}.wav"
        filepath = os.path.join(VOICE_DIR, filename)
        
        with open(filepath, 'wb') as f:
            f.write(synth_res.content)
            
        voice_url = urljoin(SERVER_URL, f'/voices/{filename}')
        return create_json_response({'status': 'success', 'url': voice_url})

    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ VOICEVOX APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return create_json_response({'error': 'Failed to connect to the voice synthesis server.'}, 500)
    except Exception as e:
        logger.error(f"âŒ éŸ³å£°ç”Ÿæˆä¸­ã®ä¸æ˜ãªã‚¨ãƒ©ãƒ¼: {e}")
        return create_json_response({'error': 'An unexpected error occurred during voice generation.'}, 500)

@app.route('/get_psychology', methods=['GET'])
def get_psychology_endpoint():
    user_uuid = request.args.get('user_uuid')
    if not user_uuid:
        return create_json_response({'error': 'user_uuid parameter is required'}, 400)
    
    with Session() as session:
        psych = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
        if not psych:
            return create_json_response({'status': 'not_analyzed', 'message': 'No psychology data found for this user.'})
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸ã«å¤‰æ›
        psych_data = {c.name: getattr(psych, c.name) for c in psych.__table__.columns}
        
        # JSONå½¢å¼ã®æ–‡å­—åˆ—ã‚’Pythonã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        for key in ['interests', 'favorite_topics']:
            if isinstance(psych_data[key], str):
                try:
                    psych_data[key] = json.loads(psych_data[key])
                except json.JSONDecodeError:
                    psych_data[key] = [] # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ç©ºãƒªã‚¹ãƒˆ
        
        # æ—¥ä»˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ISOå½¢å¼ã®æ–‡å­—åˆ—ã«å¤‰æ›
        if isinstance(psych_data.get('last_analyzed'), datetime):
            psych_data['last_analyzed'] = psych_data['last_analyzed'].isoformat()

        return create_json_response(psych_data)

@app.route('/voices/<filename>')
def serve_voice_file(filename):
    return send_from_directory(VOICE_DIR, filename)

# ==============================================================================
# åˆæœŸåŒ–ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
# ==============================================================================
def initialize_groq_client():
    global groq_client
    if GROQ_API_KEY: groq_client = Groq(api_key=GROQ_API_KEY)
def initialize_gemini_client():
    global gemini_model
    if GEMINI_API_KEY: genai.configure(api_key=GEMINI_API_KEY); gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')
def initialize_holomem_wiki():
    with Session() as session:
        if session.query(HolomemWiki).count() == 0:
            initial_data = [
                {'member_name': 'ã¨ãã®ãã‚‰', 'status': 'ç¾å½¹', 'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®è±¡å¾´ï¼', 'generation': '0æœŸç”Ÿ', 'tags': '[]'},
                {'member_name': 'ã•ãã‚‰ã¿ã“', 'status': 'ç¾å½¹', 'description': 'ã‚¨ãƒªãƒ¼ãƒˆå·«å¥³ã ã‚ˆï¼', 'generation': '0æœŸç”Ÿ', 'tags': '[]'},
                {'member_name': 'æ¡ç”Ÿã‚³ã‚³', 'status': 'å’æ¥­', 'description': 'ä¼èª¬ã®ä¼šé•·ï¼', 'generation': '4æœŸç”Ÿ', 'graduation_date': '2021-07-01', 'mochiko_feeling': 'ä¼šé•·ãŒæ®‹ã—ã¦ãã‚ŒãŸã‚‚ã®ã¯æ°¸é ã ã‚ˆï¼', 'tags': '[]'},
                {'member_name': 'æ½¤ç¾½ã‚‹ã—ã‚', 'status': 'å’æ¥­', 'description': 'æ„Ÿæƒ…è±Šã‹ãªãƒã‚¯ãƒ­ãƒãƒ³ã‚µãƒ¼ã€‚', 'generation': '3æœŸç”Ÿ', 'graduation_date': '2022-02-24', 'mochiko_feeling': 'ã¾ãŸ3æœŸç”Ÿã®ã¿ã‚“ãªã§ã‚ã¡ã‚ƒã‚ã¡ã‚ƒã—ã¦ã»ã—ã‹ã£ãŸãªâ€¦', 'tags': '[]'},
            ]
            for data in initial_data: session.add(HolomemWiki(**data))
            session.commit()
            logger.info("âœ… ãƒ›ãƒ­ãƒ¡ãƒ³Wikiã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")
def initialize_app():
    global fernet
    logger.info("="*60 + "\nğŸ”§ ã‚‚ã¡ã“AI ç©¶æ¥µç‰ˆ (v16.2) ã®åˆæœŸåŒ–ã‚’é–‹å§‹...\n" + "="*60)
    
    if BACKUP_ENCRYPTION_KEY:
        fernet = Fernet(BACKUP_ENCRYPTION_KEY.encode('utf-8'))
        logger.info("âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æš—å·åŒ–ã‚­ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
    else:
        logger.warning("âš ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æš—å·åŒ–ã‚­ãƒ¼ãŒæœªè¨­å®šã§ã™ã€‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½ã¯ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚")

    initialize_gemini_client(); initialize_groq_client()
    initialize_holomem_wiki()
    def run_scheduler():
        schedule.every().day.at("03:00").do(analyze_user_psychology, user_uuid=None) # Placeholder for periodic analysis
        if fernet:
            schedule.every().day.at("18:00").do(commit_encrypted_backup_to_github)
        while True:
            schedule.run_pending(); time.sleep(60)
    threading.Thread(target=run_scheduler, daemon=True).start()
    logger.info("â° ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’é–‹å§‹ã—ã¾ã—ãŸ (DBãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— & å®šæœŸæ€§æ ¼åˆ†æ)")
    logger.info(f"ğŸ¤– åˆ©ç”¨å¯èƒ½ãªAIãƒ¢ãƒ‡ãƒ«: Gemini={'âœ…' if gemini_model else 'âŒ'} | Llama={'âœ…' if groq_client else 'âŒ'}")
    logger.info("âœ… åˆæœŸåŒ–å®Œäº†ï¼")

# ==============================================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ==============================================================================
if __name__ == '__main__':
    initialize_app()
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
else:
    initialize_app()
    application = app
