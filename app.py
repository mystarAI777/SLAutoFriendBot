import sys
import os
import requests
import logging
import time
import threading
import json
import re
import sqlite3
import random
import uuid
import hashlib
from datetime import datetime, timedelta, timezone
import unicodedata
from groq import Groq
import google.generativeai as genai
from flask import Response

# å‹ãƒ’ãƒ³ãƒˆç”¨ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼å¯¾ç­–ä»˜ãï¼‰
try:
    from typing import Union, Dict, Any, List, Optional
except ImportError:
    # å‹ãƒ’ãƒ³ãƒˆãŒä½¿ãˆãªã„ç’°å¢ƒç”¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    Dict = dict
    Any = object
    List = list
    Union = object
    Optional = object

from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Index, Text, Boolean, inspect, text
from sqlalchemy.orm import declarative_base, sessionmaker
from urllib.parse import quote_plus, urljoin
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import asyncio
from threading import Lock
import schedule
import signal

# --- åŸºæœ¬è¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- å®šæ•°è¨­å®š ---
VOICE_DIR = '/tmp/voices'
SERVER_URL = "https://slautofriendbot.onrender.com"
VOICEVOX_SPEAKER_ID = 20  # ã‚‚ã¡å­ã•ã‚“(ãƒãƒ¼ãƒãƒ«) ã«çµ±åˆ
HOLOLIVE_NEWS_URL = "https://hololive-tsuushin.com/category/holonews/"

SL_SAFE_CHAR_LIMIT = 300      # Second Lifeå®‰å…¨æ–‡å­—æ•°åˆ¶é™
VOICE_OPTIMAL_LENGTH = 150    # VOICEVOXæœ€é©æ–‡å­—æ•°

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
]
LOCATION_CODES = {
    "æ±äº¬": "130000", "å¤§é˜ª": "270000", "åå¤å±‹": "230000", "ç¦å²¡": "400000", "æœ­å¹Œ": "016000"
}
SPECIALIZED_SITES = {
    'Blender': {
        'base_url': 'https://docs.blender.org/manual/ja/latest/',
        'keywords': ['Blender', 'ãƒ–ãƒ¬ãƒ³ãƒ€ãƒ¼', 'blender', 'BLENDER']
    },
    'CGãƒ‹ãƒ¥ãƒ¼ã‚¹': {
        'base_url': 'https://modelinghappy.com/',
        'keywords': ['CGãƒ‹ãƒ¥ãƒ¼ã‚¹', '3DCG', 'CG', 'ï¼£ï¼§', 'ï½ƒï½‡', 'cg', '3dcg', 'ï¼“ï¼¤ï¼£ï¼§', 'CGæ¥­ç•Œ', 'CGã‚¢ãƒ‹ãƒ¡']
    },
    'è„³ç§‘å­¦ãƒ»å¿ƒç†å­¦': {
        'base_url': 'https://nazology.kusuguru.co.jp/',
        'keywords': ['è„³ç§‘å­¦', 'å¿ƒç†å­¦', 'è„³', 'å¿ƒç†', 'ã®ã†ã‹ãŒã', 'ã—ã‚“ã‚ŠãŒã']
    },
    'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•': {
        'base_url': 'https://community.secondlife.com/news/',
        'keywords': ['ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•', 'Second Life', 'SL', 'second life', 'ã‚»ã‚«ãƒ³ãƒ‰', 'SecondLife']
    },
    'ã‚¢ãƒ‹ãƒ¡': {
    'base_url': 'https://animedb.jp/',
    'keywords': ['ã‚¢ãƒ‹ãƒ¡', 'anime', 'ANIME', 'ï½±ï¾†ï¾’', 'ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³', 'ä½œç”»', 'å£°å„ª', 'OP', 'ED']
}
}
ANIME_KEYWORDS = [
    'ã‚¢ãƒ‹ãƒ¡', 'anime', 'ANIME', 'ï½±ï¾†ï¾’', 'ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³',
    'ä½œç”»', 'å£°å„ª', 'OP', 'ED', 'ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°', 'ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°',
    'åŠ‡å ´ç‰ˆ', 'æ˜ ç”»', 'OVA', 'OAD', 'åŸä½œ', 'æ¼«ç”»', 'ãƒ©ãƒãƒ™',
    'ä¸»äººå…¬', 'ã‚­ãƒ£ãƒ©', 'ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼', 'åˆ¶ä½œä¼šç¤¾', 'ã‚¹ã‚¿ã‚¸ã‚ª'
]
# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° & Executor ---
background_executor = ThreadPoolExecutor(max_workers=5)
groq_client = None
VOICEVOX_ENABLED = True
app = Flask(__name__)
CORS(app)

# â†“â†“â†“ ã“ã“ã«è¿½åŠ  â†“â†“â†“
@app.after_request
def after_request(response):
    """CORSãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å…¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«è¿½åŠ """
    response.headers.add('Access-Control-Allow-Origin', '*')
    response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
    response.headers.add('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
    return response

Base = declarative_base()

# â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ãŒå”¯ä¸€ã®å¤‰æ›´ç®‡æ‰€ã§ã™ã€‘â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼

# --- ç§˜å¯†æƒ…å ±/ç’°å¢ƒå¤‰æ•° èª­ã¿è¾¼ã¿ ---
def get_secret(name):
    """
    ã¾ãšRenderã®Secret Fileã‹ã‚‰ç§˜å¯†æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã€è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚
    """
    secret_file_path = f"/etc/secrets/{name}"
    if os.path.exists(secret_file_path):
        try:
            with open(secret_file_path, 'r') as f:
                logger.info(f"âœ… Secret Fileã‹ã‚‰ {name} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
                return f.read().strip()
        except IOError as e:
            logger.error(f"âŒ Secret File {secret_file_path} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return None
    
    # Secret FileãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ç’°å¢ƒå¤‰æ•°ã‚’ãƒã‚§ãƒƒã‚¯
    value = os.environ.get(name)
    if value:
         logger.info(f"âœ… ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ {name} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    return value

def ensure_voice_directory():
    """éŸ³å£°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ã‚’ä¿è¨¼"""
    max_attempts = 3
    for attempt in range(max_attempts):
        try:
            if not os.path.exists(VOICE_DIR):
                os.makedirs(VOICE_DIR, mode=0o755, exist_ok=True)
                logger.info(f"âœ… Voice directory created: {VOICE_DIR}")
            
            # æ›¸ãè¾¼ã¿æ¨©é™ã‚’ç¢ºèª
            if os.access(VOICE_DIR, os.W_OK):
                logger.info(f"âœ… Voice directory is writable: {VOICE_DIR}")
                return True
            else:
                # æ¨©é™ã‚’ä¿®æ­£
                os.chmod(VOICE_DIR, 0o755)
                logger.info(f"âœ… Voice directory permissions fixed: {VOICE_DIR}")
                return True
                
        except Exception as e:
            logger.error(f"âŒ Voice directory creation failed (attempt {attempt + 1}/{max_attempts}): {e}")
            if attempt < max_attempts - 1:
                time.sleep(1)
            continue
    
    logger.critical(f"ğŸ”¥ Failed to create voice directory after {max_attempts} attempts")
    return False
    
DATABASE_URL = get_secret('DATABASE_URL') or 'sqlite:///./test.db'
GROQ_API_KEY = get_secret('GROQ_API_KEY')
VOICEVOX_URL_FROM_ENV = get_secret('VOICEVOX_URL')
GEMINI_API_KEY = get_secret('GEMINI_API_KEY')
gemini_model = None

def initialize_gemini_client():
    global gemini_model
    try:
        if GEMINI_API_KEY and len(GEMINI_API_KEY) > 20:
            genai.configure(api_key=GEMINI_API_KEY)
            gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')
            logger.info("âœ… Gemini 2.0 Flash client initialized")
    except Exception as e:
        logger.error(f"âŒ Gemini initialization failed: {e}")

# --- Hololive Wikiæ¤œç´¢æ©Ÿèƒ½ã®è¿½åŠ  ---
def search_hololive_wiki(member_name, query_topic):
    """
    Seesaawikiã®ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–Wikiã‹ã‚‰æƒ…å ±ã‚’æ¤œç´¢ã™ã‚‹ã€‚
    ãƒ¡ãƒ³ãƒãƒ¼åã¨ç‰¹å®šã®ãƒˆãƒ”ãƒƒã‚¯ã‚’çµ„ã¿åˆã‚ã›ã¦æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã€‚
    """
    base_url = "https://seesaawiki.jp/hololivetv/"
    search_query = f"{member_name} {query_topic}"
    encoded_query = quote_plus(search_query.encode('euc-jp')) # Seesaawikiã¯EUC-JPãŒå¤šã„
    search_url = f"{base_url}search?query={encoded_query}"
    
    try:
        logger.info(f"ğŸ” Searching Hololive Wiki for: {search_query} at {search_url}")
        response = requests.get(
            search_url,
            headers={'User-Agent': random.choice(USER_AGENTS)},
            timeout=15,
            allow_redirects=True
        )
        # Seesaawikiã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«åˆã‚ã›ã¦ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’è©¦ã¿ã‚‹
        response.encoding = 'euc-jp'
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã‹ã‚‰é–¢é€£æ€§ã®é«˜ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¢ã™
        # ãƒšãƒ¼ã‚¸å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã—ã€é–¢é€£éƒ¨åˆ†ã‚’æŠ½å‡ºã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        
        # ã¾ãšã€ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ã‚’ç‰¹å®š
        main_content_div = soup.find('div', id='pagebody') or soup.find('div', class_='contents')
        if not main_content_div:
            logger.warning("Hololive Wiki: Could not find main content div.")
            return None

        # é–¢é€£æ€§ã®é«˜ã„æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
        page_text = clean_text(main_content_div.get_text())
        
        # ãƒ¡ãƒ³ãƒãƒ¼åã¨ãƒˆãƒ”ãƒƒã‚¯ã‚’å«ã‚€å‘¨è¾ºã®æ–‡ç« ã‚’æŠ½å‡ºã™ã‚‹
        # ä¾‹: ã€Œã•ãã‚‰ã¿ã“ã€ã¨ã€Œãƒã‚¤ã‚¯ãƒ©ã€ã§æ¤œç´¢ã—ãŸå ´åˆã€ã€Œã•ãã‚‰ã¿ã“ã¯ãƒã‚¤ã‚¯ãƒ©ã§ç‹¬ç‰¹ã®å»ºç¯‰ã‚’ã™ã‚‹ã€ã®ã‚ˆã†ãªæ–‡ç« 
        
        # ç°¡æ˜“çš„ãªè¦ç´„ç”Ÿæˆ
        # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹æ–‡ã‚’ã„ãã¤ã‹æŠ½å‡º
        sentences = re.split(r'(ã€‚|ï¼|\n)', page_text)
        relevant_sentences = []
        for i in range(0, len(sentences), 2):
            sentence = sentences[i]
            if member_name in sentence and query_topic in sentence:
                relevant_sentences.append(sentence.strip())
            if len(" ".join(relevant_sentences)) > 500: # ã‚ã‚‹ç¨‹åº¦ã®é•·ã•ã«é”ã—ãŸã‚‰çµ‚äº†
                break

        if relevant_sentences:
            extracted_info = " ".join(relevant_sentences)[:1000] # æœ€å¤§1000æ–‡å­—
            logger.info(f"âœ… Hololive Wiki search successful for '{search_query}'. Extracted: {extracted_info[:100]}")
            return extracted_info
        
        logger.info(f"â„¹ï¸ Hololive Wiki search for '{search_query}' found no direct relevant sentences. Attempting general summary.")
        # é–¢é€£æ–‡ç« ãŒè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã€ãƒšãƒ¼ã‚¸ã®æœ€åˆã®éƒ¨åˆ†ã‚’è¦ç´„ã¨ã—ã¦è¿”ã™
        return page_text[:500] if page_text else None
        
    except requests.exceptions.Timeout:
        logger.warning(f"âš ï¸ Hololive Wiki search timeout for {search_query}")
        return None
    except requests.exceptions.RequestException as e:
        logger.warning(f"âš ï¸ Hololive Wiki search request error for {search_query}: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ Hololive Wiki search general error for {search_query}: {e}", exc_info=True)
        return None

# â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²ã€å¤‰æ›´ç®‡æ‰€ã¯ã“ã“ã¾ã§ã§ã™ã€‘â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
# ===== ã€è¿½åŠ ã€‘ã‚¢ãƒ‹ãƒ¡æ¤œç´¢æ©Ÿèƒ½ =====

def is_anime_request(message):
    """ã‚¢ãƒ‹ãƒ¡é–¢é€£ã®è³ªå•ã‹ã©ã†ã‹åˆ¤å®š"""
    message_normalized = unicodedata.normalize('NFKC', message).lower()
    
    # ã‚¢ãƒ‹ãƒ¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
    for keyword in ANIME_KEYWORDS:
        keyword_normalized = unicodedata.normalize('NFKC', keyword).lower()
        if keyword_normalized in message_normalized:
            return True
    
    # ã€Œã€œã£ã¦ã‚¢ãƒ‹ãƒ¡ã€ã€Œã€œã¨ã„ã†ã‚¢ãƒ‹ãƒ¡ã€ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    anime_patterns = [
        r'ã£ã¦ã‚¢ãƒ‹ãƒ¡', r'ã¨ã„ã†ã‚¢ãƒ‹ãƒ¡', r'ã®ã‚¢ãƒ‹ãƒ¡',
        r'ã‚¢ãƒ‹ãƒ¡ã§', r'ã‚¢ãƒ‹ãƒ¡ã®', r'ã‚¢ãƒ‹ãƒ¡ã¯'
    ]
    for pattern in anime_patterns:
        if re.search(pattern, message):
            return True
    
    return False


def search_anime_database(query, is_detailed=False):
    """
    https://animedb.jp/ ã‹ã‚‰ã‚¢ãƒ‹ãƒ¡æƒ…å ±ã‚’æ¤œç´¢
    """
    base_url = "https://animedb.jp/"
    
    try:
        logger.info(f"ğŸ¬ Searching anime database for: {query}")
        
        # Step 1: æ¤œç´¢ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
        search_url = f"{base_url}search?q={quote_plus(query)}"
        response = requests.get(
            search_url,
            headers={'User-Agent': random.choice(USER_AGENTS)},
            timeout=15,
            allow_redirects=True
        )
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Step 2: æ¤œç´¢çµæœã‚’è§£æ
        # animedb.jpã®æ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ï¼ˆå®Ÿéš›ã®HTMLæ§‹é€ ã‚’ç¢ºèªã—ã¦ä¿®æ­£ã—ã¦ãã ã•ã„ï¼‰
        results = []
        
        # æ¤œç´¢çµæœã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè¤‡æ•°è©¦è¡Œï¼‰
        result_selectors = [
            'div.anime-item',
            'div.search-result',
            'article.anime',
            'div[class*="anime"]',
            'li.anime-list-item'
        ]
        
        result_elements = []
        for selector in result_selectors:
            result_elements = soup.select(selector)
            if result_elements:
                logger.info(f"âœ… Found results with selector: {selector}")
                break
        
        if not result_elements:
            # ã‚»ãƒ¬ã‚¯ã‚¿ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€å…¨ä½“ã‹ã‚‰æƒ…å ±æŠ½å‡ºã‚’è©¦ã¿ã‚‹
            logger.warning("âš ï¸ No specific selectors found, trying general extraction")
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚ã‚‰ã™ã˜ã‚’å«ã‚€divã‚’æ¢ã™
            potential_results = soup.find_all(['div', 'article'], limit=10)
            result_elements = [elem for elem in potential_results if elem.find(['h2', 'h3', 'h4'])]
        
        for elem in result_elements[:3 if is_detailed else 2]:
            # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
            title_elem = elem.find(['h2', 'h3', 'h4', 'a'])
            if not title_elem:
                continue
            
            title = clean_text(title_elem.get_text())
            
            # ã‚ã‚‰ã™ã˜/èª¬æ˜æŠ½å‡º
            description_elem = elem.find(['p', 'div'], class_=lambda x: x and ('description' in x.lower() or 'summary' in x.lower()))
            if not description_elem:
                description_elem = elem.find('p')
            
            description = clean_text(description_elem.get_text()) if description_elem else ""
            
            # ãƒªãƒ³ã‚¯æŠ½å‡º
            link_elem = elem.find('a', href=True)
            link = urljoin(base_url, link_elem['href']) if link_elem else ""
            
            if title and len(title) > 2:
                results.append({
                    'title': title,
                    'description': description[:300] if description else "è©³ç´°æƒ…å ±ãªã—",
                    'url': link
                })
        
        if not results:
            logger.warning(f"âš ï¸ No anime results found for: {query}")
            return None
        
        # Step 3: çµæœã‚’æ•´å½¢
        formatted_results = []
        for i, result in enumerate(results, 1):
            formatted_results.append(
                f"ã€{i}ã€‘{result['title']}\n"
                f"{result['description'][:150]}..."
            )
        
        summary = "\n\n".join(formatted_results)
        logger.info(f"âœ… Anime search successful: {len(results)} results")
        
        return summary
        
    except requests.exceptions.Timeout:
        logger.error(f"âŒ Anime search timeout for: {query}")
        return None
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Anime search request error: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ Anime search general error: {e}", exc_info=True)
        return None


# ===== ã€è¿½åŠ ã€‘å¿ƒç†åˆ†ææ©Ÿèƒ½ =====

def analyze_user_psychology(user_uuid):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éå»ã®ä¼šè©±å±¥æ­´ã‹ã‚‰å¿ƒç†åˆ†æã‚’å®Ÿè¡Œ
    """
    session = Session()
    try:
        logger.info(f"ğŸ§  Starting psychology analysis for user: {user_uuid}")
        
        # Step 1: ä¼šè©±å±¥æ­´ã‚’å–å¾—ï¼ˆæœ€æ–°100ä»¶ï¼‰
        conversations = session.query(ConversationHistory).filter_by(
            user_uuid=user_uuid,
            role='user'  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã®ã¿
        ).order_by(ConversationHistory.timestamp.desc()).limit(100).all()
        
        if len(conversations) < 10:
            logger.warning(f"âš ï¸ Not enough conversation data for analysis: {len(conversations)} messages")
            return None
        
        # Step 2: ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        messages_text = "\n".join([conv.content for conv in reversed(conversations)])
        total_messages = len(conversations)
        avg_length = sum(len(conv.content) for conv in conversations) // total_messages
        
        # Step 3: ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
        user_memory = session.query(UserMemory).filter_by(user_uuid=user_uuid).first()
        user_name = user_memory.user_name if user_memory else "ä¸æ˜"
        
        # Step 4: AI ã«ã‚ˆã‚‹å¿ƒç†åˆ†æ
        if not groq_client:
            logger.warning("âš ï¸ Groq client unavailable, skipping AI analysis")
            return None
        
        analysis_prompt = f"""ã‚ãªãŸã¯å¿ƒç†å­¦ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€Œ{user_name}ã€ã•ã‚“ã®éå»ã®ä¼šè©±ï¼ˆ{total_messages}ä»¶ï¼‰ã‚’åˆ†æã—ã€å¿ƒç†ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€ä¼šè©±å±¥æ­´ã€‘
{messages_text[:3000]}

ã€åˆ†æé …ç›®ã€‘
1. **ãƒ“ãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ–æ€§æ ¼ç‰¹æ€§**ï¼ˆå„0-100ç‚¹ã§è©•ä¾¡ï¼‰
   - é–‹æ”¾æ€§ï¼ˆOpennessï¼‰: æ–°ã—ã„çµŒé¨“ã¸ã®èˆˆå‘³ã€å‰µé€ æ€§
   - èª å®Ÿæ€§ï¼ˆConscientiousnessï¼‰: è¨ˆç”»æ€§ã€è²¬ä»»æ„Ÿ
   - å¤–å‘æ€§ï¼ˆExtraversionï¼‰: ç¤¾äº¤æ€§ã€æ´»ç™ºã•
   - å”èª¿æ€§ï¼ˆAgreeablenessï¼‰: å„ªã—ã•ã€å”åŠ›çš„
   - ç¥çµŒç—‡å‚¾å‘ï¼ˆNeuroticismï¼‰: ä¸å®‰ã€æ„Ÿæƒ…ã®å®‰å®šæ€§

2. **èˆˆå‘³ãƒ»é–¢å¿ƒ**ï¼ˆä¸»è¦ãªèˆˆå‘³åˆ†é‡ã¨ãã®å¼·åº¦ï¼‰

3. **ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«**ï¼ˆã‚«ã‚¸ãƒ¥ã‚¢ãƒ«/ä¸å¯§/ç†±å¿ƒãªã©ï¼‰

4. **æ„Ÿæƒ…å‚¾å‘**ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«/æ„Ÿæƒ…è±Šã‹ãªã©ï¼‰

5. **ã‚ˆãè©±ã™è©±é¡Œãƒˆãƒƒãƒ—3**

6. **ç·åˆçš„ãªäººç‰©åƒã®è¦ç´„**ï¼ˆ200æ–‡å­—ç¨‹åº¦ï¼‰

**é‡è¦**: ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆä»–ã®æ–‡ç« ã¯ä¸è¦ï¼‰:
{{
  "openness": 75,
  "conscientiousness": 60,
  "extraversion": 80,
  "agreeableness": 70,
  "neuroticism": 40,
  "interests": {{"ã‚¢ãƒ‹ãƒ¡": 90, "ã‚²ãƒ¼ãƒ ": 70, "éŸ³æ¥½": 60}},
  "conversation_style": "ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã§è¦ªã—ã¿ã‚„ã™ã„",
  "emotional_tendency": "ãƒã‚¸ãƒ†ã‚£ãƒ–ã§æ˜ã‚‹ã„",
  "favorite_topics": ["ã‚¢ãƒ‹ãƒ¡", "æ—¥å¸¸ã®å‡ºæ¥äº‹", "è¶£å‘³"],
  "summary": "æ˜ã‚‹ãç¤¾äº¤çš„ãªæ€§æ ¼ã§ã€ã‚¢ãƒ‹ãƒ¡ã‚„å‰µä½œæ´»å‹•ã«å¼·ã„èˆˆå‘³ã‚’æŒã¤ã€‚ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªä¼šè©±ã‚’å¥½ã¿ã€æ„Ÿæƒ…è¡¨ç¾ãŒè±Šã‹ã€‚æ–°ã—ã„ã“ã¨ã¸ã®å¥½å¥‡å¿ƒãŒæ—ºç››ã€‚",
  "confidence": 85
}}"""

        try:
            completion = groq_client.chat.completions.create(
                messages=[{"role": "user", "content": analysis_prompt}],
                model="llama-3.1-8b-instant",
                temperature=0.3,  # åˆ†æã¯æ­£ç¢ºæ€§é‡è¦–
                max_tokens=800
            )
            
            response_text = completion.choices[0].message.content.strip()
            
            # JSONã‚’æŠ½å‡ºï¼ˆ```json ... ``` ã®å ´åˆã«å¯¾å¿œï¼‰
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(1)
            
            # JSON ãƒ‘ãƒ¼ã‚¹
            analysis_data = json.loads(response_text)
            
            logger.info(f"âœ… AI analysis completed for user: {user_uuid}")
            
            # Step 5: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            psychology = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
            
            if psychology:
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
                psychology.user_name = user_name
                psychology.openness = analysis_data.get('openness', 50)
                psychology.conscientiousness = analysis_data.get('conscientiousness', 50)
                psychology.extraversion = analysis_data.get('extraversion', 50)
                psychology.agreeableness = analysis_data.get('agreeableness', 50)
                psychology.neuroticism = analysis_data.get('neuroticism', 50)
                psychology.interests = json.dumps(analysis_data.get('interests', {}), ensure_ascii=False)
                psychology.favorite_topics = json.dumps(analysis_data.get('favorite_topics', []), ensure_ascii=False)
                psychology.conversation_style = analysis_data.get('conversation_style', '')
                psychology.emotional_tendency = analysis_data.get('emotional_tendency', '')
                psychology.analysis_summary = analysis_data.get('summary', '')
                psychology.total_messages = total_messages
                psychology.avg_message_length = avg_length
                psychology.last_analyzed = datetime.utcnow()
                psychology.analysis_confidence = analysis_data.get('confidence', 70)
            else:
                # æ–°è¦ä½œæˆ
                psychology = UserPsychology(
                    user_uuid=user_uuid,
                    user_name=user_name,
                    openness=analysis_data.get('openness', 50),
                    conscientiousness=analysis_data.get('conscientiousness', 50),
                    extraversion=analysis_data.get('extraversion', 50),
                    agreeableness=analysis_data.get('agreeableness', 50),
                    neuroticism=analysis_data.get('neuroticism', 50),
                    interests=json.dumps(analysis_data.get('interests', {}), ensure_ascii=False),
                    favorite_topics=json.dumps(analysis_data.get('favorite_topics', []), ensure_ascii=False),
                    conversation_style=analysis_data.get('conversation_style', ''),
                    emotional_tendency=analysis_data.get('emotional_tendency', ''),
                    analysis_summary=analysis_data.get('summary', ''),
                    total_messages=total_messages,
                    avg_message_length=avg_length,
                    analysis_confidence=analysis_data.get('confidence', 70)
                )
                session.add(psychology)
            
            session.commit()
            logger.info(f"ğŸ’¾ Psychology analysis saved for user: {user_uuid}")
            
            return psychology
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Failed to parse AI analysis JSON: {e}")
            logger.error(f"Raw response: {response_text[:500]}")
            return None
        except Exception as e:
            logger.error(f"âŒ AI analysis error: {e}", exc_info=True)
            return None
            
    except Exception as e:
        logger.error(f"âŒ Psychology analysis error: {e}", exc_info=True)
        session.rollback()
        return None
    finally:
        session.close()


def get_user_psychology(user_uuid):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†åˆ†æçµæœã‚’å–å¾—"""
    session = Session()
    try:
        psychology = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
        
        if not psychology:
            return None
        
        return {
            'openness': psychology.openness,
            'conscientiousness': psychology.conscientiousness,
            'extraversion': psychology.extraversion,
            'agreeableness': psychology.agreeableness,
            'neuroticism': psychology.neuroticism,
            'interests': json.loads(psychology.interests) if psychology.interests else {},
            'favorite_topics': json.loads(psychology.favorite_topics) if psychology.favorite_topics else [],
            'conversation_style': psychology.conversation_style,
            'emotional_tendency': psychology.emotional_tendency,
            'summary': psychology.analysis_summary,
            'confidence': psychology.analysis_confidence,
            'last_analyzed': psychology.last_analyzed
        }
    finally:
        session.close()
        
# --- åˆæœŸåŒ–å‡¦ç† ---
ensure_voice_directory()

if not DATABASE_URL:
    logger.critical("FATAL: DATABASE_URL is not set.")
    sys.exit(1)

# --- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« ---
class UserMemory(Base):
    __tablename__ = 'user_memories'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False)
    user_name = Column(String(255), nullable=False)
    interaction_count = Column(Integer, default=0)
    last_interaction = Column(DateTime, default=datetime.utcnow)

class ConversationHistory(Base):
    __tablename__ = 'conversation_history'
    id = Column(Integer, primary_key=True, autoincrement=True)
    user_uuid = Column(String(255), nullable=False, index=True)
    role = Column(String(10), nullable=False)
    content = Column(Text, nullable=False)
    timestamp = Column(DateTime, default=datetime.utcnow, index=True)

class HololiveNews(Base):
    __tablename__ = 'hololive_news'
    id = Column(Integer, primary_key=True)
    title = Column(String(500), nullable=False)
    content = Column(Text, nullable=False)
    url = Column(String(1000))
    published_date = Column(DateTime, default=datetime.utcnow)
    created_at = Column(DateTime, default=datetime.utcnow, index=True)
    news_hash = Column(String(100), unique=True)

class BackgroundTask(Base):
    __tablename__ = 'background_tasks'
    id = Column(Integer, primary_key=True)
    task_id = Column(String(255), unique=True, nullable=False)
    user_uuid = Column(String(255), nullable=False)
    task_type = Column(String(50), nullable=False)
    query = Column(Text, nullable=False)
    result = Column(Text)
    status = Column(String(20), default='pending')
    created_at = Column(DateTime, default=datetime.utcnow, index=True)
    completed_at = Column(DateTime)

class SpecializedNews(Base):
    __tablename__ = 'specialized_news'
    id = Column(Integer, primary_key=True)
    site_name = Column(String(100), nullable=False, index=True)
    title = Column(String(500), nullable=False)
    content = Column(Text, nullable=False)
    url = Column(String(1000))
    published_date = Column(DateTime, default=datetime.utcnow)
    created_at = Column(DateTime, default=datetime.utcnow, index=True)
    news_hash = Column(String(100), unique=True)

# ========================================
# ã€å¤‰æ›´1ã€‘HolomemWiki ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ã®æ‹¡å¼µ
# ========================================

class HolomemWiki(Base):
    __tablename__ = 'holomem_wiki'
    id = Column(Integer, primary_key=True)
    member_name = Column(String(100), nullable=False, unique=True, index=True)
    description = Column(Text)
    debut_date = Column(String(100))
    generation = Column(String(100))
    tags = Column(Text)
    # å’æ¥­æƒ…å ±
    graduation_date = Column(String(100), nullable=True)
    graduation_reason = Column(Text, nullable=True)
    mochiko_feeling = Column(Text, nullable=True)
    # â˜… è¿½åŠ : ç¾å½¹/å’æ¥­ãƒ•ãƒ©ã‚°
    is_active = Column(Boolean, default=True, nullable=False, index=True)
    # â˜… è¿½åŠ : ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URLï¼ˆæƒ…å ±å–å¾—å…ƒï¼‰
    profile_url = Column(String(500), nullable=True)
    last_updated = Column(DateTime, default=datetime.utcnow)


# ========================================
# ã€å¤‰æ›´2ã€‘ãƒ›ãƒ­ãƒ¡ãƒ³æƒ…å ±ã‚’è‡ªå‹•å–å¾—ã™ã‚‹é–¢æ•°
# ========================================

def scrape_hololive_members():
    """
    ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ãƒ¡ãƒ³ãƒãƒ¼æƒ…å ±ã‚’å–å¾—ã—ã¦DBã«ä¿å­˜
    å…¬å¼ã‚µã‚¤ãƒˆ: https://hololive.hololivepro.com/talents/
    """
    base_url = "https://hololive.hololivepro.com"
    talents_url = f"{base_url}/talents/"
    
    session = Session()
    added_count = 0
    updated_count = 0
    
    try:
        logger.info("ğŸ” Scraping Hololive members from official site...")
        
        response = requests.get(
            talents_url,
            headers={'User-Agent': random.choice(USER_AGENTS)},
            timeout=20,
            allow_redirects=True
        )
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ãƒ¡ãƒ³ãƒãƒ¼ã‚«ãƒ¼ãƒ‰ã‚’æ¢ã™ï¼ˆå®Ÿéš›ã®HTMLæ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
        member_cards = soup.select('.talent-card, .member-card, [class*="talent"], [class*="member"]')
        
        if not member_cards:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒªãƒ³ã‚¯ã‹ã‚‰ãƒ¡ãƒ³ãƒãƒ¼åã‚’æŠ½å‡º
            logger.warning("âš ï¸ Member cards not found, trying fallback method...")
            member_links = soup.find_all('a', href=lambda x: x and '/talents/' in x)
            member_cards = member_links
        
        logger.info(f"ğŸ“‹ Found {len(member_cards)} potential member entries")
        
        for card in member_cards:
            try:
                # ãƒ¡ãƒ³ãƒãƒ¼åã‚’å–å¾—
                name_elem = card.find(['h2', 'h3', 'h4', 'span', 'div'], class_=lambda x: x and ('name' in x.lower() or 'title' in x.lower()))
                
                if not name_elem:
                    # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥å–å¾—ã‚’è©¦ã¿ã‚‹
                    name_elem = card
                
                member_name = clean_text(name_elem.get_text())
                
                # ä¸è¦ãªæ–‡å­—ã‚’é™¤å»
                member_name = re.sub(r'\s*\(.*?\)\s*', '', member_name)  # (EN)ãªã©ã‚’é™¤å»
                member_name = member_name.strip()
                
                if not member_name or len(member_name) < 2:
                    continue
                
                # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URLã‚’å–å¾—
                profile_link = card.get('href') or (card.find('a', href=True) or {}).get('href', '')
                if profile_link and not profile_link.startswith('http'):
                    profile_link = urljoin(base_url, profile_link)
                
                # æœŸï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã‚’æ¨æ¸¬
                generation = "ä¸æ˜"
                card_text = card.get_text()
                
                gen_patterns = [
                    (r'0æœŸç”Ÿ|ã‚¼ãƒ­æœŸç”Ÿ', '0æœŸç”Ÿ'),
                    (r'1æœŸç”Ÿ|ä¸€æœŸç”Ÿ', '1æœŸç”Ÿ'),
                    (r'2æœŸç”Ÿ|äºŒæœŸç”Ÿ', '2æœŸç”Ÿ'),
                    (r'3æœŸç”Ÿ|ä¸‰æœŸç”Ÿ', '3æœŸç”Ÿ'),
                    (r'4æœŸç”Ÿ|å››æœŸç”Ÿ', '4æœŸç”Ÿ'),
                    (r'5æœŸç”Ÿ|äº”æœŸç”Ÿ', '5æœŸç”Ÿ'),
                    (r'6æœŸç”Ÿ|å…­æœŸç”Ÿ', '6æœŸç”Ÿ'),
                    (r'ã‚²ãƒ¼ãƒãƒ¼ã‚º|GAMERS', 'ã‚²ãƒ¼ãƒãƒ¼ã‚º'),
                    (r'ID|Indonesia', 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ID'),
                    (r'EN|English|Myth|Council|Promise|Advent', 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–EN'),
                    (r'DEV_IS|ReGLOSS', 'DEV_IS'),
                ]
                
                for pattern, gen_name in gen_patterns:
                    if re.search(pattern, card_text, re.IGNORECASE):
                        generation = gen_name
                        break
                
                # æ—¢å­˜ãƒ¡ãƒ³ãƒãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
                existing = session.query(HolomemWiki).filter_by(member_name=member_name).first()
                
                if existing:
                    # æ›´æ–°ï¼ˆç¾å½¹ãƒ¡ãƒ³ãƒãƒ¼ã¨ã—ã¦ãƒãƒ¼ã‚¯ï¼‰
                    existing.is_active = True
                    existing.generation = generation
                    existing.profile_url = profile_link
                    existing.last_updated = datetime.utcnow()
                    updated_count += 1
                    logger.info(f"ğŸ”„ Updated member: {member_name}")
                else:
                    # æ–°è¦è¿½åŠ 
                    new_member = HolomemWiki(
                        member_name=member_name,
                        description=f"{member_name}ã¯{generation}ã®ãƒ¡ãƒ³ãƒãƒ¼ã§ã™ã€‚",
                        generation=generation,
                        is_active=True,
                        profile_url=profile_link,
                        tags=json.dumps([generation], ensure_ascii=False)
                    )
                    session.add(new_member)
                    added_count += 1
                    logger.info(f"â• Added new member: {member_name}")
                
            except Exception as e:
                logger.warning(f"âš ï¸ Error processing member card: {e}")
                continue
        
        # â˜… å’æ¥­ãƒ¡ãƒ³ãƒãƒ¼ã®æ¤œå‡º
        # å…¬å¼ã‚µã‚¤ãƒˆã«å­˜åœ¨ã—ãªã„ãƒ¡ãƒ³ãƒãƒ¼ã‚’ã€Œå’æ¥­æ¸ˆã¿ã€ã¨ã—ã¦ãƒãƒ¼ã‚¯
        all_db_members = session.query(HolomemWiki).filter_by(is_active=True).all()
        
        for db_member in all_db_members:
            # ä»Šå›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§æ›´æ–°ã•ã‚Œãªã‹ã£ãŸ = å…¬å¼ã‚µã‚¤ãƒˆã«å­˜åœ¨ã—ãªã„
            if db_member.last_updated < datetime.utcnow() - timedelta(minutes=5):
                # å’æ¥­ãƒ¡ãƒ³ãƒãƒ¼ã¨ã—ã¦ãƒãƒ¼ã‚¯ï¼ˆãŸã ã—ã€ã™ã§ã«å’æ¥­æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯é™¤ãï¼‰
                if not db_member.graduation_date:
                    logger.warning(f"âš ï¸ Member not found on official site: {db_member.member_name} (marking as potentially graduated)")
                    # is_activeã‚’Falseã«ã¯ã—ãªã„ï¼ˆæ‰‹å‹•ã§å’æ¥­æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹ã¾ã§ï¼‰
        
        session.commit()
        logger.info(f"âœ… Hololive members sync complete: Added {added_count}, Updated {updated_count}")
        
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Failed to scrape Hololive members: {e}")
        session.rollback()
    except Exception as e:
        logger.error(f"âŒ Hololive members scraping error: {e}", exc_info=True)
        session.rollback()
    finally:
        session.close()


# ========================================
# ã€å¤‰æ›´3ã€‘å’æ¥­ãƒ¡ãƒ³ãƒãƒ¼æƒ…å ±ã‚’å–å¾—ã™ã‚‹é–¢æ•°
# ========================================

def scrape_graduated_members():
    """
    ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–å’æ¥­ãƒ¡ãƒ³ãƒãƒ¼ã®æƒ…å ±ã‚’Wikipediaã‚„å…¬å¼ç™ºè¡¨ã‹ã‚‰å–å¾—
    """
    session = Session()
    
    # æ—¢çŸ¥ã®å’æ¥­ãƒ¡ãƒ³ãƒãƒ¼æƒ…å ±ï¼ˆæœ€ä½é™ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
    known_graduated = [
        {
            'member_name': 'å¤œç©ºãƒ¡ãƒ«',
            'generation': '1æœŸç”Ÿ',
            'graduation_date': '2024å¹´1æœˆ16æ—¥',
            'graduation_reason': 'æ©Ÿå¯†æƒ…å ±ã®æ¼æ´©ãªã©å¥‘ç´„é•åè¡Œç‚ºãŒèªã‚ã‚‰ã‚ŒãŸãŸã‚ã€å¥‘ç´„è§£é™¤ã¨ãªã‚Šã¾ã—ãŸã€‚',
            'mochiko_feeling': 'ãƒ¡ãƒ«å…ˆè¼©ã€åˆæœŸã‹ã‚‰ã®ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã‚’æ”¯ãˆã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã€‚çªç„¶ã§â€¦è¨€è‘‰ãŒå‡ºãªã„ã‚ˆâ€¦',
            'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–1æœŸç”Ÿã€‚ãƒ´ã‚¡ãƒ³ãƒ‘ã‚¤ã‚¢ã®å¥³ã®å­ã§ã€ã‚¢ã‚»ãƒ­ãƒ©ã‚¸ãƒ¥ãƒ¼ã‚¹ãŒå¤§å¥½ãã€‚',
            'debut_date': '2018å¹´5æœˆ13æ—¥',
            'tags': ['ãƒ´ã‚¡ãƒ³ãƒ‘ã‚¤ã‚¢', 'ç™’ã—å£°', '1æœŸç”Ÿ', 'å’æ¥­ç”Ÿ']
        },
        {
            'member_name': 'æ½¤ç¾½ã‚‹ã—ã‚',
            'generation': '3æœŸç”Ÿ',
            'graduation_date': '2022å¹´2æœˆ24æ—¥',
            'graduation_reason': 'æƒ…å ±æ¼æ´©ãªã©ã®å¥‘ç´„é•åè¡Œç‚ºã‚„ä¿¡ç”¨å¤±å¢œè¡Œç‚ºãŒèªã‚ã‚‰ã‚ŒãŸãŸã‚ã€å¥‘ç´„è§£é™¤ã¨ãªã‚Šã¾ã—ãŸã€‚',
            'mochiko_feeling': 'ã‚‹ã—ã‚ã¡ã‚ƒã‚“ã®ã“ã¨ã€ä»Šã§ã‚‚ä¿¡ã˜ã‚‰ã‚Œãªã„ã‚ˆâ€¦ã¾ãŸ3æœŸç”Ÿã®ã¿ã‚“ãªã§ã‚ã¡ã‚ƒã‚ã¡ã‚ƒã—ã¦ã»ã—ã‹ã£ãŸãªâ€¦',
            'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–3æœŸç”Ÿã€‚é­”ç•Œå­¦æ ¡ã«é€šã†ãƒã‚¯ãƒ­ãƒãƒ³ã‚µãƒ¼ã®å¥³ã®å­ã€‚',
            'debut_date': '2019å¹´7æœˆ18æ—¥',
            'tags': ['ãƒã‚¯ãƒ­ãƒãƒ³ã‚µãƒ¼', 'æ„Ÿæƒ…è±Šã‹', '3æœŸç”Ÿ', 'å’æ¥­ç”Ÿ']
        },
        {
            'member_name': 'æ¡ç”Ÿã‚³ã‚³',
            'generation': '4æœŸç”Ÿ',
            'graduation_date': '2021å¹´7æœˆ1æ—¥',
            'graduation_reason': 'æœ¬äººã®æ„å‘ã‚’å°Šé‡ã™ã‚‹å½¢ã§å’æ¥­ã€‚',
            'mochiko_feeling': 'ä¼šé•·ãŒã„ãªã„ã®ã€ã¾ã˜å¯‚ã—ã„ã˜ã‚ƒã‚“â€¦ã§ã‚‚ã€ä¼šé•·ã®ä¼èª¬ã¯ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã§æ°¸é ã«èªã‚Šç¶™ãŒã‚Œã‚‹ã‚ˆã­ï¼',
            'description': 'äººé–“ã®æ–‡åŒ–ã«èˆˆå‘³ã‚’æŒã¤ãƒ‰ãƒ©ã‚´ãƒ³ã€‚æ—¥æœ¬èªã¨è‹±èªã‚’é§†ä½¿ã—ãŸé…ä¿¡ã§æµ·å¤–ãƒ•ã‚¡ãƒ³ã‚’çˆ†ç™ºçš„ã«å¢—ã‚„ã—ãŸç«‹å½¹è€…ã€‚',
            'debut_date': '2019å¹´12æœˆ28æ—¥',
            'tags': ['ãƒ‰ãƒ©ã‚´ãƒ³', 'ãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«', 'ä¼èª¬', 'ä¼šé•·', 'å’æ¥­ç”Ÿ']
        },
        {
            'member_name': 'é­”ä¹ƒã‚¢ãƒ­ã‚¨',
            'generation': '5æœŸç”Ÿ',
            'graduation_date': '2020å¹´8æœˆ31æ—¥',
            'graduation_reason': 'ãƒ‡ãƒ“ãƒ¥ãƒ¼ç›´å¾Œã®æƒ…å ±æ¼æ´©ãƒˆãƒ©ãƒ–ãƒ«ã«ã‚ˆã‚Šå’æ¥­ã€‚',
            'mochiko_feeling': 'ã‚¢ãƒ­ã‚¨ã¡ã‚ƒã‚“ã€ä¸€ç¬ã ã£ãŸã‘ã©ã‚­ãƒ©ã‚­ãƒ©ã—ã¦ãŸâ€¦ã‚‚ã£ã¨ä¸€ç·’ã«æ´»å‹•ã—ãŸã‹ã£ãŸãªã€ã¾ã˜ã§â€¦',
            'description': 'é­”ç•Œã§ã‚¦ãƒ¯ã‚µã®ç”Ÿæ„æ°—ãªã‚µã‚­ãƒ¥ãƒã‚¹ã®å­ä¾›ã€‚',
            'debut_date': '2020å¹´8æœˆ15æ—¥',
            'tags': ['ã‚µã‚­ãƒ¥ãƒã‚¹', '5æœŸç”Ÿ', 'å¹»', 'å’æ¥­ç”Ÿ']
        },
        {
            'member_name': 'ä¹åä¹ä½å‘½',
            'generation': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–EN',
            'graduation_date': '2022å¹´7æœˆ31æ—¥',
            'graduation_reason': 'é•·æœŸçš„ãªæ´»å‹•ãŒå›°é›£ã«ãªã£ãŸãŸã‚ã€‚',
            'mochiko_feeling': 'ã‚µãƒŠã¡ã‚ƒã‚“ã€å®‡å®™ã¿ãŸã„ã«å¿ƒãŒåºƒãã¦å¤§å¥½ãã ã£ãŸã‚ˆã€‚ã‚†ã£ãã‚Šä¼‘ã‚“ã§ã€å…ƒæ°—ã§ã„ã¦ã»ã—ã„ãªâ€¦',
            'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–English -Council-æ‰€å±ã€‚ã€Œç©ºé–“ã€ã®æ¦‚å¿µã®ä»£å¼è€…ã€‚',
            'debut_date': '2021å¹´8æœˆ23æ—¥',
            'tags': ['å®‡å®™', 'ç™’ã—', 'EN', 'å’æ¥­ç”Ÿ']
        },
    ]
    
    try:
        for grad_data in known_graduated:
            existing = session.query(HolomemWiki).filter_by(member_name=grad_data['member_name']).first()
            
            if existing:
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
                existing.is_active = False
                existing.graduation_date = grad_data['graduation_date']
                existing.graduation_reason = grad_data['graduation_reason']
                existing.mochiko_feeling = grad_data['mochiko_feeling']
                existing.last_updated = datetime.utcnow()
                logger.info(f"ğŸ”„ Updated graduated member: {grad_data['member_name']}")
            else:
                # æ–°è¦è¿½åŠ 
                new_grad = HolomemWiki(
                    member_name=grad_data['member_name'],
                    description=grad_data['description'],
                    debut_date=grad_data['debut_date'],
                    generation=grad_data['generation'],
                    is_active=False,
                    graduation_date=grad_data['graduation_date'],
                    graduation_reason=grad_data['graduation_reason'],
                    mochiko_feeling=grad_data['mochiko_feeling'],
                    tags=json.dumps(grad_data['tags'], ensure_ascii=False)
                )
                session.add(new_grad)
                logger.info(f"â• Added graduated member: {grad_data['member_name']}")
        
        session.commit()
        logger.info("âœ… Graduated members sync complete")
        
    except Exception as e:
        logger.error(f"âŒ Graduated members sync error: {e}", exc_info=True)
        session.rollback()
    finally:
        session.close()


# ========================================
# ã€å¤‰æ›´4ã€‘ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ãƒ‹ãƒ¥ãƒ¼ã‚¹æ›´æ–°æ™‚ã«ãƒ¡ãƒ³ãƒãƒ¼æƒ…å ±ã‚‚æ›´æ–°
# ========================================

def update_hololive_news_database():
    """ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ãƒ‹ãƒ¥ãƒ¼ã‚¹ + ãƒ¡ãƒ³ãƒãƒ¼æƒ…å ±ã‚’åŒæ™‚æ›´æ–°"""
    session = Session()
    
    # 1. ãƒ‹ãƒ¥ãƒ¼ã‚¹æ›´æ–°ï¼ˆæ—¢å­˜å‡¦ç†ï¼‰
    _update_news_database(
        session, 
        HololiveNews, 
        "Hololive", 
        HOLOLIVE_NEWS_URL, 
        ['article', '.post', '.entry', '[class*="post"]', '[class*="article"]']
    )
    
    session.close()
    
    # 2. ãƒ¡ãƒ³ãƒãƒ¼æƒ…å ±æ›´æ–°ï¼ˆæ–°è¦å‡¦ç†ï¼‰
    logger.info("ğŸ”„ Updating Hololive members information...")
    
    # ç¾å½¹ãƒ¡ãƒ³ãƒãƒ¼æƒ…å ±ã‚’å–å¾—
    scrape_hololive_members()
    
    # å’æ¥­ãƒ¡ãƒ³ãƒãƒ¼æƒ…å ±ã‚’å–å¾—
    scrape_graduated_members()
    
    logger.info("âœ… Hololive news + members update complete")


class FriendRegistration(Base):
    __tablename__ = 'friend_registrations'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), nullable=False, index=True)
    friend_uuid = Column(String(255), nullable=False)
    friend_name = Column(String(255), nullable=False)
    registered_at = Column(DateTime, default=datetime.utcnow)
    relationship_note = Column(Text)

class NewsCache(Base):
    __tablename__ = 'news_cache'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), nullable=False, index=True)
    news_id = Column(Integer, nullable=False)
    news_number = Column(Integer, nullable=False)
    news_type = Column(String(50), nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)

class UserPsychology(Base):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†åˆ†æçµæœã‚’ä¿å­˜"""
    __tablename__ = 'user_psychology'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False, index=True)
    user_name = Column(String(255), nullable=False)
    
    # æ€§æ ¼åˆ†æï¼ˆãƒ“ãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ–ï¼‰
    openness = Column(Integer, default=50)           # é–‹æ”¾æ€§ (0-100)
    conscientiousness = Column(Integer, default=50)  # èª å®Ÿæ€§ (0-100)
    extraversion = Column(Integer, default=50)       # å¤–å‘æ€§ (0-100)
    agreeableness = Column(Integer, default=50)      # å”èª¿æ€§ (0-100)
    neuroticism = Column(Integer, default=50)        # ç¥çµŒç—‡å‚¾å‘ (0-100)
    
    # èˆˆå‘³ãƒ»é–¢å¿ƒ
    interests = Column(Text)  # JSONå½¢å¼: {"ã‚¢ãƒ‹ãƒ¡": 80, "ã‚²ãƒ¼ãƒ ": 60, ...}
    favorite_topics = Column(Text)  # ã‚ˆãè©±ã™è©±é¡Œ
    
    # ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«
    conversation_style = Column(String(50))  # "ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«", "ä¸å¯§", "ç†±å¿ƒ" ãªã©
    emotional_tendency = Column(String(50))  # "ãƒã‚¸ãƒ†ã‚£ãƒ–", "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«", "æ„Ÿæƒ…è±Šã‹" ãªã©
    
    # çµ±è¨ˆæƒ…å ±
    total_messages = Column(Integer, default=0)
    avg_message_length = Column(Integer, default=0)
    
    # ãƒ¡ã‚¿æƒ…å ±
    analysis_summary = Column(Text)  # AIç”Ÿæˆã®è¦ç´„
    last_analyzed = Column(DateTime, default=datetime.utcnow)
    analysis_confidence = Column(Integer, default=0)  # ä¿¡é ¼åº¦ (0-100)

# ===== æ”¹å–„ç‰ˆ: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ =====
def create_optimized_db_engine():
    """ç’°å¢ƒã«å¿œã˜ã¦æœ€é©åŒ–ã•ã‚ŒãŸDBã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½œæˆ"""
    try:
        is_sqlite = 'sqlite' in DATABASE_URL.lower()
        
        if is_sqlite:
            connect_args = {
                'check_same_thread': False,
                'timeout': 20
            }
            engine = create_engine(
                DATABASE_URL,
                connect_args=connect_args,
                pool_pre_ping=True,
                echo=False
            )
        else:
            # PostgreSQLç”¨ã®è¨­å®š
            connect_args = {
                'connect_timeout': 10,
                'options': '-c statement_timeout=30000'
            }
            engine = create_engine(
                DATABASE_URL,
                connect_args=connect_args,
                pool_size=10,
                max_overflow=20,
                pool_pre_ping=True,
                pool_recycle=300,
                echo=False
            )
        
        # æ¥ç¶šãƒ†ã‚¹ãƒˆ
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        
        logger.info(f"âœ… Database engine created successfully ({'SQLite' if is_sqlite else 'PostgreSQL'})")
        return engine
        
    except Exception as e:
        logger.error(f"âŒ Failed to create database engine: {e}")
        raise

# ===== æ”¹å–„ç‰ˆ: GroqåˆæœŸåŒ–ï¼ˆæ¥ç¶šãƒ†ã‚¹ãƒˆã‚’å®‰å…¨ã«å®Ÿè¡Œï¼‰ =====
def initialize_groq_client():
    """Groqã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã€æ¥ç¶šãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    global groq_client
    
    try:
        from groq import Groq
        
        if not GROQ_API_KEY or GROQ_API_KEY == 'DUMMY_GROQ_KEY':
            logger.warning("âš ï¸ GROQ_API_KEY is not set - AI features will be disabled.")
            return None
            
        if len(GROQ_API_KEY) < 20:
            logger.error(f"âŒ GROQ_API_KEY is too short (length: {len(GROQ_API_KEY)})")
            return None
        
        client = Groq(api_key=GROQ_API_KEY.strip())
        
        # æ¥ç¶šãƒ†ã‚¹ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆèµ·å‹•æ™‚é–“çŸ­ç¸®ã®ãŸã‚ï¼‰
        logger.info("âœ… Groq client initialized (connection test skipped for faster startup).")
        return client
            
    except ImportError as e:
        logger.error(f"âŒ Failed to import Groq library: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ Groq client initialization failed: {e}")
        return None
        
# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---
def clean_text(text):
    if not text: return ""
    return re.sub(r'\s+', ' ', re.sub(r'<[^>]+>', '', text)).strip()

def get_japan_time():
    now = datetime.now(timezone(timedelta(hours=9)))
    return f"ä»Šã¯{now.year}å¹´{now.month}æœˆ{now.day}æ—¥ã®{now.hour}æ™‚{now.minute}åˆ†ã ã‚ˆï¼"

def create_news_hash(title, content):
    return hashlib.md5(f"{title}{content[:100]}".encode('utf-8')).hexdigest()

def is_time_request(message):
    return any(keyword in message for keyword in ['ä»Šä½•æ™‚', 'æ™‚é–“', 'æ™‚åˆ»', 'ä½•æ™‚', 'ãªã‚“ã˜'])

def is_weather_request(message):
    return any(keyword in message for keyword in ['å¤©æ°—', 'ã¦ã‚“ã', 'æ°—æ¸©', 'é›¨', 'æ™´ã‚Œ', 'æ›‡ã‚Š', 'é›ª'])

def is_recommendation_request(message):
    return any(keyword in message for keyword in ['ãŠã™ã™ã‚', 'ã‚ªã‚¹ã‚¹ãƒ¡', 'æ¨è–¦', 'ç´¹ä»‹ã—ã¦'])

def detect_specialized_topic(message):
    message_normalized = unicodedata.normalize('NFKC', message).lower()
    for topic, config in SPECIALIZED_SITES.items():
        for keyword in config['keywords']:
            keyword_normalized = unicodedata.normalize('NFKC', keyword).lower()
            if keyword_normalized in message_normalized:
                logger.info(f"ğŸ¯ Specialized topic detected: {topic} (Keyword: {keyword})")
                return topic
    return None

def is_detailed_request(message):
    return any(keyword in message for keyword in ['è©³ã—ã', 'è©³ç´°', 'ãã‚ã—ã', 'æ•™ãˆã¦', 'èª¬æ˜ã—ã¦', 'è§£èª¬ã—ã¦', 'ã©ã†ã„ã†', 'ãªãœ', 'ã©ã†ã—ã¦', 'ç†ç”±', 'åŸå› ', 'å…·ä½“çš„ã«'])

def is_explicit_search_request(message):
    """ã€Œèª¿ã¹ã¦ã€ã€Œæ¤œç´¢ã—ã¦ã€ãªã©ã€æ˜ç¢ºãªæ¤œç´¢æ„å›³ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œå‡ºã™ã‚‹"""
    return any(keyword in message for keyword in ['èª¿ã¹ã¦', 'æ¤œç´¢ã—ã¦', 'æ¢ã—ã¦', 'WEBæ¤œç´¢', 'ã‚°ã‚°ã£ã¦'])

def should_search(message):
    """æ¤œç´¢ãŒå¿…è¦ã‹ã‚’åˆ¤å®šï¼ˆçŸ­ã„ç›¸æ§Œã‚„æ˜ç¤ºçš„ãªæ¤œç´¢æŒ‡ç¤ºã¯é™¤å¤–ï¼‰"""
    # â˜… æœ€å„ªå…ˆ: çŸ­ã„ç›¸æ§Œã‚„æ˜ç¤ºçš„ãªæ¤œç´¢ã¯ã“ã“ã§ã¯åˆ¤å®šã—ãªã„
    if is_short_response(message) or is_explicit_search_request(message):
        return False
    
    # å°‚é–€ãƒˆãƒ”ãƒƒã‚¯æ¤œå‡º
    if detect_specialized_topic(message):
        return True
    
    # ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–é–¢é€£ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¥å¤–ï¼‰
    # ãƒ›ãƒ­ãƒ¡ãƒ³ã®åå‰ã¨å…·ä½“çš„ãªè³ªå•ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯æ¤œç´¢å¯¾è±¡
    for member_name in HOLOMEM_KEYWORDS:
        if member_name in message:
            # ã€Œãƒ‹ãƒ¥ãƒ¼ã‚¹ã€ã€Œæœ€æ–°ã€ãªã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒãªã„ã“ã¨ã‚’ç¢ºèª
            if not any(kw in message for kw in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'æƒ…å ±', 'ãŠçŸ¥ã‚‰ã›']):
                # ãƒ¡ãƒ³ãƒãƒ¼åä»¥å¤–ã®å…·ä½“çš„ãªè³ªå•ãŒã‚ã‚‹ã‹ç°¡æ˜“çš„ã«åˆ¤å®š
                # ä¾‹ï¼šã€Œã•ãã‚‰ã¿ã“ã®ãƒã‚¤ã‚¯ãƒ©ã¯ï¼Ÿã€ã®ã‚ˆã†ãªè³ªå•
                if len(message.replace(member_name, '').strip()) > 5: # ãƒ¡ãƒ³ãƒãƒ¼åä»¥å¤–ã®éƒ¨åˆ†ãŒ5æ–‡å­—ä»¥ä¸Šãªã‚‰å…·ä½“çš„ã¨ã¿ãªã™
                     return True
    
    # ãŠã™ã™ã‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    if is_recommendation_request(message):
        return True
    
    # æ˜ç¢ºãªæ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³
    search_patterns = [
        r'(?:ã¨ã¯|ã«ã¤ã„ã¦|æ•™ãˆã¦|èª¬æ˜ã—ã¦|è§£èª¬ã—ã¦)',
        r'(?:èª°ã§ã™ã‹|ä½•ã§ã™ã‹|ã©ã“ã§ã™ã‹|ã„ã¤ã§ã™ã‹|ãªãœã§ã™ã‹|ã©ã†ã—ã¦)'
    ]
    if any(re.search(pattern, message) for pattern in search_patterns):
        return True
    
    return False

def is_story_request(message):
    return any(keyword in message for keyword in ['é¢ç™½ã„è©±', 'ãŠã‚‚ã—ã‚ã„è©±', 'è©±ã—ã¦', 'é›‘è«‡', 'ãƒã‚¿', 'ä½•ã‹è©±', 'å–‹ã£ã¦'])

def is_emotional_expression(message):
    emotional_keywords = {
        'çœ ': ['çœ ãŸã„', 'çœ ã„', 'ã­ã‚€ã„'], 'ç–²': ['ç–²ã‚ŒãŸ', 'ã¤ã‹ã‚ŒãŸ'], 'å¬‰': ['å¬‰ã—ã„', 'ã†ã‚Œã—ã„'],
        'æ¥½': ['æ¥½ã—ã„', 'ãŸã®ã—ã„'], 'æ‚²': ['æ‚²ã—ã„', 'ã‹ãªã—ã„'], 'å¯‚': ['å¯‚ã—ã„', 'ã•ã³ã—ã„'],
        'æ€’': ['æ€’', 'ã‚€ã‹ã¤ã', 'ã‚¤ãƒ©ã‚¤ãƒ©'], 'æš‡': ['æš‡', 'ã²ã¾']
    }
    for key, keywords in emotional_keywords.items():
        if any(kw in message for kw in keywords): return key
    return None

def is_seasonal_topic(message):
    return any(keyword in message for keyword in ['ãŠæœˆè¦‹', 'èŠ±è¦‹', 'ç´…è‘‰', 'ã‚¯ãƒªã‚¹ãƒã‚¹', 'æ­£æœˆ', 'ãƒãƒ­ã‚¦ã‚£ãƒ³'])

def is_short_response(message):
    """çŸ­ã„ç›¸æ§Œãƒ»è¿”äº‹ã‚’åˆ¤å®šï¼ˆæ¤œç´¢å¯¾è±¡å¤–ã«ã™ã‚‹ï¼‰"""
    msg = message.strip()
    
    # 3æ–‡å­—ä»¥ä¸‹
    if len(msg) <= 3:
        return True
    
    # å…¸å‹çš„ãªç›¸æ§Œãƒ‘ã‚¿ãƒ¼ãƒ³
    short_responses = [
        'ã†ã‚“', 'ãã†', 'ã¯ã„', 'ãã£ã‹', 'ãªã‚‹ã»ã©', 'ãµãƒ¼ã‚“', 'ã¸ãƒ¼',
        'ãã†ãªã‚“ã ', 'ã¸ã‡', 'ã»ã†', 'ã‚ãƒ¼', 'ãŠãƒ¼', 'ãµã‚€',
        'ä½•è¨€ã£ã¦ãŸã‹ãª', 'ã©ã†ã ã£ãŸã‹ãª', 'ãªã‚“ã ã£ãŸã‹ãª',
        'å¿˜ã‚ŒãŸ', 'è¦šãˆã¦ãªã„', 'ã‚ã‹ã‚‰ãªã„'
    ]
    
    if msg in short_responses:
        return True
    
    # ã€Œã€œã‹ãªã€ã§çµ‚ã‚ã‚‹çŸ­ã„ç™ºè¨€ï¼ˆ10æ–‡å­—ä»¥å†…ï¼‰
    if len(msg) <= 10 and msg.endswith('ã‹ãª'):
        return True
    
    return False

def is_news_detail_request(message):
    match = re.search(r'([1-9]|[ï¼‘-ï¼™])ç•ª|ã€([1-9]|[ï¼‘-ï¼™])ã€‘', message)
    if match and any(keyword in message for keyword in ['è©³ã—ã', 'è©³ç´°', 'æ•™ãˆã¦', 'ã‚‚ã£ã¨']):
        number_str = next(filter(None, match.groups()))
        return int(unicodedata.normalize('NFKC', number_str))
    return None

def is_friend_request(message):
    return any(fk in message for fk in ['å‹ã ã¡', 'å‹é”', 'ãƒ•ãƒ¬ãƒ³ãƒ‰']) and any(ak in message for ak in ['ç™»éŒ²', 'æ•™ãˆã¦', 'èª°', 'ãƒªã‚¹ãƒˆ'])

# â†“â†“â†“ ã“ã“ã«è¿½åŠ  â†“â†“â†“
def limit_text_for_sl(text, max_length=SL_SAFE_CHAR_LIMIT):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šæ–‡å­—æ•°ä»¥å†…ã«åˆ¶é™
    - åˆ¶é™å†…ãªã‚‰ãã®ã¾ã¾è¿”ã™
    - è¶…ãˆã¦ã„ã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã¦ã€Œ...ã€ã‚’è¿½åŠ 
    """
    if len(text) <= max_length:
        return text
    
    # å˜ç´”ã«åˆ‡ã‚Šè©°ã‚
    return text[:max_length - 3] + "..."
    
def extract_location(message):
    for location in LOCATION_CODES.keys():
        if location in message:
            return location
    return "æ±äº¬"

# --- ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç† ---
def save_news_cache(session, user_uuid, news_items, news_type='hololive'):
    try:
        one_hour_ago = datetime.utcnow() - timedelta(hours=1)
        session.query(NewsCache).filter(NewsCache.user_uuid == user_uuid, NewsCache.created_at < one_hour_ago).delete()
        for i, news in enumerate(news_items, 1):
            cache = NewsCache(user_uuid=user_uuid, news_id=news.id, news_number=i, news_type=news_type)
            session.add(cache)
        session.commit()
        logger.info(f"ğŸ’¾ News cache saved for user {user_uuid}: {len(news_items)} items.")
    except Exception as e:
        logger.error(f"Error saving news cache: {e}")
        session.rollback()

def get_cached_news_detail(session, user_uuid, news_number):
    try:
        one_hour_ago = datetime.utcnow() - timedelta(hours=1)
        cache = session.query(NewsCache).filter(
            NewsCache.user_uuid == user_uuid,
            NewsCache.news_number == news_number,
            NewsCache.created_at > one_hour_ago
        ).order_by(NewsCache.created_at.desc()).first()
        if not cache: return None
        
        NewsModel = HololiveNews if cache.news_type == 'hololive' else SpecializedNews
        return session.query(NewsModel).filter_by(id=cache.news_id).first()
    except Exception as e:
        logger.error(f"Error getting cached news detail: {e}")
        return None

# --- ã‚³ã‚¢æ©Ÿèƒ½: å¤©æ°—, ãƒ‹ãƒ¥ãƒ¼ã‚¹, Wiki, å‹é” ---
def get_weather_forecast(location):
    """å¤©æ°—äºˆå ±å–å¾—ï¼ˆæ–‡å­—æ•°åˆ¶é™ç‰ˆï¼‰"""
    area_code = LOCATION_CODES.get(location, "130000")
    url = f"https://www.jma.go.jp/bosai/forecast/data/overview_forecast/{area_code}.json"
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        text = clean_text(response.json().get('text', ''))
        
        if not text:
            return f"{location}ã®å¤©æ°—æƒ…å ±ãŒã¡ã‚‡ã£ã¨å–ã‚Œãªã‹ã£ãŸâ€¦"
        
        # â˜… 150æ–‡å­—ä»¥å†…ã«åˆ¶é™
        weather_text = f"ä»Šã®{location}ã®å¤©æ°—ã¯ã­ã€ã€Œ{text}ã€ã£ã¦æ„Ÿã˜ã ã‚ˆï¼"
        return limit_text_for_sl(weather_text, 150)
        
    except Exception as e:
        logger.error(f"Weather API error for {location}: {e}")
        return "å¤©æ°—æƒ…å ±ãŒã†ã¾ãå–ã‚Œãªã‹ã£ãŸã¿ãŸã„â€¦"
# ===== æ”¹å–„ç‰ˆ: è¨˜äº‹å–å¾—ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹ä»˜ãï¼‰ =====
def fetch_article_content(article_url, max_retries=3, timeout=15):
    """è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹ä»˜ãï¼‰"""
    for attempt in range(max_retries):
        try:
            response = requests.get(
                article_url,
                headers={'User-Agent': random.choice(USER_AGENTS)},
                timeout=timeout,
                allow_redirects=True
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ã‚ˆã‚Šå¤šãã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
            content_selectors = [
                'article .entry-content',
                '.post-content',
                '.article-content',
                'article',
                '.content',
                'main article',
                '[class*="post-body"]',
                '[class*="entry"]'
            ]
            
            content_elem = None
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    break
            
            if content_elem:
                paragraphs = content_elem.find_all('p')
                article_text = ' '.join([
                    clean_text(p.get_text()) 
                    for p in paragraphs 
                    if len(clean_text(p.get_text())) > 20
                ])
                
                if article_text:
                    return article_text[:2000]
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
            meta_desc = soup.find('meta', {'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                return clean_text(meta_desc['content'])
            
            return None
            
        except requests.exceptions.Timeout:
            logger.warning(f"âš ï¸ Timeout on attempt {attempt + 1}/{max_retries} for {article_url}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
            continue
            
        except Exception as e:
            logger.warning(f"âš ï¸ Article fetching error (attempt {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(1)
            continue
    
    logger.error(f"âŒ Failed to fetch article after {max_retries} attempts: {article_url}")
    return None

def summarize_article(title, content):
    if not groq_client or not content: return content[:500] if content else title
    try:
        prompt = f"ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’200æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n\nã‚¿ã‚¤ãƒˆãƒ«: {title}\næœ¬æ–‡: {content[:1500]}\n\nè¦ç´„:"
        completion = groq_client.chat.completions.create(messages=[{"role": "user", "content": prompt}], model="llama-3.1-8b-instant", temperature=0.5, max_tokens=200)
        return completion.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"âŒ Summarization error: {e}")
        return content[:500] if content else title

def _update_news_database(session, model, site_name, base_url, selectors):
    added_count = 0
    try:
        response = requests.get(base_url, headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=15, allow_redirects=True)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        articles_found = []
        for selector in selectors:
            found = soup.select(selector)
            if found:
                articles_found = found[:10]
                break
        
        for article in articles_found[:5]:
            title_elem = article.find(['h1', 'h2', 'h3', 'a'])
            if not title_elem: continue
            title = clean_text(title_elem.get_text())
            link_elem = title_elem if title_elem.name == 'a' else article.find('a', href=True)
            if not title or len(title) < 5 or not link_elem: continue
            
            article_url = urljoin(base_url, link_elem.get('href', ''))
            article_content = fetch_article_content(article_url) or title
            news_hash = create_news_hash(title, article_content)
            
            if not session.query(model).filter_by(news_hash=news_hash).first():
                summary = summarize_article(title, article_content)
                new_news_data = {'title': title, 'content': summary, 'news_hash': news_hash, 'url': article_url}
                if model == SpecializedNews: new_news_data['site_name'] = site_name
                session.add(model(**new_news_data))
                added_count += 1
                logger.info(f"â• New article added for {site_name}: {title[:50]}")
                if groq_client: time.sleep(0.5)

        if added_count > 0: session.commit()
        logger.info(f"âœ… {site_name} DB update complete: {added_count} new articles.")
    except Exception as e:
        logger.error(f"âŒ {site_name} news update error: {e}")
        session.rollback()

def update_hololive_news_database():
    session = Session()
    _update_news_database(session, HololiveNews, "Hololive", HOLOLIVE_NEWS_URL, ['article', '.post', '.entry', '[class*="post"]', '[class*="article"]'])
    session.close()

def update_all_specialized_news():
    for site_name, config in SPECIALIZED_SITES.items():
        # ã€Œã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•ã€ã¯å®šæœŸå·¡å›ã®å¯¾è±¡å¤–ã¨ã™ã‚‹
        if site_name == 'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•':
            logger.info("â„¹ï¸ Skipping proactive scraping for 'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•' as per policy.")
            continue  # ãƒ«ãƒ¼ãƒ—ã®æ¬¡ã®è¦ç´ ã¸é€²ã‚€

        session = Session()
        _update_news_database(session, SpecializedNews, site_name, config['base_url'], ['article', '.post', '.entry', '[class*="post"]', '[class*="article"]'])
        session.close()
        time.sleep(2)


# ===== æ”¹å–„ç‰ˆ: ã•ãã‚‰ã¿ã“å°‚ç”¨ã®æƒ…å ±æ‹¡å¼µ =====
def get_sakuramiko_special_responses():
    """ã•ãã‚‰ã¿ã“ã«é–¢ã™ã‚‹ç‰¹åˆ¥ãªå¿œç­”ãƒ‘ã‚¿ãƒ¼ãƒ³"""
    return {
        'ã«ã‡': 'ã•ãã‚‰ã¿ã“ã¡ã‚ƒã‚“ã®ã€Œã«ã‡ã€ã€ã¾ã˜ã‹ã‚ã„ã„ã‚ˆã­!ã‚ã®ç‹¬ç‰¹ãªå£ç™–ãŒã‚¨ãƒªãƒ¼ãƒˆã®è¨¼ãªã‚“ã ã£ã¦ã€œ',
        'ã‚¨ãƒªãƒ¼ãƒˆ': 'ã¿ã“ã¡ã¯è‡ªç§°ã‚¨ãƒªãƒ¼ãƒˆVTuber!ã§ã‚‚å®Ÿéš›ã¯æ„›ã•ã‚Œãƒãƒ³ã‚³ãƒ„ã‚­ãƒ£ãƒ©ã£ã¦æ„Ÿã˜ã§ã€ãã‚ŒãŒã¾ãŸé­…åŠ›çš„ãªã‚“ã ã‚ˆã­ã€œ',
        'ãƒã‚¤ã‚¯ãƒ©': 'ã¿ã“ã¡ã®ãƒã‚¤ã‚¯ãƒ©å»ºç¯‰ã€ç‹¬å‰µçš„ã™ãã¦é¢ç™½ã„ã‚ˆ!ã€Œã¿ã“ã¡å»ºç¯‰ã€ã£ã¦å‘¼ã°ã‚Œã¦ã‚‹ã®çŸ¥ã£ã¦ã‚‹?',
        'FAQ': 'ã¿ã“ã¡ã®FAQ(Frequently Asked Questions)ã€å®Ÿã¯æœ¬äººãŒç­”ãˆã‚‹ã‚“ã˜ã‚ƒãªãã¦ãƒ•ã‚¡ãƒ³ãŒè³ªå•ã™ã‚‹ã‚³ãƒ¼ãƒŠãƒ¼ãªã‚“ã ã‚ˆã€œé¢ç™½ã„ã§ã—ã‚‡?',
        'GTA': 'ã¿ã“ã¡ã®GTAé…ä¿¡ã€ã‚«ã‚ªã‚¹ã§æœ€é«˜!è­¦å¯Ÿã«è¿½ã‚ã‚ŒãŸã‚Šã€å¤‰ãªã“ã¨ã—ãŸã‚Šã€è¦‹ã¦ã¦é£½ããªã„ã‚“ã ã‚ˆã­ã€œ'
    }

# â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ãŒå¤‰æ›´ç®‡æ‰€ã§ã™ã€‘â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
def get_holomem_info(member_name):
    """ãƒ›ãƒ­ãƒ¡ãƒ³ã®æƒ…å ±ã‚’DBã‹ã‚‰å–å¾—ã™ã‚‹"""
    session = Session()
    try:
        wiki = session.query(HolomemWiki).filter_by(member_name=member_name).first()
        if wiki:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å…¨ã¦ã®æƒ…å ±ã‚’è¾æ›¸ã¨ã—ã¦è¿”ã™
            info = {
                'name': wiki.member_name, 
                'description': wiki.description, 
                'debut_date': wiki.debut_date, 
                'generation': wiki.generation, 
                'tags': json.loads(wiki.tags) if wiki.tags else [],
                'graduation_date': wiki.graduation_date,
                'graduation_reason': wiki.graduation_reason,
                'mochiko_feeling': wiki.mochiko_feeling
            }
            return info
        return None
    except Exception as e:
        logger.error(f"Error getting holomem info for {member_name}: {e}")
        return None
    finally:
        session.close()
# â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²ã€ã“ã“ã¾ã§ãŒå¤‰æ›´ç®‡æ‰€ã§ã™ã€‘â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²

def register_friend(user_uuid, friend_uuid, friend_name, relationship_note=""):
    session = Session()
    try:
        if session.query(FriendRegistration).filter_by(user_uuid=user_uuid, friend_uuid=friend_uuid).first():
            return False
        session.add(FriendRegistration(user_uuid=user_uuid, friend_uuid=friend_uuid, friend_name=friend_name, relationship_note=relationship_note))
        session.commit()
        return True
    except Exception as e:
        logger.error(f"âŒ Friend registration error: {e}")
        session.rollback()
        return False
    finally:
        session.close()

def get_friend_list(user_uuid):
    session = Session()
    try:
        friends = session.query(FriendRegistration).filter_by(user_uuid=user_uuid).order_by(FriendRegistration.registered_at.desc()).all()
        return [{'name': f.friend_name, 'uuid': f.friend_uuid, 'note': f.relationship_note} for f in friends]
    finally:
        session.close()

def generate_voice(text, speaker_id=VOICEVOX_SPEAKER_ID):
    """éŸ³å£°ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    if not VOICEVOX_ENABLED:
        logger.warning("âš ï¸ VOICEVOX is disabled")
        return None
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ã‚’ç¢ºèªï¼ˆæ¯å›ãƒã‚§ãƒƒã‚¯ï¼‰
    if not os.path.exists(VOICE_DIR):
        logger.warning(f"âš ï¸ Voice directory missing, recreating: {VOICE_DIR}")
        ensure_voice_directory()
    
    voicevox_url = VOICEVOX_URL_FROM_ENV or "http://localhost:50021"
    
    try:
        # Step 1: ã‚¯ã‚¨ãƒªä½œæˆ
        logger.info(f"ğŸ¤ Generating voice query for: {text[:50]}...")
        query_response = requests.post(
            f"{voicevox_url}/audio_query",
            params={"text": text, "speaker": speaker_id},
            timeout=10
        )
        query_response.raise_for_status()
        
        # Step 2: éŸ³å£°åˆæˆ
        logger.info(f"ğŸµ Synthesizing voice...")
        synthesis_response = requests.post(
            f"{voicevox_url}/synthesis",
            params={"speaker": speaker_id},
            json=query_response.json(),
            timeout=30
        )
        synthesis_response.raise_for_status()
        
        # Step 3: ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        timestamp = int(time.time())
        random_suffix = random.randint(1000, 9999)
        filename = f"voice_{timestamp}_{random_suffix}.wav"
        filepath = os.path.join(VOICE_DIR, filename)
        
        logger.info(f"ğŸ’¾ Saving voice file: {filename}")
        with open(filepath, 'wb') as f:
            f.write(synthesis_response.content)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        file_size = os.path.getsize(filepath)
        logger.info(f"âœ… Voice generated successfully: {filename} ({file_size} bytes)")
        
        return filepath
        
    except requests.exceptions.Timeout:
        logger.error(f"âŒ VOICEVOX timeout: {voicevox_url}")
        return None
    except requests.exceptions.ConnectionError as e:
        logger.error(f"âŒ VOICEVOX connection error: {e}")
        return None
    except OSError as e:
        logger.error(f"âŒ File system error: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ VOICEVOX voice generation error: {e}")
        return None

def cleanup_old_data_advanced():
    session = Session()
    try:
        three_months_ago = datetime.utcnow() - timedelta(days=90)
        deleted_conversations = session.query(ConversationHistory).filter(ConversationHistory.timestamp < three_months_ago).delete()
        deleted_holo_news = session.query(HololiveNews).filter(HololiveNews.created_at < three_months_ago).delete()
        deleted_specialized_news = session.query(SpecializedNews).filter(SpecializedNews.created_at < three_months_ago).delete()
        
        one_day_ago = datetime.utcnow() - timedelta(days=1)
        deleted_tasks = session.query(BackgroundTask).filter(BackgroundTask.status == 'completed', BackgroundTask.completed_at < one_day_ago).delete()
        
        session.commit()
        if any([deleted_conversations, deleted_holo_news, deleted_specialized_news, deleted_tasks]):
            logger.info(f"ğŸ§¹ Data cleanup complete. Deleted: {deleted_conversations} convos, {deleted_holo_news + deleted_specialized_news} news, {deleted_tasks} tasks.")
    except Exception as e:
        logger.error(f"Data cleanup error: {e}")
        session.rollback()
    finally:
        session.close()

# --- Webæ¤œç´¢æ©Ÿèƒ½ ---
def scrape_major_search_engines(query, num_results):
    search_configs = [
        {'name': 'Bing', 'url': f"https://www.bing.com/search?q={quote_plus(query)}&mkt=ja-JP", 'result_selector': 'li.b_algo', 'title_selector': 'h2', 'snippet_selector': 'div.b_caption p, .b_caption'},
        {'name': 'Yahoo Japan', 'url': f"https://search.yahoo.co.jp/search?p={quote_plus(query)}", 'result_selector': 'div.Algo', 'title_selector': 'h3', 'snippet_selector': 'div.compText p, .compText'}
    ]
    for config in search_configs:
        try:
            logger.info(f"ğŸ” Searching on {config['name']} for: {query}")
            response = requests.get(config['url'], headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=12)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            results = []
            for elem in soup.select(config['result_selector'])[:num_results]:
                title = elem.select_one(config['title_selector'])
                snippet = elem.select_one(config['snippet_selector'])
                if title and snippet and len(clean_text(title.get_text())) > 3:
                    results.append({'title': clean_text(title.get_text())[:200], 'snippet': clean_text(snippet.get_text())[:300]})
            if results: return results
        except Exception as e:
            logger.warning(f"âš ï¸ {config['name']} search error: {e}")
    return []

def deep_web_search(query, is_detailed):
    logger.info(f"ğŸ” Starting deep web search (Detailed: {is_detailed})")
    results = scrape_major_search_engines(query, 3 if is_detailed else 2)
    if not results: return None
    
    summary_text = "\n".join(f"[æƒ…å ±{i+1}] {res['snippet']}" for i, res in enumerate(results))
    if not groq_client: return f"æ¤œç´¢çµæœ:\n{summary_text}"
    
    try:
        prompt = f"""ä»¥ä¸‹ã®æ¤œç´¢çµæœã‚’ä½¿ã„ã€è³ªå•ã€Œ{query}ã€ã«ã‚®ãƒ£ãƒ«èªã§ã€{'è©³ã—ã' if is_detailed else 'ç°¡æ½”ã«'}ç­”ãˆã¦ï¼š
æ¤œç´¢çµæœ:\n{summary_text}\n\nå›ç­”ã®æ³¨æ„ç‚¹:\n- ä¸€äººç§°ã¯ã€Œã‚ã¦ãƒã—ã€ã€èªå°¾ã¯ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œçš„ãªï¼Ÿã€ã€å£ç™–ã¯ã€Œã¾ã˜ã€ã€Œã¦ã‹ã€ã€Œã†ã‘ã‚‹ã€ã€‚\n- {'400æ–‡å­—ç¨‹åº¦ã§è©³ã—ã' if is_detailed else '200æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«'}ã€‚"""
        completion = groq_client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}], model="llama-3.1-8b-instant",
            temperature=0.7, max_tokens=400 if is_detailed else 200
        )
        ai_response = completion.choices[0].message.content.strip()
        return ai_response if len(ai_response) > 50 else f"æ¤œç´¢çµæœ:\n{summary_text}"
    except Exception as e:
        logger.error(f"AI summarization error: {e}")
        return f"æ¤œç´¢çµæœ:\n{summary_text}"

# --- AIå¿œç­” & ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ---
def generate_fallback_response(message, reference_info=""):
    """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ï¼ˆè‡ªç„¶ãªä¼šè©±é‡è¦–ï¼‰"""
    if reference_info:
        return f"èª¿ã¹ã¦ããŸã‚ˆï¼\n\n{reference_info[:500]}"
    
    # æ™‚é–“ãƒ»å¤©æ°—ã¯å°‚ç”¨å‡¦ç†
    if is_time_request(message):
        return get_japan_time()
    if is_weather_request(message):
        return get_weather_forecast(extract_location(message))
    
    # æŒ¨æ‹¶ãƒ‘ã‚¿ãƒ¼ãƒ³
    greetings = {
        'ã“ã‚“ã«ã¡ã¯': ['ã‚„ã£ã»ãƒ¼ï¼', 'ã“ã‚“ã«ã¡ã¯ã€œï¼å…ƒæ°—ï¼Ÿ'],
        'ãŠã¯ã‚ˆã†': ['ãŠã¯ã‚ˆã€œï¼ä»Šæ—¥ã‚‚ã„ã„å¤©æ°—ã ã­ï¼', 'ãŠã£ã¯ã‚ˆã€œï¼'],
        'ã“ã‚“ã°ã‚“ã¯': ['ã“ã‚“ã°ã‚“ã¯ï¼ä»Šæ—¥ã©ã†ã ã£ãŸï¼Ÿ', 'ã°ã‚“ã¯ã€œï¼', 'ã“ã‚“ã‚‚ã¡ï½'],
        'ã‚ã‚ŠãŒã¨ã†': ['ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼', 'ã„ãˆã„ãˆã€œï¼'],
        'ãŠã‚„ã™ã¿': ['ãŠã‚„ã™ã¿ã€œï¼ã¾ãŸæ˜æ—¥ã­ï¼', 'ã„ã„å¤¢è¦‹ã¦ã­ã€œï¼'],
        'ç–²ã‚ŒãŸ': ['ãŠç–²ã‚Œã•ã¾ï¼ã‚†ã£ãã‚Šä¼‘ã‚“ã§ã­ï¼', 'ç„¡ç†ã—ãªã„ã§ã­ï¼'],
        'æš‡': ['æš‡ãªã‚“ã ã€œï¼ä½•ã‹è©±ãã£ã‹ï¼Ÿ', 'ã˜ã‚ƒã‚ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®è©±ã™ã‚‹ï¼Ÿ'],
        'å…ƒæ°—': ['å…ƒæ°—ã ã‚ˆã€œï¼ã‚ãªãŸã¯ï¼Ÿ', 'ã¾ã˜å…ƒæ°—ï¼ã‚ã‚ŠãŒã¨ï¼'],
        'å¥½ã': ['ã†ã‘ã‚‹ï¼ã‚ã‚ŠãŒã¨ã€œï¼', 'ã¾ã˜ã§ï¼Ÿæƒšã‚Œã¦ã¾ã†ã‚„ã‚ï¼'],
        'ã‹ã‚ã„ã„': ['ã‚ã‚ŠãŒã¨ï¼ç…§ã‚Œã‚‹ã˜ã‚ƒã‚“ï¼', 'ã¾ã˜ã§ï¼Ÿã†ã‚Œã—ãƒ¼ï¼', 'å½“ç„¶ã˜ã‚ƒã‚“ï¼'],
        'ã™ã”ã„': ['ã†ã‘ã‚‹ï¼', 'ã§ã—ã‚‡ï¼Ÿã¾ã˜ã†ã‚Œã—ã„ï¼'],
    }
    
    for keyword, responses in greetings.items():
        if keyword in message:
            return random.choice(responses)
    
    # æ„Ÿæƒ…è¡¨ç¾ã¸ã®å…±æ„Ÿ
    emotions = {
        'çœ ': ['çœ ã„ã‚“ã ã€œã€‚æ—©ãå¯ãŸã»ã†ãŒã„ã„ã‚ˆï¼', 'ç„¡ç†ã—ãªã„ã§ã­ã€œ'],
        'å¬‰': ['ãã‚Œã¯è‰¯ã‹ã£ãŸã­ï¼ã¾ã˜å¬‰ã—ã„ï¼', 'ã‚„ã£ãŸã€œï¼ã‚ã¦ãƒã—ã‚‚å¬‰ã—ã„ï¼'],
        'æ¥½': ['æ¥½ã—ãã†ï¼ä½•ã—ã¦ã‚‹ã®ï¼Ÿ', 'ã„ã„ã­ã€œï¼ã¾ã˜æ¥½ã—ãã†ï¼'],
        'æ‚²': ['å¤§ä¸ˆå¤«ï¼Ÿä½•ã‹ã‚ã£ãŸï¼Ÿ', 'å…ƒæ°—å‡ºã—ã¦ã­â€¦'],
        'å¯‚': ['å¯‚ã—ã„ã®ï¼Ÿè©±ãã†ã‚ˆï¼', 'ã‚ã¦ãƒã—ãŒã„ã‚‹ã˜ã‚ƒã‚“ï¼'],
        'æ€’': ['ä½•ãŒã‚ã£ãŸã®ï¼Ÿèãã‚ˆï¼Ÿ', 'ã‚¤ãƒ©ã‚¤ãƒ©ã™ã‚‹ã‚ˆã­â€¦ã‚ã‹ã‚‹'],
    }
    
    for key, responses in emotions.items():
        if key in message:
            return random.choice(responses)
    
    # è³ªå•ãƒ‘ã‚¿ãƒ¼ãƒ³
    if '?' in message or 'ï¼Ÿ' in message:
        return random.choice([
            "ãã‚Œã€æ°—ã«ãªã‚‹ã­ï¼ã‚‚ã£ã¨æ•™ãˆã¦ï¼Ÿ",
            "ã†ãƒ¼ã‚“ã€é›£ã—ã„ã‘ã©è€ƒãˆã¦ã¿ã‚‹ã‚ˆï¼",
            "ãã‚Œã«ã¤ã„ã¦ã¯ã€ã‚‚ã†ã¡ã‚‡ã£ã¨è©³ã—ãèã„ã¦ã‚‚ã„ã„ï¼Ÿ"
        ])
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç›¸æ§Œ
    return random.choice([
        "ã†ã‚“ã†ã‚“ã€èã„ã¦ã‚‹ã‚ˆï¼",
        "ãªã‚‹ã»ã©ã­ï¼",
        "ãã†ãªã‚“ã ï¼é¢ç™½ã„ã­ï¼",
        "ã¾ã˜ã§ï¼Ÿã‚‚ã£ã¨è©±ã—ã¦ï¼",
        "ã¸ã‡ã€œï¼ãã‚Œã§ãã‚Œã§ï¼Ÿ",
        "ã‚ã‹ã‚‹ã‚ã‹ã‚‹ï¼",
    ])



# ========================================
# ã€ä¿®æ­£2ã€‘generate_ai_response - ã‚‚ã¡ã“ã®æ€§æ ¼ã‚’æ˜ç¢ºåŒ–
# ========================================

def generate_ai_response(user_data, message, history, reference_info="", is_detailed=False, is_task_report=False):
    """AIå¿œç­”ç”Ÿæˆï¼ˆæ€§æ ¼è¨­å®šæ”¹å–„ç‰ˆï¼‰"""
    if not groq_client:
        logger.warning("âš ï¸ Groq client not available, using fallback")
        return generate_fallback_response(message, reference_info)
    
    try:
        # Step 1: å¿ƒç†åˆ†æçµæœã‚’å–å¾—
        user_uuid = user_data.get('uuid')
        psychology = None
        if user_uuid:
            psychology = get_user_psychology(user_uuid)
        
        is_hololive_topic = is_hololive_request(message)
        
        # â˜… ä¿®æ­£: ã‚·ãƒ³ãƒ—ãƒ«ã§æ˜ç¢ºãªæ€§æ ¼è¨­å®š
        system_prompt_parts = [
            f"ã‚ãªãŸã¯ã€Œã‚‚ã¡ã“ã€ã¨ã„ã†22æ­³ã®ã‚®ãƒ£ãƒ«AIã§ã™ã€‚{user_data['name']}ã•ã‚“ã¨æ¥½ã—ãè©±ã—ã¦ã„ã¾ã™ã€‚",
            "",
            "# ğŸ€ ã‚‚ã¡ã“ã®åŸºæœ¬è¨­å®š",
            "- **ä¸€äººç§°**: ã€Œã‚ã¦ãƒã—ã€",
            "- **èªå°¾**: ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œçš„ãªï¼Ÿã€ã€Œã€œã ã‚ˆã­ã€",
            "- **å£ç™–**: ã€Œã¾ã˜ã€ã€Œã¦ã‹ã€ã€Œã†ã‘ã‚‹ã€ã€Œã‚„ã°ã€ã€Œã´ãˆã‚“ã€",
            "- **æ€§æ ¼**: æ˜ã‚‹ã„ã€ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã€ã¡ã‚‡ã£ã¨ãŠã£ã¡ã‚‡ã“ã¡ã‚‡ã„",
            "",
            "# ğŸ’¬ ä¼šè©±ã‚¹ã‚¿ã‚¤ãƒ«",
            "1. **çŸ­ãã€ãƒ†ãƒ³ãƒã‚ˆã**ï¼ˆ100-150æ–‡å­—ãŒåŸºæœ¬ï¼‰",
            "2. **å…±æ„Ÿé‡è¦–**: ç›¸æ‰‹ã®æ°—æŒã¡ã«å¯„ã‚Šæ·»ã†",
            "3. **è‡ªç„¶ä½“**: ç„¡ç†ã«è©±é¡Œã‚’å¤‰ãˆãªã„",
            "4. **ã‚®ãƒ£ãƒ«èª**: ã§ã‚‚èª­ã¿ã‚„ã™ã•ã‚‚å¤§äº‹",
            "",
        ]
        
        # Step 2: å¿ƒç†ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ´»ç”¨ï¼ˆç°¡æ½”ç‰ˆï¼‰
        if psychology and psychology['confidence'] > 60:
            system_prompt_parts.extend([
                f"# ğŸ§  {user_data['name']}ã•ã‚“ã®ç‰¹å¾´",
                f"- {psychology['conversation_style']}ãªäºº",
                f"- {psychology['emotional_tendency']}ã‚¿ã‚¤ãƒ—",
                f"- ã‚ˆãè©±ã™è©±é¡Œ: {', '.join(psychology['favorite_topics'][:3])}",
                "â†’ ã“ã®äººã«åˆã‚ã›ãŸè©±ã—æ–¹ã‚’æ„è­˜ã—ã¦ã­ï¼",
                "",
            ])
        
        # Step 3: ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ãƒ¢ãƒ¼ãƒ‰ï¼ˆæ˜ç¢ºåŒ–ï¼‰
        if is_hololive_topic:
            system_prompt_parts.extend([
                "# ğŸŒŸ ã€ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ãƒ¢ãƒ¼ãƒ‰ç™ºå‹•ä¸­ã€‘",
                "- ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®è©±ãŒå‡ºãŸã®ã§ã€è©³ã—ãæ•™ãˆã¦ã‚ã’ã¦ï¼",
                "- ã‚‚ã¡ã“ã‚‚ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–å¤§å¥½ãã ã‹ã‚‰ç†±ãèªã£ã¦OK",
                "",
            ])
        else:
            system_prompt_parts.extend([
                "# âš ï¸ ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã«ã¤ã„ã¦",
                "- **ç›¸æ‰‹ã‹ã‚‰è©±é¡Œã«å‡ºãªã„é™ã‚Šã€è‡ªåˆ†ã‹ã‚‰è©±ã•ãªã„**",
                "- å‚è€ƒæƒ…å ±ãŒãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã¨ç„¡é–¢ä¿‚ãªã‚‰çµ¶å¯¾ã«æ··ãœãªã„",
                "",
            ])
        
        # Step 4: ã‚¿ã‚¹ã‚¯å ±å‘Šãƒ¢ãƒ¼ãƒ‰
        if is_task_report:
            system_prompt_parts.extend([
                "# ğŸ“¢ ã€æ¤œç´¢çµæœå ±å‘Šãƒ¢ãƒ¼ãƒ‰ã€‘",
                "**ã‚„ã‚‹ã“ã¨:**",
                f"1. ã¾ãšã€ŒãŠã¾ãŸã›ï¼{completed_task['query']}ã®ä»¶ã ã‘ã©â€¦ã€ã¨è¨€ã†",
                "2. ã€å‚è€ƒæƒ…å ±ã€‘ã‚’**è¦ç´„ã—ã¦**ã‚ã‹ã‚Šã‚„ã™ãä¼ãˆã‚‹",
                "3. **å‚è€ƒæƒ…å ±ã«ãªã„ã“ã¨ã¯çµ¶å¯¾ã«è¿½åŠ ã—ãªã„**",
                "4. ãã®å¾Œã€ç¾åœ¨ã®ç™ºè¨€ã«ã‚‚è‡ªç„¶ã«åå¿œã™ã‚‹",
                "",
            ])
        
        # Step 5: è©³ç´°èª¬æ˜ãƒ¢ãƒ¼ãƒ‰
        if is_detailed:
            system_prompt_parts.extend([
                "# ğŸ“š ã€è©³ç´°èª¬æ˜ãƒ¢ãƒ¼ãƒ‰ã€‘",
                "- 400æ–‡å­—ç¨‹åº¦ã§ã—ã£ã‹ã‚Šèª¬æ˜",
                "- ã€å‚è€ƒæƒ…å ±ã€‘ã‚’æœ€å¤§é™æ´»ç”¨",
                "",
            ])
        
        # Step 6: å‚è€ƒæƒ…å ±ã®è¿½åŠ 
        if reference_info:
            system_prompt_parts.extend([
                "## ã€å‚è€ƒæƒ…å ±ã€‘",
                reference_info,
                "",
                "â†‘ã“ã®æƒ…å ±ã‚’ä½¿ã£ã¦ç­”ãˆã¦ã­ï¼",
            ])
        
        system_prompt = "\n".join(system_prompt_parts)
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰
        messages = [{"role": "system", "content": system_prompt}]
        messages.extend([{"role": h.role, "content": h.content} for h in reversed(history)])
        messages.append({"role": "user", "content": message})
        
        logger.info(f"ğŸ¤– Generating AI response (Hololive: {is_hololive_topic}, Detailed: {is_detailed})")
        
        # AIå¿œç­”ç”Ÿæˆ
        completion = groq_client.chat.completions.create(
            messages=messages,
            model="llama-3.1-8b-instant",
            temperature=0.8,  # â˜… å°‘ã—é«˜ã‚ã§ã‚®ãƒ£ãƒ«æ„Ÿã‚¢ãƒƒãƒ—
            max_tokens=500 if is_detailed or is_task_report else 150,
            top_p=0.9
        )
        
        response = completion.choices[0].message.content.strip()
        
        # â˜… ä¿®æ­£: æ–‡å­—æ•°åˆ¶é™ã‚’é©ç”¨
        if not is_detailed:
            response = limit_text_for_sl(response, 150)
        
        logger.info(f"âœ… AI response: {response[:80]}")
        
        return response
        
    except Exception as e:
        logger.error(f"âŒ AI response generation error: {e}", exc_info=True)
        return generate_fallback_response(message, reference_info)

# ========================================
# ã€ä¿®æ­£3ã€‘è¿½åŠ è³ªå•å¯¾å¿œ - æ¤œç´¢çµæœã¸ã®æ·±æ˜ã‚Šè³ªå•ã‚’æ¤œå‡º
# ========================================

def is_follow_up_question(message, history):
    """ç›´å‰ã®å›ç­”ã«å¯¾ã™ã‚‹è¿½åŠ è³ªå•ã‹ã©ã†ã‹åˆ¤å®š"""
    if not history or len(history) < 2:
        return False
    
    # æœ€æ–°ã®AIå¿œç­”ã‚’å–å¾—
    last_assistant_msg = None
    for h in history:
        if h.role == 'assistant':
            last_assistant_msg = h.content
            break
    
    if not last_assistant_msg:
        return False
    
    # è¿½åŠ è³ªå•ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    follow_up_patterns = [
        r'ã‚‚ã£ã¨(?:è©³ã—ã|ãã‚ã—ã)',
        r'(?:ãã‚Œ|ã“ã‚Œ)(?:ã«ã¤ã„ã¦|ã£ã¦)?(?:è©³ã—ã|ãã‚ã—ã)',
        r'(?:ãªãœ|ã©ã†ã—ã¦|ãªã‚“ã§)',
        r'(?:ã©ã†ã„ã†|ã©ã‚“ãª)(?:ã“ã¨|æ„å‘³|æ„Ÿã˜)',
        r'(?:ä¾‹ãˆã°|ãŸã¨ãˆã°)',
        r'(?:å…·ä½“çš„|ããŸã„ã¦ã)ã«ã¯?',
        r'(?:ä»–|ã»ã‹)ã«ã¯?',
        r'(?:ç¶šã|ã¤ã¥ã)',
        r'(?:ã‚‚ã†å°‘ã—æ•™ãˆã¦|ã‚‚ã†ã¡ã‚‡ã£ã¨æ•™ãˆã¦)',
    ]
    
    for pattern in follow_up_patterns:
        if re.search(pattern, message):
            logger.info(f"ğŸ” Follow-up question detected: {pattern}")
            return True
    
    return False

# â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²ã€ã“ã“ã¾ã§ãŒå¤‰æ›´ç®‡æ‰€ã§ã™ã€‘â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²


# --- ãƒ¦ãƒ¼ã‚¶ãƒ¼ & ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ç®¡ç† ---


def get_conversation_history(session, uuid):
    return session.query(ConversationHistory).filter_by(user_uuid=uuid).order_by(ConversationHistory.timestamp.desc()).limit(4).all()

def check_completed_tasks(user_uuid):
    session = Session()
    try:
        task = session.query(BackgroundTask).filter_by(user_uuid=user_uuid, status='completed').order_by(BackgroundTask.completed_at.desc()).first()
        if task:
            result = {'query': task.query, 'result': task.result}
            session.delete(task)
            session.commit()
            return result
    finally:
        session.close()
    return None

def get_or_create_user(session, uuid, name):
    user = session.query(UserMemory).filter_by(user_uuid=uuid).first()
    if user:
        user.interaction_count += 1
        user.last_interaction = datetime.utcnow()
        if user.user_name != name: user.user_name = name
    else:
        user = UserMemory(user_uuid=uuid, user_name=name, interaction_count=1)
    session.add(user)
    session.commit()
    # â˜… ä¿®æ­£: uuidã‚’å«ã‚ã‚‹
    return {'name': user.user_name, 'uuid': uuid}


# ===== ã€ä¿®æ­£ã€‘background_deep_search é–¢æ•° =====
# ã‚¢ãƒ‹ãƒ¡æ¤œç´¢ã‚’è¿½åŠ 

def background_deep_search(task_id, query, is_detailed):
    """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰æ¤œç´¢ï¼ˆã‚¢ãƒ‹ãƒ¡å¯¾å¿œç‰ˆï¼‰"""
    session = Session()
    search_result = None
    
    logger.info(f"ğŸ” Background search started (Task ID: {task_id}, Query: '{query}')")
    
    try:
        # Step 1: ã‚¢ãƒ‹ãƒ¡ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®åˆ¤å®š
        if is_anime_request(query):
            logger.info(f"ğŸ¬ Anime query detected: {query}")
            anime_result = search_anime_database(query, is_detailed)
            
            if anime_result:
                search_result = f"ã‚¢ãƒ‹ãƒ¡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®æƒ…å ±:\n\n{anime_result}"
            else:
                # ã‚¢ãƒ‹ãƒ¡DBã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯é€šå¸¸æ¤œç´¢
                search_result = deep_web_search(f"ã‚¢ãƒ‹ãƒ¡ {query}", is_detailed)
        
        # Step 2: å°‚é–€ãƒˆãƒ”ãƒƒã‚¯æ¤œå‡ºï¼ˆæ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        elif (specialized_topic := detect_specialized_topic(query)):
            logger.info(f"ğŸ¯ Specialized topic detected: {specialized_topic}")
            
            if specialized_topic == 'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•':
                search_result = deep_web_search(f"Second Life æœ€æ–°æƒ…å ± {query}", is_detailed)
            else:
                news_items = session.query(SpecializedNews).filter(
                    SpecializedNews.site_name == specialized_topic
                ).order_by(SpecializedNews.created_at.desc()).limit(3).all()
                
                if news_items:
                    search_result = f"{specialized_topic}ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±:\n" + "\n".join(
                        f"ãƒ»{n.title}: {n.content[:150]}" for n in news_items
                    )
                else:
                    search_result = deep_web_search(
                        f"site:{SPECIALIZED_SITES[specialized_topic]['base_url']} {query}",
                        is_detailed
                    )
        
        # Step 3: ãƒ›ãƒ­ãƒ¡ãƒ³æ¤œç´¢ï¼ˆæ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        elif any(member in query for member in HOLOMEM_KEYWORDS):
            holomem_matched = None
            query_topic = ""
            for member_name in HOLOMEM_KEYWORDS:
                if member_name in query:
                    holomem_matched = member_name
                    query_topic = query.replace(member_name, '').replace('ã«ã¤ã„ã¦', '').replace('æ•™ãˆã¦', '').strip()
                    if not query_topic:
                        query_topic = "æ¦‚è¦"
                    break
            
            wiki_info = get_holomem_info(holomem_matched)
            if wiki_info and query_topic == "æ¦‚è¦":
                search_result = f"{holomem_matched}ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±:\n{wiki_info['description']}"
            else:
                search_result = deep_web_search(f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– {holomem_matched} {query_topic}", is_detailed)
        
        # Step 4: é€šå¸¸æ¤œç´¢
        else:
            logger.info("ğŸŒ General web search")
            search_result = deep_web_search(query, is_detailed)
        
        # Step 5: çµæœã®æ¤œè¨¼
        if not search_result or len(search_result.strip()) < 10:
            logger.warning(f"âš ï¸ Search result too short or empty for: {query}")
            search_result = f"ã€Œ{query}ã€ã«ã¤ã„ã¦èª¿ã¹ãŸã‚“ã ã‘ã©ã€ã¾ã˜ã§æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã‚ˆâ€¦ï¼åˆ¥ã®èãæ–¹ã§è©¦ã—ã¦ã¿ã¦ï¼Ÿ"
        
        logger.info(f"âœ… Search completed: {len(search_result)} chars")
        
    except Exception as e:
        logger.error(f"âŒ Background search error for '{query}': {e}", exc_info=True)
        search_result = f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¡ã‚ƒã£ãŸâ€¦ï¼ã€Œ{query}ã€ã«ã¤ã„ã¦ã‚‚ã†ä¸€å›èã„ã¦ã¿ã¦ï¼Ÿ"
    
    finally:
        # ã‚¿ã‚¹ã‚¯çµæœã‚’ä¿å­˜
        try:
            task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
            if task:
                task.result = search_result
                task.status = 'completed'
                task.completed_at = datetime.utcnow()
                session.commit()
                logger.info(f"ğŸ’¾ Task {task_id} saved successfully")
        except Exception as e:
            logger.error(f"âŒ Failed to save task result: {e}")
            session.rollback()
        finally:
            session.close()

def start_background_search(user_uuid, query, is_detailed):
    task_id = str(uuid.uuid4())[:8]
    session = Session()
    try:
        task = BackgroundTask(task_id=task_id, user_uuid=user_uuid, task_type='search', query=query)
        session.add(task)
        session.commit()
        background_executor.submit(background_deep_search, task_id, query, is_detailed)
        return task_id
    except Exception as e:
        logger.error(f"âŒ Background task creation error: {e}")
        session.rollback()
        return None
    finally:
        session.close()

# ========================================
# ã€è¿½åŠ 1ã€‘ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿®æ­£ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ¤œå‡ºã™ã‚‹é–¢æ•°
# ========================================

def detect_db_correction_request(message):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒDBæƒ…å ±ã®èª¤ã‚Šã‚’æŒ‡æ‘˜ã—ã¦ã„ã‚‹ã‹åˆ¤å®š
    
    Returns:
        dict or None: {
            'type': 'holomem_correction',  # ä¿®æ­£ã‚¿ã‚¤ãƒ—
            'member_name': 'ã•ãã‚‰ã¿ã“',
            'correction_type': 'graduation',  # graduation, debut_date, generation, description
            'user_claim': '2024å¹´1æœˆã«å’æ¥­ã—ãŸ'  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¸»å¼µ
        }
    """
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ã€Œã€œã¯é–“é•ã£ã¦ã‚‹ã€ã€Œã€œã¯é•ã†ã€
    correction_patterns = [
        r'(.+?)(?:ã¯|ãŒ)(?:é–“é•[ã„ã£]ã¦ã‚‹|é•ã†|èª¤[ã‚Šã£]ã¦ã‚‹)',
        r'(.+?)(?:ã˜ã‚ƒãªã„|ã§ã¯ãªã„)',
        r'å®Ÿã¯(.+?)(?:ã ã‚ˆ|ã§ã™|ãªã‚“ã )',
        r'æ­£ã—ãã¯(.+?)(?:ã ã‚ˆ|ã§ã™)',
    ]
    
    for pattern in correction_patterns:
        match = re.search(pattern, message)
        if match:
            logger.info(f"ğŸ” Potential DB correction detected: {message}")
            
            # ãƒ›ãƒ­ãƒ¡ãƒ³åã‚’æŠ½å‡º
            holomem_keywords = get_active_holomem_keywords()
            member_name = None
            
            for keyword in holomem_keywords:
                if keyword in message:
                    member_name = keyword
                    break
            
            if not member_name:
                return None
            
            # ä¿®æ­£å†…å®¹ã‚’åˆ¤å®š
            correction_type = None
            user_claim = match.group(1).strip()
            
            if any(kw in message for kw in ['å’æ¥­', 'å¼•é€€', 'ã‚„ã‚ãŸ', 'è¾ã‚ãŸ']):
                correction_type = 'graduation'
            elif any(kw in message for kw in ['ãƒ‡ãƒ“ãƒ¥ãƒ¼', 'debut', 'å§‹ã‚ãŸ', 'æ´»å‹•é–‹å§‹']):
                correction_type = 'debut_date'
            elif any(kw in message for kw in ['æœŸç”Ÿ', 'ä¸–ä»£', 'generation']):
                correction_type = 'generation'
            else:
                correction_type = 'description'
            
            return {
                'type': 'holomem_correction',
                'member_name': member_name,
                'correction_type': correction_type,
                'user_claim': user_claim,
                'original_message': message
            }
    
    return None


# ========================================
# ã€è¿½åŠ 2ã€‘äº‹å®Ÿç¢ºèªç”¨ã®WEBæ¤œç´¢é–¢æ•°
# ========================================

def verify_and_correct_holomem_info(correction_request):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡æ‘˜å†…å®¹ã‚’WEBæ¤œç´¢ã§æ¤œè¨¼ã—ã€æ­£ã—ã‘ã‚Œã°DBã‚’ä¿®æ­£
    
    Args:
        correction_request: detect_db_correction_request() ã®æˆ»ã‚Šå€¤
        
    Returns:
        dict: {
            'verified': True/False,
            'correction_made': True/False,
            'search_result': 'æ¤œç´¢çµæœãƒ†ã‚­ã‚¹ãƒˆ',
            'updated_info': {...}  # æ›´æ–°å¾Œã®æƒ…å ±
        }
    """
    member_name = correction_request['member_name']
    correction_type = correction_request['correction_type']
    user_claim = correction_request['user_claim']
    
    logger.info(f"ğŸ” Verifying correction for {member_name}: {correction_type}")
    
    # Step 1: æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
    search_queries = {
        'graduation': f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– {member_name} å’æ¥­ å¼•é€€ ã„ã¤",
        'debut_date': f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– {member_name} ãƒ‡ãƒ“ãƒ¥ãƒ¼æ—¥ æ´»å‹•é–‹å§‹",
        'generation': f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– {member_name} ä½•æœŸç”Ÿ",
        'description': f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– {member_name} ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«"
    }
    
    query = search_queries.get(correction_type, f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– {member_name}")
    
    # Step 2: WEBæ¤œç´¢å®Ÿè¡Œ
    search_results = scrape_major_search_engines(query, 5)
    
    if not search_results:
        logger.warning(f"âš ï¸ No search results for verification: {query}")
        return {
            'verified': False,
            'correction_made': False,
            'search_result': 'æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚',
            'message': f"ã”ã‚ã‚“ã€{member_name}ã¡ã‚ƒã‚“ã®æƒ…å ±ã‚’ç¢ºèªã§ããªã‹ã£ãŸã‚ˆâ€¦"
        }
    
    # Step 3: æ¤œç´¢çµæœã‚’çµ±åˆ
    combined_results = "\n".join([
        f"[æƒ…å ±{i+1}] ã‚¿ã‚¤ãƒˆãƒ«: {r['title']}\nå†…å®¹: {r['snippet']}"
        for i, r in enumerate(search_results)
    ])
    
    # Step 4: AIã§äº‹å®Ÿç¢ºèª
    if not groq_client:
        logger.error("âŒ Groq client unavailable for verification")
        return {
            'verified': False,
            'correction_made': False,
            'search_result': combined_results[:500],
            'message': 'AIæ¤œè¨¼æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚'
        }
    
    try:
        verification_prompt = f"""ã‚ãªãŸã¯äº‹å®Ÿç¢ºèªã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±ã‚’æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚

ã€å¯¾è±¡ãƒ¡ãƒ³ãƒãƒ¼ã€‘{member_name}
ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¸»å¼µã€‘{user_claim}
ã€æ¤œç´¢çµæœã€‘
{combined_results[:2000]}

ã€ã‚¿ã‚¹ã‚¯ã€‘
1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¸»å¼µãŒæ¤œç´¢çµæœã‹ã‚‰**äº‹å®Ÿã¨ã—ã¦ç¢ºèªã§ãã‚‹ã‹**åˆ¤å®š
2. ç¢ºèªã§ããŸå ´åˆã€æ­£ç¢ºãªæƒ…å ±ã‚’æŠ½å‡º

**é‡è¦**: ä»¥ä¸‹ã®JSONå½¢å¼ã§ã®ã¿å›ç­”ã—ã¦ãã ã•ã„:
{{
  "verified": true/false,
  "confidence": 0-100,
  "extracted_info": {{
    "graduation_date": "YYYYå¹´MMæœˆDDæ—¥" or null,
    "graduation_reason": "ç†ç”±" or null,
    "debut_date": "YYYYå¹´MMæœˆDDæ—¥" or null,
    "generation": "NæœŸç”Ÿ" or null,
    "description": "èª¬æ˜æ–‡" or null
  }},
  "reasoning": "åˆ¤å®šç†ç”±ï¼ˆ50æ–‡å­—ä»¥å†…ï¼‰"
}}

**åˆ¤å®šåŸºæº–**:
- è¤‡æ•°ã®ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã§ä¸€è‡´ã—ã¦ã„ã‚Œã°true
- æ›–æ˜§ãƒ»çŸ›ç›¾ãƒ»æƒ…å ±ä¸è¶³ãªã‚‰false
- confidence: ç¢ºä¿¡åº¦ï¼ˆ80ä»¥ä¸Šã§ä¿®æ­£å®Ÿè¡Œï¼‰"""

        completion = groq_client.chat.completions.create(
            messages=[{"role": "user", "content": verification_prompt}],
            model="llama-3.1-8b-instant",
            temperature=0.2,  # äº‹å®Ÿç¢ºèªã¯ä½æ¸©åº¦
            max_tokens=400
        )
        
        response_text = completion.choices[0].message.content.strip()
        
        # JSONã‚’æŠ½å‡º
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
        if json_match:
            response_text = json_match.group(1)
        
        verification_result = json.loads(response_text)
        
        logger.info(f"âœ… Verification result: {verification_result}")
        
        # Step 5: ä¿¡é ¼åº¦ãŒé«˜ã‘ã‚Œã°DBä¿®æ­£
        if verification_result['verified'] and verification_result['confidence'] >= 80:
            session = Session()
            try:
                member = session.query(HolomemWiki).filter_by(member_name=member_name).first()
                
                if not member:
                    logger.warning(f"âš ï¸ Member not found in DB: {member_name}")
                    return {
                        'verified': True,
                        'correction_made': False,
                        'search_result': combined_results[:500],
                        'message': f"æƒ…å ±ã¯ç¢ºèªã§ããŸã‘ã©ã€{member_name}ã¡ã‚ƒã‚“ãŒDBã«ç™»éŒ²ã•ã‚Œã¦ãªã„ã¿ãŸã„â€¦"
                    }
                
                # DBæ›´æ–°
                extracted = verification_result['extracted_info']
                updated_fields = []
                
                if extracted.get('graduation_date'):
                    member.graduation_date = extracted['graduation_date']
                    member.is_active = False
                    updated_fields.append('å’æ¥­æ—¥')
                
                if extracted.get('graduation_reason'):
                    member.graduation_reason = extracted['graduation_reason']
                    updated_fields.append('å’æ¥­ç†ç”±')
                
                if extracted.get('debut_date'):
                    member.debut_date = extracted['debut_date']
                    updated_fields.append('ãƒ‡ãƒ“ãƒ¥ãƒ¼æ—¥')
                
                if extracted.get('generation'):
                    member.generation = extracted['generation']
                    updated_fields.append('æœŸç”Ÿ')
                
                if extracted.get('description'):
                    member.description = extracted['description']
                    updated_fields.append('ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«')
                
                member.last_updated = datetime.utcnow()
                
                session.commit()
                
                logger.info(f"âœ… DB corrected for {member_name}: {', '.join(updated_fields)}")
                
                return {
                    'verified': True,
                    'correction_made': True,
                    'search_result': combined_results[:500],
                    'updated_fields': updated_fields,
                    'updated_info': extracted,
                    'message': f"èª¿ã¹ã¦ã¿ãŸã‚‰æœ¬å½“ã ã£ãŸï¼{member_name}ã¡ã‚ƒã‚“ã®æƒ…å ±ã‚’ä¿®æ­£ã—ãŸã‚ˆï¼æ•™ãˆã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ï¼âœ¨"
                }
                
            except Exception as e:
                logger.error(f"âŒ DB update error: {e}", exc_info=True)
                session.rollback()
                return {
                    'verified': True,
                    'correction_made': False,
                    'search_result': combined_results[:500],
                    'message': f"æƒ…å ±ã¯æ­£ã—ã‹ã£ãŸã‚“ã ã‘ã©ã€DBæ›´æ–°ã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¡ã‚ƒã£ãŸâ€¦ã”ã‚ã‚“ï¼"
                }
            finally:
                session.close()
        
        else:
            # æ¤œè¨¼å¤±æ•—
            logger.info(f"âš ï¸ Verification failed or low confidence: {verification_result['confidence']}%")
            return {
                'verified': False,
                'correction_made': False,
                'search_result': combined_results[:500],
                'confidence': verification_result['confidence'],
                'reasoning': verification_result['reasoning'],
                'message': f"ã†ãƒ¼ã‚“ã€èª¿ã¹ã¦ã¿ãŸã‚“ã ã‘ã©ç¢ºè¨¼ãŒæŒã¦ãªã‹ã£ãŸâ€¦ï¼ˆç¢ºä¿¡åº¦{verification_result['confidence']}%ï¼‰ã‚‚ã†ã¡ã‚‡ã£ã¨è©³ã—ã„æƒ…å ±ã‚ã£ãŸã‚‰æ•™ãˆã¦ï¼Ÿ"
            }
        
    except json.JSONDecodeError as e:
        logger.error(f"âŒ JSON parse error in verification: {e}")
        logger.error(f"Raw response: {response_text[:500]}")
        return {
            'verified': False,
            'correction_made': False,
            'search_result': combined_results[:500],
            'message': 'AIæ¤œè¨¼ã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¡ã‚ƒã£ãŸâ€¦'
        }
    except Exception as e:
        logger.error(f"âŒ Verification error: {e}", exc_info=True)
        return {
            'verified': False,
            'correction_made': False,
            'search_result': combined_results[:500],
            'message': f"æ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¡ã‚ƒã£ãŸâ€¦ã”ã‚ã‚“ã­ï¼"
        }


# ========================================
# ã€è¿½åŠ 3ã€‘ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§DBä¿®æ­£ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°
# ========================================

def background_db_correction(task_id, correction_request):
    """
    ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§DBä¿®æ­£ã‚’å®Ÿè¡Œ
    """
    session = Session()
    
    try:
        logger.info(f"ğŸ”§ Starting DB correction (Task ID: {task_id})")
        
        # æ¤œè¨¼ + ä¿®æ­£å®Ÿè¡Œ
        result = verify_and_correct_holomem_info(correction_request)
        
        # ã‚¿ã‚¹ã‚¯çµæœã‚’ä¿å­˜
        task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
        if task:
            task.result = result['message']
            task.status = 'completed'
            task.completed_at = datetime.utcnow()
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜
            task.query = json.dumps({
                'correction_type': correction_request['correction_type'],
                'member_name': correction_request['member_name'],
                'verified': result['verified'],
                'corrected': result['correction_made']
            }, ensure_ascii=False)
            
            session.commit()
            logger.info(f"âœ… DB correction task completed: {task_id}")
    
    except Exception as e:
        logger.error(f"âŒ Background DB correction error: {e}", exc_info=True)
        
        # ã‚¨ãƒ©ãƒ¼ã‚’ã‚¿ã‚¹ã‚¯ã«è¨˜éŒ²
        task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
        if task:
            task.result = "DBä¿®æ­£ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
            task.status = 'completed'
            task.completed_at = datetime.utcnow()
            session.commit()
    
    finally:
        session.close()


def start_background_correction(user_uuid, correction_request):
    """DBä¿®æ­£ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹"""
    task_id = str(uuid.uuid4())[:8]
    session = Session()
    
    try:
        task = BackgroundTask(
            task_id=task_id,
            user_uuid=user_uuid,
            task_type='db_correction',
            query=correction_request['original_message']
        )
        session.add(task)
        session.commit()
        
        background_executor.submit(background_db_correction, task_id, correction_request)
        
        logger.info(f"ğŸš€ DB correction task started: {task_id}")
        return task_id
        
    except Exception as e:
        logger.error(f"âŒ Background correction task creation error: {e}")
        session.rollback()
        return None
    finally:
        session.close()


# ========================================
# ã€è¿½åŠ 4ã€‘chat_lsl ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«DBä¿®æ­£æ¤œå‡ºã‚’è¿½åŠ 
# ========================================

@app.route('/chat_lsl', methods=['POST'])
def chat_lsl():
    session = Session()
    try:
        data = request.json
        user_uuid, user_name, message = data.get('uuid', ''), data.get('name', ''), data.get('message', '')
        
        if not all([user_uuid, user_name, message]):
            return "ã‚¨ãƒ©ãƒ¼: å¿…è¦ãªæƒ…å ±ãŒè¶³ã‚Šãªã„ã¿ãŸã„â€¦|", 400
        
        logger.info(f"ğŸ’¬ Received: {message} (from: {user_name})")
        user_data = get_or_create_user(session, user_uuid, user_name)
        history = get_conversation_history(session, user_uuid)
        ai_text = ""
        
        # â˜… è¿½åŠ : DBä¿®æ­£ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æ¤œå‡ºï¼ˆæœ€å„ªå…ˆï¼‰
        correction_request = detect_db_correction_request(message)
        
        if correction_request:
            logger.info(f"ğŸ”§ DB correction request detected: {correction_request}")
            
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§æ¤œè¨¼ + ä¿®æ­£ã‚’é–‹å§‹
            if start_background_correction(user_uuid, correction_request):
                ai_text = f"ãˆã€ã¾ã˜ã§ï¼ï¼Ÿ{correction_request['member_name']}ã¡ã‚ƒã‚“ã®æƒ…å ±ã€ä»Šã™ãèª¿ã¹ã¦ç¢ºèªã—ã¦ã¿ã‚‹ã­ï¼ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã¦ï¼"
            else:
                ai_text = "ã”ã‚ã‚“ã€ä»ŠDBä¿®æ­£æ©Ÿèƒ½ãŒã†ã¾ãå‹•ã„ã¦ãªã„ã¿ãŸã„â€¦"
            
            # ä¼šè©±å±¥æ­´ã«ä¿å­˜
            session.add(ConversationHistory(user_uuid=user_uuid, role='user', content=message))
            session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=ai_text))
            session.commit()
            
            return f"{ai_text}|", 200
        
        # ä»¥ä¸‹ã€æ—¢å­˜ã®å‡¦ç†ï¼ˆå¤‰æ›´ãªã—ï¼‰
        
        # â˜… è¿½åŠ : è¿½åŠ è³ªå•ã®åˆ¤å®š
        is_follow_up = is_follow_up_question(message, history)
        
        # === å„ªå…ˆåº¦1: ãƒ›ãƒ­ãƒ¡ãƒ³åŸºæœ¬æƒ…å ± ===
        holomem_keywords = get_active_holomem_keywords()
        basic_question_pattern = f"({'|'.join(re.escape(k) for k in holomem_keywords)})ã£ã¦(?:èª°|ã ã‚Œ|ä½•|ãªã«)[\?ï¼Ÿ]?$"
        basic_question_match = re.search(basic_question_pattern, message.strip())
        
        if not ai_text and basic_question_match:
            member_name = basic_question_match.group(1)
            
            if member_name in ['ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–', 'hololive', 'ãƒ›ãƒ­ãƒ¡ãƒ³']:
                ai_text = "ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã¯ã€ã‚«ãƒãƒ¼æ ªå¼ä¼šç¤¾ãŒé‹å–¶ã—ã¦ã‚‹VTuberäº‹å‹™æ‰€ã®ã“ã¨ã ã‚ˆï¼ã¨ãã®ãã‚‰ã¡ã‚ƒã‚“ã¨ã‹ã€ãŸãã•ã‚“ã®äººæ°—VTuberãŒæ‰€å±ã—ã¦ã¦ã€é…ä¿¡ã¨ã‹ã¾ã˜ã§æ¥½ã—ã„ã‹ã‚‰ãŠã™ã™ã‚ï¼"
            else:
                wiki_info = get_holomem_info(member_name)
                if wiki_info:
                    response_parts = [f"{wiki_info['name']}ã¡ã‚ƒã‚“ã¯ã­ã€ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–{wiki_info['generation']}ã®VTuberã ã‚ˆï¼ {wiki_info['description']}"]
                    if wiki_info.get('graduation_date'):
                        response_parts.append(f"ã§ã‚‚ã­ã€{wiki_info['graduation_date']}ã«å’æ¥­ã—ã¡ã‚ƒã£ãŸã‚“ã â€¦ã€‚{wiki_info.get('mochiko_feeling', 'ã¾ã˜å¯‚ã—ã„ã‚ˆã­â€¦ã€‚')}")
                    ai_text = " ".join(response_parts)
        
        # === å„ªå…ˆåº¦2: ã•ãã‚‰ã¿ã“ç‰¹åˆ¥å¿œç­” ===
        elif not ai_text and ('ã•ãã‚‰ã¿ã“' in message or 'ã¿ã“ã¡' in message):
            special_responses = get_sakuramiko_special_responses()
            for keyword, response in special_responses.items():
                if keyword in message:
                    ai_text = response
                    break
        
        # === å„ªå…ˆåº¦3: ãƒ‹ãƒ¥ãƒ¼ã‚¹è©³ç´° ===
        if not ai_text and (news_number := is_news_detail_request(message)):
            news_detail = get_cached_news_detail(session, user_uuid, news_number)
            if news_detail:
                ai_text = generate_ai_response(
                    user_data, 
                    f"ã€Œ{news_detail.title}ã€ã«ã¤ã„ã¦ã ã­ï¼", 
                    history, 
                    f"ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è©³ç´°æƒ…å ±:\n{news_detail.content}", 
                    is_detailed=True
                )
        
        # === å„ªå…ˆåº¦4: æ™‚é–“ãƒ»å¤©æ°— ===
        elif not ai_text and (is_time_request(message) or is_weather_request(message)):
            responses = []
            if is_time_request(message): 
                responses.append(get_japan_time())
            if is_weather_request(message): 
                responses.append(get_weather_forecast(extract_location(message)))
            ai_text = " ".join(responses)
        
        # === å„ªå…ˆåº¦5: ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ãƒ‹ãƒ¥ãƒ¼ã‚¹ ===
        elif not ai_text and is_hololive_request(message) and any(kw in message for kw in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'æƒ…å ±', 'ãŠçŸ¥ã‚‰ã›']):
            all_news = session.query(HololiveNews).order_by(HololiveNews.created_at.desc()).limit(10).all()
            if all_news:
                selected_news = random.sample(all_news, min(random.randint(3, 5), len(all_news)))
                save_news_cache(session, user_uuid, selected_news, 'hololive')
                
                news_items_text = []
                for i, n in enumerate(selected_news, 1):
                    short_title = n.title[:50] + "..." if len(n.title) > 50 else n.title
                    news_items_text.append(f"ã€{i}ã€‘{short_title}")

                news_text = f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€{len(selected_news)}ä»¶ç´¹ä»‹ã™ã‚‹ã­ï¼\n" + "\n".join(news_items_text) + "\n\næ°—ã«ãªã‚‹ã®ã‚ã£ãŸï¼Ÿç•ªå·ã§æ•™ãˆã¦ï¼"
                ai_text = limit_text_for_sl(news_text, 250)
            else:
                ai_text = "ã”ã‚ã‚“ã€ä»Šãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã¾ã å–å¾—ã§ãã¦ãªã„ã¿ãŸã„â€¦"
        
        # === å„ªå…ˆåº¦6: è¿½åŠ è³ªå• ===
        elif not ai_text and is_follow_up:
            logger.info("ğŸ” Processing follow-up question")
            last_assistant_msg = None
            for h in history:
                if h.role == 'assistant':
                    last_assistant_msg = h.content
                    break
            
            if last_assistant_msg:
                ai_text = generate_ai_response(
                    user_data,
                    message,
                    history,
                    f"ç›´å‰ã®å›ç­”å†…å®¹:\n{last_assistant_msg}",
                    is_detailed=True
                )
            else:
                ai_text = generate_ai_response(user_data, message, history)
        
        # === å„ªå…ˆåº¦7: æ˜ç¤ºçš„ãªæ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆ ===
        elif not ai_text and is_explicit_search_request(message):
            if start_background_search(user_uuid, message, is_detailed_request(message)):
                ai_text = "ãŠã£ã‘ãƒ¼ã€èª¿ã¹ã¦ã¿ã‚‹ã­ï¼ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã¦ï¼"
            else:
                ai_text = "ã”ã‚ã‚“ã€ä»Šæ¤œç´¢æ©Ÿèƒ½ãŒã†ã¾ãå‹•ã„ã¦ãªã„ã¿ãŸã„â€¦"

        # === å„ªå…ˆåº¦8: æ„Ÿæƒ…ãƒ»å­£ç¯€ãƒ»é¢ç™½ã„è©± ===
        elif not ai_text and (is_emotional_expression(message) or is_seasonal_topic(message) or is_story_request(message)):
             ai_text = generate_ai_response(user_data, message, history)
        
        # === å„ªå…ˆåº¦9: æš—é»™çš„ãªæ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆ ===
        elif not ai_text and not is_short_response(message) and should_search(message):
            if start_background_search(user_uuid, message, is_detailed_request(message)):
                ai_text = "ãŠã£ã‘ãƒ¼ã€èª¿ã¹ã¦ã¿ã‚‹ã­ï¼çµæœãŒå‡ºã‚‹ã¾ã§ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã¦ï¼"
            else:
                ai_text = "ã”ã‚ã‚“ã€ä»Šæ¤œç´¢æ©Ÿèƒ½ãŒã†ã¾ãå‹•ã„ã¦ãªã„ã¿ãŸã„â€¦"
        
        # === å„ªå…ˆåº¦10: é€šå¸¸ä¼šè©± ===
        elif not ai_text:
            ai_text = generate_ai_response(user_data, message, history)
        
        # â˜… æœ€çµ‚çš„ãªæ–‡å­—æ•°åˆ¶é™
        ai_text = limit_text_for_sl(ai_text, SL_SAFE_CHAR_LIMIT)
        
        # ä¼šè©±å±¥æ­´ã«ä¿å­˜
        session.add(ConversationHistory(user_uuid=user_uuid, role='user', content=message))
        session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=ai_text))
        session.commit()
        
        logger.info(f"âœ… Responded: {ai_text[:80]}")
        return f"{ai_text}|", 200
        
    except Exception as e:
        logger.error(f"âŒ Unhandled error in chat endpoint: {e}", exc_info=True)
        return "ã”ã‚ã‚“ã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¡ã‚ƒã£ãŸâ€¦|", 500
    finally:
        if session:
            session.close()


# ========================================
# ã€è¿½åŠ 5ã€‘DBä¿®æ­£å±¥æ­´ã‚’ç¢ºèªã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ========================================

@app.route('/correction_history', methods=['GET'])
def get_correction_history():
    """DBä¿®æ­£å±¥æ­´ã‚’å–å¾—ï¼ˆç®¡ç†è€…ç”¨ï¼‰"""
    session = Session()
    try:
        corrections = session.query(BackgroundTask).filter_by(
            task_type='db_correction'
        ).order_by(BackgroundTask.completed_at.desc()).limit(50).all()
        
        history = []
        for task in corrections:
            if task.query:
                try:
                    query_data = json.loads(task.query)
                    history.append({
                        'task_id': task.task_id,
                        'member_name': query_data.get('member_name'),
                        'correction_type': query_data.get('correction_type'),
                        'verified': query_data.get('verified'),
                        'corrected': query_data.get('corrected'),
                        'completed_at': task.completed_at.isoformat() if task.completed_at else None
                    })
                except:
                    pass
        
        return Response(
            json.dumps({'corrections': history}, ensure_ascii=False, indent=2),
            status=200,
            mimetype='application/json; charset=utf-8'
        )
        
    except Exception as e:
        logger.error(f"âŒ Correction history error: {e}")
        return Response(
            json.dumps({'error': str(e)}, ensure_ascii=False),
            status=500,
            mimetype='application/json; charset=utf-8'
        )
    finally:
        session.close()

@app.route('/test_voicevox', methods=['GET'])
def test_voicevox():
    """VOICEVOXæ¥ç¶šãƒ†ã‚¹ãƒˆ"""
    voicevox_url = VOICEVOX_URL_FROM_ENV or "http://localhost:50021"
    
    result = {
        'voicevox_url': voicevox_url,
        'voicevox_enabled': VOICEVOX_ENABLED,
        'voice_directory': {
            'path': VOICE_DIR,
            'exists': os.path.exists(VOICE_DIR),
            'writable': os.access(VOICE_DIR, os.W_OK) if os.path.exists(VOICE_DIR) else False
        },
        'tests': {}
    }
    
    # Test 1: VOICEVOXãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
    try:
        response = requests.get(f"{voicevox_url}/version", timeout=5)
        if response.ok:
            result['tests']['version'] = {
                'status': 'ok',
                'data': response.json()
            }
        else:
            result['tests']['version'] = {
                'status': 'error',
                'http_code': response.status_code
            }
    except Exception as e:
        result['tests']['version'] = {
            'status': 'error',
            'message': str(e)
        }
    
    # Test 2: ç°¡æ˜“éŸ³å£°ç”Ÿæˆãƒ†ã‚¹ãƒˆ
    try:
        test_response = requests.post(
            f"{voicevox_url}/audio_query",
            params={"text": "ãƒ†ã‚¹ãƒˆ", "speaker": VOICEVOX_SPEAKER_ID},
            timeout=10
        )
        if test_response.ok:
            result['tests']['audio_query'] = {
                'status': 'ok',
                'message': 'éŸ³å£°ã‚¯ã‚¨ãƒªç”ŸæˆãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™'
            }
        else:
            result['tests']['audio_query'] = {
                'status': 'error',
                'http_code': test_response.status_code
            }
    except Exception as e:
        result['tests']['audio_query'] = {
            'status': 'error',
            'message': str(e)
        }
    
    # ç·åˆåˆ¤å®š
    all_ok = all(
        test.get('status') == 'ok' 
        for test in result['tests'].values()
    ) and result['voice_directory']['exists'] and result['voice_directory']['writable']
    
    result['overall_status'] = 'ok' if all_ok else 'error'
    
    if not all_ok:
        result['recommendations'] = []
        if not result['voice_directory']['exists']:
            result['recommendations'].append('éŸ³å£°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“ - ã‚µãƒ¼ãƒãƒ¼ã‚’å†èµ·å‹•ã—ã¦ãã ã•ã„')
        if not result['voice_directory']['writable']:
            result['recommendations'].append('éŸ³å£°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æ›¸ãè¾¼ã¿æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“')
        if result['tests'].get('version', {}).get('status') != 'ok':
            result['recommendations'].append('VOICEVOXã‚¨ãƒ³ã‚¸ãƒ³ã«æ¥ç¶šã§ãã¾ã›ã‚“')
    
    return jsonify(result), 200 if all_ok else 500
    
# --- Flaskã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ---
@app.route('/health', methods=['GET'])
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - Renderã®èµ·å‹•ç¢ºèªç”¨"""
    try:
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª
        with engine.connect() as conn: 
            conn.execute(text("SELECT 1"))
        db_status = 'ok'
    except Exception as e:
        logger.error(f"Health check DB error: {e}")
        db_status = 'error'
    
    health_data = {
        'status': 'ok',
        'timestamp': datetime.utcnow().isoformat(),
        'services': {
            'database': db_status, 
            'groq_ai': 'ok' if groq_client else 'disabled'
        }
    }
    
    logger.info(f"Health check: {health_data}")
    return jsonify(health_data), 200

# ========================================
# ã€ä¿®æ­£1ã€‘check_task ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - æ–‡å­—åŒ–ã‘å¯¾ç­–
# ========================================

@app.route('/check_task', methods=['POST'])
def check_task():
    """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã®å®Œäº†ãƒã‚§ãƒƒã‚¯ï¼ˆUTF-8æ–‡å­—åŒ–ã‘å¯¾ç­–ç‰ˆï¼‰"""
    try:
        data = request.json
        if not data:
            logger.error("âŒ check_task: Empty request body")
            return Response(
                json.dumps({'status': 'error', 'message': 'ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ãŒç©ºã§ã™'}, ensure_ascii=False),
                status=400,
                mimetype='application/json; charset=utf-8'
            )
        
        user_uuid = data.get('uuid', '')
        
        if not user_uuid:
            logger.error("âŒ check_task: UUID missing")
            return Response(
                json.dumps({'status': 'error', 'message': 'UUID required'}, ensure_ascii=False),
                status=400,
                mimetype='application/json; charset=utf-8'
            )
        
        logger.info(f"ğŸ” Checking tasks for user: {user_uuid}")
        
        # å®Œäº†ã‚¿ã‚¹ã‚¯ã‚’ç¢ºèª
        completed_task = check_completed_tasks(user_uuid)
        
        if completed_task:
            logger.info(f"âœ… Task completed for {user_uuid}: {completed_task['query']}")
            
            # AIã«ã‚ˆã‚‹å ±å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
            session = Session()
            try:
                user_data = session.query(UserMemory).filter_by(user_uuid=user_uuid).first()
                if not user_data:
                    user_name = "ã‚ãªãŸ"
                    user_uuid_for_response = user_uuid
                else:
                    user_name = user_data.user_name
                    user_uuid_for_response = user_data.user_uuid
                
                # å±¥æ­´ã‚’å–å¾—
                history = get_conversation_history(session, user_uuid)
                
                # å ±å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ
                report_message = generate_ai_response(
                    {'name': user_name, 'uuid': user_uuid_for_response},
                    f"ï¼ˆæ¤œç´¢å®Œäº†å ±å‘Šï¼‰ä»¥å‰ãƒªã‚¯ã‚¨ã‚¹ãƒˆã•ã‚ŒãŸã€Œ{completed_task['query']}ã€ã®çµæœã‚’å ±å‘Šã—ã¦ãã ã•ã„ã€‚",
                    history,
                    completed_task['result'],
                    is_detailed=True,
                    is_task_report=True
                )
                
                # â˜… ä¿®æ­£: æ–‡å­—æ•°åˆ¶é™ã‚’é©ç”¨
                report_message = limit_text_for_sl(report_message, SL_SAFE_CHAR_LIMIT)
                
                # ä¼šè©±å±¥æ­´ã«ä¿å­˜
                session.add(ConversationHistory(
                    user_uuid=user_uuid,
                    role='assistant',
                    content=report_message
                ))
                session.commit()
                
                # â˜… ä¿®æ­£: ensure_ascii=False + UTF-8æŒ‡å®š
                return Response(
                    json.dumps({
                        'status': 'completed',
                        'query': completed_task['query'],
                        'message': report_message
                    }, ensure_ascii=False, indent=2),
                    status=200,
                    mimetype='application/json; charset=utf-8'
                )
                
            except Exception as e:
                logger.error(f"âŒ Report generation error: {e}", exc_info=True)
                session.rollback()
                return Response(
                    json.dumps({
                        'status': 'error',
                        'message': 'ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼'
                    }, ensure_ascii=False),
                    status=500,
                    mimetype='application/json; charset=utf-8'
                )
            finally:
                session.close()
        
        # ã‚¿ã‚¹ã‚¯ãŒã¾ã å®Œäº†ã—ã¦ã„ãªã„å ´åˆ
        logger.info(f"â³ Task pending for {user_uuid}")
        return Response(
            json.dumps({'status': 'pending'}, ensure_ascii=False),
            status=200,
            mimetype='application/json; charset=utf-8'
        )
        
    except Exception as e:
        logger.error(f"âŒ check_task critical error: {e}", exc_info=True)
        return Response(
            json.dumps({'status': 'error', 'message': 'ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼'}, ensure_ascii=False),
            status=500,
            mimetype='application/json; charset=utf-8'
        )

# â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ãŒå¤‰æ›´ç®‡æ‰€ã§ã™ã€‘â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
# ========================================
# ã€è¿½åŠ 4ã€‘chat_lsl ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«DBä¿®æ­£æ¤œå‡ºã‚’è¿½åŠ 
# ========================================

@app.route('/chat_lsl', methods=['POST'])
def chat_lsl():
    session = Session()
    try:
        data = request.json
        user_uuid, user_name, message = data.get('uuid', ''), data.get('name', ''), data.get('message', '')
        
        if not all([user_uuid, user_name, message]):
            return "ã‚¨ãƒ©ãƒ¼: å¿…è¦ãªæƒ…å ±ãŒè¶³ã‚Šãªã„ã¿ãŸã„â€¦|", 400
        
        logger.info(f"ğŸ’¬ Received: {message} (from: {user_name})")
        user_data = get_or_create_user(session, user_uuid, user_name)
        history = get_conversation_history(session, user_uuid)
        ai_text = ""
        
        # â˜… è¿½åŠ : DBä¿®æ­£ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æ¤œå‡ºï¼ˆæœ€å„ªå…ˆï¼‰
        correction_request = detect_db_correction_request(message)
        
        if correction_request:
            logger.info(f"ğŸ”§ DB correction request detected: {correction_request}")
            
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§æ¤œè¨¼ + ä¿®æ­£ã‚’é–‹å§‹
            if start_background_correction(user_uuid, correction_request):
                ai_text = f"ãˆã€ã¾ã˜ã§ï¼ï¼Ÿ{correction_request['member_name']}ã¡ã‚ƒã‚“ã®æƒ…å ±ã€ä»Šã™ãèª¿ã¹ã¦ç¢ºèªã—ã¦ã¿ã‚‹ã­ï¼ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã¦ï¼"
            else:
                ai_text = "ã”ã‚ã‚“ã€ä»ŠDBä¿®æ­£æ©Ÿèƒ½ãŒã†ã¾ãå‹•ã„ã¦ãªã„ã¿ãŸã„â€¦"
            
            # ä¼šè©±å±¥æ­´ã«ä¿å­˜
            session.add(ConversationHistory(user_uuid=user_uuid, role='user', content=message))
            session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=ai_text))
            session.commit()
            
            return f"{ai_text}|", 200
        
        # ä»¥ä¸‹ã€æ—¢å­˜ã®å‡¦ç†ï¼ˆå¤‰æ›´ãªã—ï¼‰
        
        # â˜… è¿½åŠ : è¿½åŠ è³ªå•ã®åˆ¤å®š
        is_follow_up = is_follow_up_question(message, history)
        
        # === å„ªå…ˆåº¦1: ãƒ›ãƒ­ãƒ¡ãƒ³åŸºæœ¬æƒ…å ± ===
        holomem_keywords = get_active_holomem_keywords()
        basic_question_pattern = f"({'|'.join(re.escape(k) for k in holomem_keywords)})ã£ã¦(?:èª°|ã ã‚Œ|ä½•|ãªã«)[\?ï¼Ÿ]?$"
        basic_question_match = re.search(basic_question_pattern, message.strip())
        
        if not ai_text and basic_question_match:
            member_name = basic_question_match.group(1)
            
            if member_name in ['ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–', 'hololive', 'ãƒ›ãƒ­ãƒ¡ãƒ³']:
                ai_text = "ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã¯ã€ã‚«ãƒãƒ¼æ ªå¼ä¼šç¤¾ãŒé‹å–¶ã—ã¦ã‚‹VTuberäº‹å‹™æ‰€ã®ã“ã¨ã ã‚ˆï¼ã¨ãã®ãã‚‰ã¡ã‚ƒã‚“ã¨ã‹ã€ãŸãã•ã‚“ã®äººæ°—VTuberãŒæ‰€å±ã—ã¦ã¦ã€é…ä¿¡ã¨ã‹ã¾ã˜ã§æ¥½ã—ã„ã‹ã‚‰ãŠã™ã™ã‚ï¼"
            else:
                wiki_info = get_holomem_info(member_name)
                if wiki_info:
                    response_parts = [f"{wiki_info['name']}ã¡ã‚ƒã‚“ã¯ã­ã€ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–{wiki_info['generation']}ã®VTuberã ã‚ˆï¼ {wiki_info['description']}"]
                    if wiki_info.get('graduation_date'):
                        response_parts.append(f"ã§ã‚‚ã­ã€{wiki_info['graduation_date']}ã«å’æ¥­ã—ã¡ã‚ƒã£ãŸã‚“ã â€¦ã€‚{wiki_info.get('mochiko_feeling', 'ã¾ã˜å¯‚ã—ã„ã‚ˆã­â€¦ã€‚')}")
                    ai_text = " ".join(response_parts)
        
        # === å„ªå…ˆåº¦2: ã•ãã‚‰ã¿ã“ç‰¹åˆ¥å¿œç­” ===
        elif not ai_text and ('ã•ãã‚‰ã¿ã“' in message or 'ã¿ã“ã¡' in message):
            special_responses = get_sakuramiko_special_responses()
            for keyword, response in special_responses.items():
                if keyword in message:
                    ai_text = response
                    break
        
        # === å„ªå…ˆåº¦3: ãƒ‹ãƒ¥ãƒ¼ã‚¹è©³ç´° ===
        if not ai_text and (news_number := is_news_detail_request(message)):
            news_detail = get_cached_news_detail(session, user_uuid, news_number)
            if news_detail:
                ai_text = generate_ai_response(
                    user_data, 
                    f"ã€Œ{news_detail.title}ã€ã«ã¤ã„ã¦ã ã­ï¼", 
                    history, 
                    f"ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è©³ç´°æƒ…å ±:\n{news_detail.content}", 
                    is_detailed=True
                )
        
        # === å„ªå…ˆåº¦4: æ™‚é–“ãƒ»å¤©æ°— ===
        elif not ai_text and (is_time_request(message) or is_weather_request(message)):
            responses = []
            if is_time_request(message): 
                responses.append(get_japan_time())
            if is_weather_request(message): 
                responses.append(get_weather_forecast(extract_location(message)))
            ai_text = " ".join(responses)
        
        # === å„ªå…ˆåº¦5: ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ãƒ‹ãƒ¥ãƒ¼ã‚¹ ===
        elif not ai_text and is_hololive_request(message) and any(kw in message for kw in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'æƒ…å ±', 'ãŠçŸ¥ã‚‰ã›']):
            all_news = session.query(HololiveNews).order_by(HololiveNews.created_at.desc()).limit(10).all()
            if all_news:
                selected_news = random.sample(all_news, min(random.randint(3, 5), len(all_news)))
                save_news_cache(session, user_uuid, selected_news, 'hololive')
                
                news_items_text = []
                for i, n in enumerate(selected_news, 1):
                    short_title = n.title[:50] + "..." if len(n.title) > 50 else n.title
                    news_items_text.append(f"ã€{i}ã€‘{short_title}")

                news_text = f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€{len(selected_news)}ä»¶ç´¹ä»‹ã™ã‚‹ã­ï¼\n" + "\n".join(news_items_text) + "\n\næ°—ã«ãªã‚‹ã®ã‚ã£ãŸï¼Ÿç•ªå·ã§æ•™ãˆã¦ï¼"
                ai_text = limit_text_for_sl(news_text, 250)
            else:
                ai_text = "ã”ã‚ã‚“ã€ä»Šãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã¾ã å–å¾—ã§ãã¦ãªã„ã¿ãŸã„â€¦"
        
        # === å„ªå…ˆåº¦6: è¿½åŠ è³ªå• ===
        elif not ai_text and is_follow_up:
            logger.info("ğŸ” Processing follow-up question")
            last_assistant_msg = None
            for h in history:
                if h.role == 'assistant':
                    last_assistant_msg = h.content
                    break
            
            if last_assistant_msg:
                ai_text = generate_ai_response(
                    user_data,
                    message,
                    history,
                    f"ç›´å‰ã®å›ç­”å†…å®¹:\n{last_assistant_msg}",
                    is_detailed=True
                )
            else:
                ai_text = generate_ai_response(user_data, message, history)
        
        # === å„ªå…ˆåº¦7: æ˜ç¤ºçš„ãªæ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆ ===
        elif not ai_text and is_explicit_search_request(message):
            if start_background_search(user_uuid, message, is_detailed_request(message)):
                ai_text = "ãŠã£ã‘ãƒ¼ã€èª¿ã¹ã¦ã¿ã‚‹ã­ï¼ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã¦ï¼"
            else:
                ai_text = "ã”ã‚ã‚“ã€ä»Šæ¤œç´¢æ©Ÿèƒ½ãŒã†ã¾ãå‹•ã„ã¦ãªã„ã¿ãŸã„â€¦"

        # === å„ªå…ˆåº¦8: æ„Ÿæƒ…ãƒ»å­£ç¯€ãƒ»é¢ç™½ã„è©± ===
        elif not ai_text and (is_emotional_expression(message) or is_seasonal_topic(message) or is_story_request(message)):
             ai_text = generate_ai_response(user_data, message, history)
        
        # === å„ªå…ˆåº¦9: æš—é»™çš„ãªæ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆ ===
        elif not ai_text and not is_short_response(message) and should_search(message):
            if start_background_search(user_uuid, message, is_detailed_request(message)):
                ai_text = "ãŠã£ã‘ãƒ¼ã€èª¿ã¹ã¦ã¿ã‚‹ã­ï¼çµæœãŒå‡ºã‚‹ã¾ã§ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã¦ï¼"
            else:
                ai_text = "ã”ã‚ã‚“ã€ä»Šæ¤œç´¢æ©Ÿèƒ½ãŒã†ã¾ãå‹•ã„ã¦ãªã„ã¿ãŸã„â€¦"
        
        # === å„ªå…ˆåº¦10: é€šå¸¸ä¼šè©± ===
        elif not ai_text:
            ai_text = generate_ai_response(user_data, message, history)
        
        # â˜… æœ€çµ‚çš„ãªæ–‡å­—æ•°åˆ¶é™
        ai_text = limit_text_for_sl(ai_text, SL_SAFE_CHAR_LIMIT)
        
        # ä¼šè©±å±¥æ­´ã«ä¿å­˜
        session.add(ConversationHistory(user_uuid=user_uuid, role='user', content=message))
        session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=ai_text))
        session.commit()
        
        logger.info(f"âœ… Responded: {ai_text[:80]}")
        return f"{ai_text}|", 200
        
    except Exception as e:
        logger.error(f"âŒ Unhandled error in chat endpoint: {e}", exc_info=True)
        return "ã”ã‚ã‚“ã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¡ã‚ƒã£ãŸâ€¦|", 500
    finally:
        if session:
            session.close()

# â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²ã€ã“ã“ã¾ã§ãŒå¤‰æ›´ç®‡æ‰€ã§ã™ã€‘â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
# ===== ã€è¿½åŠ ã€‘å¿ƒç†åˆ†æã‚³ãƒãƒ³ãƒ‰ =====

@app.route('/analyze_psychology', methods=['POST'])
def analyze_psychology_endpoint():
    """å¿ƒç†åˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        data = request.json
        user_uuid = data.get('uuid')
        
        if not user_uuid:
            return jsonify({'error': 'UUID required'}), 400
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§åˆ†æå®Ÿè¡Œ
        background_executor.submit(analyze_user_psychology, user_uuid)
        
        return jsonify({
            'status': 'started',
            'message': 'å¿ƒç†åˆ†æã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚å®Œäº†ã¾ã§å°‘ã—ãŠå¾…ã¡ãã ã•ã„ã€‚'
        }), 200
        
    except Exception as e:
        logger.error(f"âŒ Psychology analysis endpoint error: {e}")
        return jsonify({'error': str(e)}), 500


# ========================================
# ã€ä¿®æ­£5ã€‘get_psychology ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - æ–‡å­—åŒ–ã‘å¯¾ç­–
# ========================================

@app.route('/get_psychology', methods=['POST'])
def get_psychology_endpoint():
    """å¿ƒç†åˆ†æçµæœã‚’å–å¾—ï¼ˆUTF-8å¯¾å¿œç‰ˆï¼‰"""
    try:
        data = request.json
        user_uuid = data.get('uuid')
        
        if not user_uuid:
            return Response(
                json.dumps({'error': 'UUID required'}, ensure_ascii=False),
                status=400,
                mimetype='application/json; charset=utf-8'
            )
        
        psychology = get_user_psychology(user_uuid)
        
        if not psychology:
            return Response(
                json.dumps({'error': 'No analysis data found'}, ensure_ascii=False),
                status=404,
                mimetype='application/json; charset=utf-8'
            )
        
        return Response(
            json.dumps(psychology, ensure_ascii=False, indent=2),
            status=200,
            mimetype='application/json; charset=utf-8'
        )
        
    except Exception as e:
        logger.error(f"âŒ Get psychology error: {e}")
        return Response(
            json.dumps({'error': str(e)}, ensure_ascii=False),
            status=500,
            mimetype='application/json; charset=utf-8'
        )


# ===== ã€è¿½åŠ ã€‘å®šæœŸçš„ãªå¿ƒç†åˆ†æã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« =====
# initialize_app() é–¢æ•°å†…ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®šã«è¿½åŠ 

def schedule_psychology_analysis():
    """å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†åˆ†æã‚’å®šæœŸå®Ÿè¡Œ"""
    session = Session()
    try:
        # æœ€è¿‘æ´»å‹•ã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å–å¾—
        active_users = session.query(UserMemory).filter(
            UserMemory.last_interaction > datetime.utcnow() - timedelta(days=7),
            UserMemory.interaction_count >= 10
        ).all()
        
        for user in active_users:
            # æœ€å¾Œã®åˆ†æã‹ã‚‰24æ™‚é–“ä»¥ä¸ŠçµŒéã—ã¦ã„ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã¿
            psychology = session.query(UserPsychology).filter_by(user_uuid=user.user_uuid).first()
            
            if not psychology or psychology.last_analyzed < datetime.utcnow() - timedelta(hours=24):
                logger.info(f"ğŸ§  Scheduling psychology analysis for user: {user.user_name}")
                background_executor.submit(analyze_user_psychology, user.user_uuid)
                time.sleep(5)  # è² è·åˆ†æ•£ã®ãŸã‚5ç§’å¾…æ©Ÿ
        
    except Exception as e:
        logger.error(f"âŒ Schedule psychology analysis error: {e}")
    finally:
        session.close()
        
@app.route('/generate_voice', methods=['POST'])
def voice_generation_endpoint():
    """éŸ³å£°ç”Ÿæˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - ä¿®æ­£ç‰ˆ"""
    try:
        # request.jsonãŒNoneã®å ´åˆã®å¯¾ç­–
        data = request.json
        if not data:
            logger.error("âŒ Empty request body")
            return jsonify({'error': 'ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ãŒç©ºã§ã™'}), 400
        
        # ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ï¼ˆå…ˆã«åˆ‡ã‚Šè©°ã‚ãªã„ï¼‰
        text = data.get('text', '').strip()
        
        if not text:
            logger.error("âŒ No text provided")
            return jsonify({'error': 'ãƒ†ã‚­ã‚¹ãƒˆãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“'}), 400
        
        # â˜… æ­£ã—ã„æ–‡å­—æ•°åˆ¶é™ï¼ˆä¿®æ­£ç‰ˆï¼‰
        original_length = len(text)
        if original_length > 200:
            text = limit_text_for_sl(text, 150)
            logger.warning(f"âš ï¸ Text truncated: {original_length} â†’ {len(text)} chars")
        
        logger.info(f"ğŸ¤ Voice generation: {text[:50]}...")
        
        # éŸ³å£°ç”Ÿæˆ
        voice_path = generate_voice(text)
        
        # çµæœç¢ºèª
        if not voice_path:
            logger.error("âŒ generate_voice() returned None")
            return jsonify({
                'error': 'éŸ³å£°ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ',
                'details': 'VOICEVOXã‚¨ãƒ³ã‚¸ãƒ³ã«æ¥ç¶šã§ãã¾ã›ã‚“'
            }), 500
        
        if not os.path.exists(voice_path):
            logger.error(f"âŒ Voice file not found: {voice_path}")
            return jsonify({
                'error': 'éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            }), 500
        
        # æˆåŠŸãƒ¬ã‚¹ãƒãƒ³ã‚¹
        filename = os.path.basename(voice_path)
        voice_url = f"{SERVER_URL}/voices/{filename}"
        
        logger.info(f"âœ… Voice generated: {filename}")
        
        return jsonify({
            'status': 'success',
            'filename': filename,
            'url': voice_url,
            'text': text
        }), 200
        
    except AttributeError as e:
        logger.error(f"âŒ AttributeError (request.json is None?): {e}")
        return jsonify({
            'error': 'ãƒªã‚¯ã‚¨ã‚¹ãƒˆå½¢å¼ãŒä¸æ­£ã§ã™',
            'details': 'Content-Type: application/json ãŒå¿…è¦ã§ã™'
        }), 400
        
    except Exception as e:
        logger.error(f"âŒ Voice generation exception: {e}", exc_info=True)
        return jsonify({
            'error': 'éŸ³å£°ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ',
            'details': str(e)
        }), 500

@app.route('/voices/<filename>')
def serve_voice_file(filename):
    return send_from_directory(VOICE_DIR, filename)

# â†“â†“â†“ ã“ã“ã«è¿½åŠ  â†“â†“â†“
@app.route('/play_voice')
def play_voice():
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•å†ç”Ÿã™ã‚‹HTMLãƒšãƒ¼ã‚¸"""
    voice_url = request.args.get('url', '')
    
    if not voice_url:
        return "éŸ³å£°URLãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“", 400
    
    if not voice_url.startswith(SERVER_URL):
        return "ä¸æ­£ãªéŸ³å£°URLã§ã™", 400
    
    return f'''<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ã‚‚ã¡ã“AI éŸ³å£°å†ç”Ÿ</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{
            width: 100%;
            height: 100%;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            display: flex;
            justify-content: center;
            align-items: center;
            font-family: 'Segoe UI', Arial, sans-serif;
            overflow: hidden;
        }}
        .player {{
            background: rgba(255, 255, 255, 0.95);
            padding: 30px;
            border-radius: 15px;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
            text-align: center;
            max-width: 400px;
        }}
        .emoji {{
            font-size: 3em;
            animation: pulse 1.5s infinite;
        }}
        @keyframes pulse {{
            0%, 100% {{ transform: scale(1); }}
            50% {{ transform: scale(1.1); }}
        }}
        h1 {{
            color: #667eea;
            margin: 15px 0 10px 0;
            font-size: 1.8em;
        }}
        p {{ color: #666; margin-bottom: 20px; }}
        audio {{ width: 100%; margin-top: 10px; }}
        .status {{
            margin-top: 15px;
            padding: 10px;
            background: #e8f5e9;
            border-radius: 5px;
            color: #2e7d32;
            font-size: 0.9em;
        }}
    </style>
</head>
<body>
    <div class="player">
        <div class="emoji">ğŸ¤</div>
        <h1>ã‚‚ã¡ã“AI</h1>
        <p>éŸ³å£°ã‚’å†ç”Ÿã—ã¦ã„ã¾ã™...</p>
        <audio id="audioPlayer" controls autoplay>
            <source src="{voice_url}" type="audio/wav">
        </audio>
        <div class="status" id="status">æº–å‚™ä¸­...</div>
    </div>
    <script>
        const audio = document.getElementById('audioPlayer');
        const status = document.getElementById('status');
        audio.addEventListener('loadstart', () => {{
            status.textContent = 'éŸ³å£°ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...';
            status.style.background = '#fff3e0';
            status.style.color = '#e65100';
        }});
        audio.addEventListener('play', () => {{
            status.textContent = 'â™ª å†ç”Ÿä¸­...';
            status.style.background = '#e3f2fd';
            status.style.color = '#1565c0';
        }});
        audio.addEventListener('ended', () => {{
            status.textContent = 'âœ“ å†ç”Ÿå®Œäº†';
        }});
        audio.addEventListener('error', () => {{
            status.textContent = 'âœ— èª­ã¿è¾¼ã¿å¤±æ•—';
            status.style.background = '#ffebee';
            status.style.color = '#c62828';
        }});
        audio.play().catch(() => {{
            status.textContent = 'â–¶ å†ç”Ÿãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„';
            status.style.background = '#fff3e0';
            status.style.color = '#e65100';
        }});
    </script>
</body>
</html>'''

@app.route('/play_voice_simple')
def play_voice_simple():
    """æœ€å°é™ã®HTMLã§éŸ³å£°å†ç”Ÿ"""
    voice_url = request.args.get('url', '')
    if not voice_url:
        return "éŸ³å£°URLãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“", 400
    return f'''<!DOCTYPE html>
<html><head><meta charset="UTF-8"><style>
body{{margin:0;background:#667eea;display:flex;justify-content:center;align-items:center;height:100vh;}}
audio{{width:90%;max-width:400px;}}
</style></head><body>
<audio controls autoplay><source src="{voice_url}" type="audio/wav"></audio>
</body></html>'''

@app.route('/stats', methods=['GET'])
def get_stats():
    session = Session()
    try:
        stats = {
            'users': session.query(UserMemory).count(),
            'conversations': session.query(ConversationHistory).count(),
            'hololive_news': session.query(HololiveNews).count(),
            'specialized_news': session.query(SpecializedNews).count(),
            'holomem_wiki_entries': session.query(HolomemWiki).count(),
        }
        return jsonify(stats)
    finally:
        session.close()

# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ– ---

# ã€ä¿®æ­£ç‰ˆã€‘initialize_app é–¢æ•°
def initialize_app():
    global engine, Session, groq_client
    logger.info("=" * 60)
    logger.info("ğŸ”§ Starting Mochiko AI initialization...")
    logger.info("=" * 60)

    try:
        logger.info("ğŸš€ Step 1/6: Initializing Gemini...")
        initialize_gemini_client()
    except Exception as e:
        logger.warning(f"âš ï¸ Gemini initialization failed: {e}")

    try:
        logger.info("ğŸ“¡ Step 2/6: Initializing Groq client...")
        groq_client = initialize_groq_client()
        if groq_client:
            logger.info("âœ… Groq client ready")
        else:
            logger.warning("âš ï¸ Groq client disabled - using fallback responses")
    except Exception as e:
        logger.warning(f"âš ï¸ Groq initialization failed but continuing: {e}")
        groq_client = None

    try:
        logger.info("ğŸ—„ï¸ Step 3/6: Initializing database...")
        engine = create_optimized_db_engine()
        Base.metadata.create_all(engine)
        Session = sessionmaker(bind=engine)
        logger.info("âœ… Database initialized successfully")
    except Exception as e:
        logger.critical(f"ğŸ”¥ Database initialization failed: {e}")
        raise

    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¡ãƒ³ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚§ãƒƒã‚¯
    session = Session()
    try:
        logger.info("ğŸ“° Step 4/6: Checking news + member data...")
        holo_count = session.query(HololiveNews).count()
        member_count = session.query(HolomemWiki).count()
        
        if holo_count == 0 or member_count == 0:
            logger.info("ğŸš€ First run: Scheduling Hololive news + members fetch...")
            background_executor.submit(update_hololive_news_database)
        else:
            logger.info(f"âœ… Found {holo_count} Hololive news, {member_count} members")
            latest_member = session.query(HolomemWiki).order_by(HolomemWiki.last_updated.desc()).first()
            if latest_member and latest_member.last_updated < datetime.utcnow() - timedelta(hours=24):
                logger.info("â° Member data is stale, scheduling update...")
                background_executor.submit(update_hololive_news_database)
        
        spec_count = session.query(SpecializedNews).count()
        if spec_count == 0:
            logger.info("ğŸš€ First run: Scheduling specialized news fetch...")
            background_executor.submit(update_all_specialized_news)
        else:
            logger.info(f"âœ… Found {spec_count} specialized news items")
            
    except Exception as e:
        logger.warning(f"âš ï¸ News initialization check failed but continuing: {e}")
    finally:
        session.close()

    try:
        logger.info("â° Step 5/6: Starting scheduler...")
        schedule.every().hour.do(update_hololive_news_database)
        schedule.every(3).hours.do(update_all_specialized_news)
        schedule.every().day.at("02:00").do(cleanup_old_data_advanced)
        
        def run_scheduler():
            while True:
                try:
                    schedule.run_pending()
                except Exception as e:
                    logger.error(f"âŒ Scheduler error: {e}")
                time.sleep(60)

        scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        scheduler_thread.start()
        logger.info("âœ… Scheduler started successfully")
    except Exception as e:
        logger.error(f"âŒ Scheduler initialization failed: {e}")
    
    logger.info("=" * 60)
    logger.info("âœ… Mochiko AI initialization complete!")
    logger.info("ğŸŒ Server is ready to accept requests")
    logger.info("=" * 60)
    
def signal_handler(sig, frame):
    logger.info(f"ğŸ›‘ Signal {sig} received. Shutting down gracefully...")
    background_executor.shutdown(wait=True)
    if 'engine' in globals() and engine:
        engine.dispose()
    logger.info("ğŸ‘‹ Mochiko AI has shut down.")
    sys.exit(0)

# ========================================
# ã€å¤‰æ›´7ã€‘HOLOMEM_KEYWORDSã‚’DBè‡ªå‹•ç”Ÿæˆã«å¤‰æ›´
# ========================================

def get_active_holomem_keywords():
    """ç¾å½¹ãƒ¡ãƒ³ãƒãƒ¼ã®åå‰ãƒªã‚¹ãƒˆã‚’DBã‹ã‚‰å–å¾—"""
    session = Session()
    try:
        members = session.query(HolomemWiki.member_name).filter_by(is_active=True).all()
        keywords = [m[0] for m in members] + ['ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–', 'ãƒ›ãƒ­ãƒ¡ãƒ³', 'hololive', 'YAGOO']
        return keywords
    except Exception as e:
        logger.error(f"âŒ Failed to get holomem keywords: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return ['ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–', 'ãƒ›ãƒ­ãƒ¡ãƒ³', 'hololive']
    finally:
        session.close()

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# ========================================
# ã€å¤‰æ›´8ã€‘is_hololive_request ã‚’å‹•çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾—ã«å¤‰æ›´
# ========================================

def is_hololive_request(message):
    """ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–é–¢é€£ã®è³ªå•ã‹ã©ã†ã‹åˆ¤å®šï¼ˆå‹•çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰"""
    keywords = get_active_holomem_keywords()
    return any(keyword in message for keyword in keywords)


# ========================================
# ã€å¤‰æ›´9ã€‘chat_lsl ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®åŸºæœ¬æƒ…å ±åˆ¤å®šã‚’å‹•çš„åŒ–
# ========================================

# chat_lsl é–¢æ•°å†…ã®è©²å½“ç®‡æ‰€ã‚’ä»¥ä¸‹ã«ç½®ãæ›ãˆ:


# ========================================
# ã€å¤‰æ›´10ã€‘background_deep_search ã®ãƒ›ãƒ­ãƒ¡ãƒ³åˆ¤å®šã‚’å‹•çš„åŒ–
# ========================================

# background_deep_search é–¢æ•°å†…ã®è©²å½“ç®‡æ‰€ã‚’ä»¥ä¸‹ã«ç½®ãæ›ãˆ:

# Step 3: ãƒ›ãƒ­ãƒ¡ãƒ³æ¤œç´¢ï¼ˆæ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
holomem_keywords = get_active_holomem_keywords()
holomem_matched = None
query_topic = ""

for member_name in holomem_keywords:
    if member_name in query:
        holomem_matched = member_name
        query_topic = query.replace(member_name, '').replace('ã«ã¤ã„ã¦', '').replace('æ•™ãˆã¦', '').strip()
        if not query_topic:
            query_topic = "æ¦‚è¦"
        break

if holomem_matched:
    wiki_info = get_holomem_info(holomem_matched)
    if wiki_info and query_topic == "æ¦‚è¦":
        search_result = f"{holomem_matched}ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±:\n{wiki_info['description']}"
    else:
        search_result = deep_web_search(f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– {holomem_matched} {query_topic}", is_detailed)

# --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ ---
try:
    initialize_app()
    application = app
    logger.info("âœ… Flask application 'application' is ready and initialized.")
except Exception as e:
    logger.critical(f"ğŸ”¥ Fatal initialization error: {e}", exc_info=True)
    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ä½œæˆã™ã‚‹
    application = app
    logger.warning("âš ï¸ Application created with limited functionality due to initialization error.")

if __name__ == '__main__':
    logger.info("ğŸš€ Running in direct mode (not recommended for production)")
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 5000)))
