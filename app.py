# ==============================================================================
# ã‚‚ã¡ã“AI - å…¨æ©Ÿèƒ½çµ±åˆå®Œå…¨ç‰ˆ (v34.1)
#
# ãƒ™ãƒ¼ã‚¹: v34.0.0
# ä¿®æ­£ç‚¹:
# 1. æ¬ è½ã—ã¦ã„ãŸDBãƒ¢ãƒ‡ãƒ« (NewsCache, SpecializedNews) ã‚’è¿½åŠ 
# 2. æ¬ è½ã—ã¦ã„ãŸåˆ¤å®šé–¢æ•° (is_anime_request, is_news_detail_request) ã‚’è¿½åŠ 
# 3. å…¨æ©Ÿèƒ½ï¼ˆå¿ƒç†åˆ†æã€ã‚¢ãƒ‹ãƒ¡æ¤œç´¢ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€å¤©æ°—ã€RAGï¼‰ã®é€£æºç¢ºèª
# ==============================================================================

# ===== æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª =====
import sys
import os
import requests
import logging
import time
import json
import re
import random
import uuid
import hashlib
import unicodedata
import traceback
import threading
import atexit
import glob
import signal
from html import escape
from datetime import datetime, timedelta, timezone
from urllib.parse import quote_plus, urljoin, urlparse
from functools import wraps, lru_cache
from threading import Lock, RLock
from concurrent.futures import ThreadPoolExecutor
from collections import OrderedDict, defaultdict
from contextlib import contextmanager
from dataclasses import dataclass, field
from typing import Optional, Dict, List, Any, Union

# ===== ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒª =====
from flask import Flask, request, jsonify, send_from_directory, Response
from flask_cors import CORS
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Text, Boolean, Index, text
from sqlalchemy.orm import declarative_base, sessionmaker
from sqlalchemy import pool
from bs4 import BeautifulSoup
import schedule
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from groq import Groq

# ==============================================================================
# åŸºæœ¬è¨­å®šã¨ãƒ­ã‚®ãƒ³ã‚°
# ==============================================================================
log_file_path = '/tmp/mochiko.log'
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(log_file_path, encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ==============================================================================
# å®šæ•°è¨­å®š
# ==============================================================================
VOICE_DIR = '/tmp/voices'
os.makedirs(VOICE_DIR, exist_ok=True)

SERVER_URL = os.environ.get('RENDER_EXTERNAL_URL', "http://localhost:5000")
VOICEVOX_SPEAKER_ID = 20
SL_SAFE_CHAR_LIMIT = 600
MIN_MESSAGES_FOR_ANALYSIS = 10
SEARCH_TIMEOUT = 10
VOICE_FILE_MAX_AGE_HOURS = 24
VOICE_OPTIMAL_LENGTH = 150

GROQ_MODELS = [
    "llama-3.3-70b-versatile",
    "llama-3.1-70b-versatile",
    "llama-3.1-8b-instant",
    "mixtral-8x7b-32768",
    "gemma2-9b-it"
]

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:125.0) Gecko/20100101 Firefox/125.0',
]

LOCATION_CODES = {
    "æ±äº¬": "130000", "å¤§é˜ª": "270000", "åå¤å±‹": "230000",
    "ç¦å²¡": "400000", "æœ­å¹Œ": "016000"
}

VOICEVOX_URLS = [
    'http://voicevox-engine:50021', 'http://voicevox:50021',
    'http://127.0.0.1:50021', 'http://localhost:50021'
]

HOLOMEM_KEYWORDS = [
    'ã¨ãã®ãã‚‰', 'ãƒ­ãƒœå­ã•ã‚“', 'ã•ãã‚‰ã¿ã“', 'æ˜Ÿè¡—ã™ã„ã›ã„', 'AZKi', 'å¤œç©ºãƒ¡ãƒ«',
    'ã‚¢ã‚­ãƒ»ãƒ­ãƒ¼ã‚¼ãƒ³ã‚¿ãƒ¼ãƒ«', 'èµ¤äº•ã¯ã‚ã¨', 'ç™½ä¸Šãƒ•ãƒ–ã‚­', 'å¤è‰²ã¾ã¤ã‚Š', 'æ¹Šã‚ãã‚',
    'ç´«å’²ã‚·ã‚ªãƒ³', 'ç™¾é¬¼ã‚ã‚„ã‚', 'ç™’æœˆã¡ã‚‡ã“', 'å¤§ç©ºã‚¹ãƒãƒ«', 'å¤§ç¥ãƒŸã‚ª', 'çŒ«åˆãŠã‹ã‚†',
    'æˆŒç¥ã“ã‚ã­', 'å…ç”°ãºã“ã‚‰', 'ä¸çŸ¥ç«ãƒ•ãƒ¬ã‚¢', 'ç™½éŠ€ãƒã‚¨ãƒ«', 'å®é˜ãƒãƒªãƒ³', 'å¤©éŸ³ã‹ãªãŸ',
    'è§’å·»ã‚ãŸã‚', 'å¸¸é—‡ãƒˆãƒ¯', 'å§«æ£®ãƒ«ãƒ¼ãƒŠ', 'é›ªèŠ±ãƒ©ãƒŸã‚£', 'æ¡ƒéˆ´ã­ã­', 'ç…ç™½ã¼ãŸã‚“',
    'å°¾ä¸¸ãƒãƒ«ã‚«', 'ãƒ©ãƒ—ãƒ©ã‚¹ãƒ»ãƒ€ãƒ¼ã‚¯ãƒã‚¹', 'é·¹å¶ºãƒ«ã‚¤', 'åšè¡£ã“ã‚ˆã‚Š', 'æ²™èŠ±å‰ã‚¯ãƒ­ãƒ±',
    'é¢¨çœŸã„ã‚ã¯', 'æ£®ã‚«ãƒªã‚ªãƒš', 'å°é³¥éŠã‚­ã‚¢ãƒ©', 'ä¸€ä¼Šé‚£å°“æ –', 'ãŒã†ã‚‹ãƒ»ãã‚‰',
    'ãƒ¯ãƒˆã‚½ãƒ³ãƒ»ã‚¢ãƒ¡ãƒªã‚¢', 'IRyS', 'ã‚»ãƒ¬ã‚¹ãƒ»ãƒ•ã‚¡ã‚¦ãƒŠ', 'ã‚ªãƒ¼ãƒ­ãƒ»ã‚¯ãƒ­ãƒ‹ãƒ¼', 'ä¸ƒè©©ãƒ ãƒ¡ã‚¤',
    'ãƒã‚³ã‚¹ãƒ»ãƒ™ãƒ¼ãƒ«ã‚º', 'ã‚·ã‚ªãƒªãƒ»ãƒãƒ´ã‚§ãƒ©', 'å¤çŸ³ãƒ“ã‚¸ãƒ¥ãƒ¼', 'ãƒãƒªãƒƒã‚µãƒ»ãƒ¬ã‚¤ãƒ´ãƒ³ã‚¯ãƒ­ãƒ•ãƒˆ',
    'ãƒ•ãƒ¯ãƒ¯ãƒ»ã‚¢ãƒ“ã‚¹ã‚¬ãƒ¼ãƒ‰', 'ãƒ¢ã‚³ã‚³ãƒ»ã‚¢ãƒ“ã‚¹ã‚¬ãƒ¼ãƒ‰', 'ã‚¢ãƒ¦ãƒ³ãƒ€ãƒ»ãƒªã‚¹', 'ãƒ ãƒ¼ãƒŠãƒ»ãƒ›ã‚·ãƒãƒ´ã‚¡',
    'ã‚¢ã‚¤ãƒ©ãƒ‹ãƒ»ã‚¤ã‚ªãƒ•ã‚£ãƒ•ãƒ†ã‚£ãƒ¼ãƒ³', 'ã‚¯ãƒ¬ã‚¤ã‚¸ãƒ¼ãƒ»ã‚ªãƒªãƒ¼', 'ã‚¢ãƒ¼ãƒ‹ãƒ£ãƒ»ãƒ¡ãƒ«ãƒ•ã‚£ãƒƒã‚µ',
    'ãƒ‘ãƒ´ã‚©ãƒªã‚¢ãƒ»ãƒ¬ã‚¤ãƒ', 'ç«å¨é’', 'éŸ³ä¹ƒç€¬å¥', 'ä¸€æ¡è‰ã€…è¯', 'å„’çƒé¢¨äº­ã‚‰ã§ã‚“',
    'è½Ÿã¯ã˜ã‚', 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–', 'ãƒ›ãƒ­ãƒ¡ãƒ³', 'hololive', 'YAGOO'
]

ANIME_KEYWORDS = [
    'ã‚¢ãƒ‹ãƒ¡', 'anime', 'ANIME', 'ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³', 'ä½œç”»', 'å£°å„ª',
    'OP', 'ED', 'ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°', 'ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°', 'åŠ‡å ´ç‰ˆ', 'æ˜ ç”»',
    'OVA', 'OAD', 'åŸä½œ', 'æ¼«ç”»', 'ãƒ©ãƒãƒ™', 'ä¸»äººå…¬', 'ã‚­ãƒ£ãƒ©',
    'ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼', 'åˆ¶ä½œä¼šç¤¾', 'ã‚¹ã‚¿ã‚¸ã‚ª'
]

SPECIALIZED_SITES = {
    'Blender': {
        'base_url': 'https://docs.blender.org/manual/ja/latest/',
        'keywords': ['Blender', 'ãƒ–ãƒ¬ãƒ³ãƒ€ãƒ¼', 'blender', 'BLENDER']
    },
    'CGãƒ‹ãƒ¥ãƒ¼ã‚¹': {
        'base_url': 'https://modelinghappy.com/',
        'keywords': ['CGãƒ‹ãƒ¥ãƒ¼ã‚¹', '3DCG', 'CG', 'cg', '3dcg', 'CGã‚¢ãƒ‹ãƒ¡']
    },
    'è„³ç§‘å­¦ãƒ»å¿ƒç†å­¦': {
        'base_url': 'https://nazology.kusuguru.co.jp/',
        'keywords': ['è„³ç§‘å­¦', 'å¿ƒç†å­¦', 'è„³', 'å¿ƒç†']
    },
    'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•': {
        'base_url': 'https://community.secondlife.com/news/',
        'keywords': ['ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•', 'Second Life', 'SL', 'SecondLife']
    },
    'ã‚¢ãƒ‹ãƒ¡': {
        'base_url': 'https://animedb.jp/',
        'keywords': ['ã‚¢ãƒ‹ãƒ¡', 'anime', 'ANIME']
    }
}

# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
# ==============================================================================
@dataclass
class GroqModelStatus:
    is_limited: bool = False
    reset_time: Optional[datetime] = None
    last_error: Optional[str] = None

@dataclass
class UserData:
    uuid: str
    name: str
    interaction_count: int

# ==============================================================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ç®¡ç†
# ==============================================================================
class GlobalState:
    def __init__(self):
        self._lock = RLock()
        self._voicevox_enabled = False
        self._active_voicevox_url = None

    @property
    def voicevox_enabled(self) -> bool:
        with self._lock: return self._voicevox_enabled
    @voicevox_enabled.setter
    def voicevox_enabled(self, value: bool):
        with self._lock: self._voicevox_enabled = value
    @property
    def active_voicevox_url(self) -> Optional[str]:
        with self._lock: return self._active_voicevox_url
    @active_voicevox_url.setter
    def active_voicevox_url(self, value: Optional[str]):
        with self._lock: self._active_voicevox_url = value

class GroqModelManager:
    def __init__(self, models: List[str]):
        self._lock = RLock()
        self._status: Dict[str, GroqModelStatus] = {m: GroqModelStatus() for m in models}
        self._models = models

    def is_available(self, model: str) -> bool:
        with self._lock:
            s = self._status.get(model)
            if not s: return False
            if not s.is_limited: return True
            if s.reset_time and datetime.utcnow() >= s.reset_time:
                s.is_limited = False
                s.reset_time = None
                return True
            return False

    def mark_limited(self, model: str, wait_minutes: int = 5, error_msg: str = ""):
        with self._lock:
            if model in self._status:
                self._status[model].is_limited = True
                self._status[model].reset_time = datetime.utcnow() + timedelta(minutes=wait_minutes)

    def get_status_report(self) -> str:
        with self._lock:
            lines = ["ğŸ¦™ Groq ãƒ¢ãƒ‡ãƒ«ç¨¼åƒçŠ¶æ³:"]
            for m in self._models:
                s = self._status[m]
                if s.is_limited:
                    jst = (s.reset_time + timedelta(hours=9)).strftime('%H:%M:%S') if s.reset_time else "ä¸æ˜"
                    lines.append(f"  âŒ {m}: åˆ¶é™ä¸­ (è§£é™¤: {jst})")
                else:
                    lines.append(f"  âœ… {m}: OK")
            return "\n".join(lines)

    def get_available_models(self) -> List[str]:
        with self._lock: return [m for m in self._models if self.is_available(m)]

global_state = GlobalState()
groq_model_manager = GroqModelManager(GROQ_MODELS)
background_executor = ThreadPoolExecutor(max_workers=5)
groq_client: Optional[Groq] = None
gemini_model = None
engine = None
Session = None

# ==============================================================================
# Flask ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
# ==============================================================================
app = Flask(__name__)
application = app
app.config['JSON_AS_ASCII'] = False
CORS(app)

@app.after_request
def after_request(response):
    response.headers.add('Access-Control-Allow-Origin', '*')
    response.headers.add('Access-Control-Allow-Headers', 'Content-Type, Authorization')
    response.headers.add('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
    return response

Base = declarative_base()

# ==============================================================================
# ç§˜å¯†æƒ…å ±/ç’°å¢ƒå¤‰æ•°
# ==============================================================================
def get_secret(name: str) -> Optional[str]:
    env_value = os.environ.get(name)
    if env_value and env_value.strip():
        return env_value.strip()
    try:
        secret_file_path = f"/etc/secrets/{name}"
        if os.path.exists(secret_file_path):
            with open(secret_file_path, 'r') as f:
                val = f.read().strip()
                if val: return val
    except: pass
    return None

DATABASE_URL = get_secret('DATABASE_URL') or 'sqlite:///./mochiko_ultimate.db'
GROQ_API_KEY = get_secret('GROQ_API_KEY')
GEMINI_API_KEY = get_secret('GEMINI_API_KEY')
VOICEVOX_URL_FROM_ENV = get_secret('VOICEVOX_URL')

# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
# ==============================================================================
class UserMemory(Base):
    __tablename__ = 'user_memories'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False, index=True)
    user_name = Column(String(255), nullable=False)
    interaction_count = Column(Integer, default=0)
    last_interaction = Column(DateTime, default=datetime.utcnow)

class ConversationHistory(Base):
    __tablename__ = 'conversation_history'
    id = Column(Integer, primary_key=True, autoincrement=True)
    user_uuid = Column(String(255), nullable=False, index=True)
    role = Column(String(10), nullable=False)
    content = Column(Text, nullable=False)
    timestamp = Column(DateTime, default=datetime.utcnow, index=True)

class UserPsychology(Base):
    __tablename__ = 'user_psychology'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False, index=True)
    user_name = Column(String(255), nullable=False)
    openness = Column(Integer, default=50)
    extraversion = Column(Integer, default=50)
    favorite_topics = Column(Text, nullable=True)
    analysis_confidence = Column(Integer, default=0)
    last_analyzed = Column(DateTime, nullable=True)
    # è¿½åŠ ã‚«ãƒ©ãƒ ï¼ˆå¿ƒç†åˆ†æã®è©³ç´°ç”¨ï¼‰
    conscientiousness = Column(Integer, default=50)
    agreeableness = Column(Integer, default=50)
    neuroticism = Column(Integer, default=50)
    interests = Column(Text, nullable=True)
    conversation_style = Column(String(255), nullable=True)
    emotional_tendency = Column(String(255), nullable=True)
    analysis_summary = Column(Text, nullable=True)
    total_messages = Column(Integer, default=0)
    avg_message_length = Column(Integer, default=0)

class BackgroundTask(Base):
    __tablename__ = 'background_tasks'
    id = Column(Integer, primary_key=True)
    task_id = Column(String(255), unique=True, nullable=False)
    user_uuid = Column(String(255), nullable=False, index=True)
    task_type = Column(String(50), nullable=False)
    query = Column(Text, nullable=False)
    result = Column(Text, nullable=True)
    status = Column(String(20), default='pending', index=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime, nullable=True)

class HolomemWiki(Base):
    __tablename__ = 'holomem_wiki'
    id = Column(Integer, primary_key=True)
    member_name = Column(String(100), nullable=False, unique=True, index=True)
    description = Column(Text, nullable=True)
    generation = Column(String(100), nullable=True)
    debut_date = Column(String(100), nullable=True)
    tags = Column(Text, nullable=True)
    status = Column(String(50), default='ç¾å½¹', nullable=False)
    graduation_date = Column(String(100), nullable=True)
    graduation_reason = Column(Text, nullable=True)
    mochiko_feeling = Column(Text, nullable=True)
    last_updated = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

class HololiveNews(Base):
    __tablename__ = 'hololive_news'
    id = Column(Integer, primary_key=True)
    title = Column(String(500), nullable=False)
    content = Column(Text, nullable=False)
    url = Column(String(1000), unique=True)
    news_hash = Column(String(100), unique=True, index=True)
    created_at = Column(DateTime, default=datetime.utcnow, index=True)

class HolomemNickname(Base):
    __tablename__ = 'holomem_nicknames'
    id = Column(Integer, primary_key=True)
    nickname = Column(String(100), unique=True, nullable=False, index=True)
    fullname = Column(String(100), nullable=False)

class HololiveGlossary(Base):
    __tablename__ = 'hololive_glossary'
    id = Column(Integer, primary_key=True)
    term = Column(String(100), unique=True, nullable=False, index=True)
    description = Column(Text, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)

# --- æ¬ è½ã—ã¦ã„ãŸãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ  ---
class NewsCache(Base):
    __tablename__ = 'news_cache'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), nullable=False, index=True)
    news_id = Column(Integer, nullable=False)
    news_number = Column(Integer, nullable=False)
    news_type = Column(String(50), nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)

class SpecializedNews(Base):
    __tablename__ = 'specialized_news'
    id = Column(Integer, primary_key=True)
    site_name = Column(String(100), nullable=False, index=True)
    title = Column(String(500), nullable=False)
    content = Column(Text, nullable=False)
    url = Column(String(1000), unique=True)
    news_hash = Column(String(100), unique=True, index=True)
    created_at = Column(DateTime, default=datetime.utcnow, index=True)
    published_date = Column(DateTime, default=datetime.utcnow)

# ==============================================================================
# ã‚»ãƒƒã‚·ãƒ§ãƒ³ & ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ==============================================================================
class RateLimiter:
    def __init__(self, max_requests: int, time_window: timedelta):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests: Dict[str, List[datetime]] = defaultdict(list)
        self._lock = threading.Lock()
    def is_allowed(self, user_id: str) -> bool:
        with self._lock:
            now = datetime.utcnow(); cutoff = now - self.time_window
            self.requests[user_id] = [t for t in self.requests[user_id] if t > cutoff]
            if len(self.requests[user_id]) >= self.max_requests: return False
            self.requests[user_id].append(now); return True
    def cleanup_old_entries(self):
        with self._lock:
            now = datetime.utcnow(); cutoff = now - self.time_window
            for uid in list(self.requests.keys()):
                self.requests[uid] = [t for t in self.requests[uid] if t > cutoff]
                if not self.requests[uid]: del self.requests[uid]

chat_rate_limiter = RateLimiter(max_requests=10, time_window=timedelta(minutes=1))

@contextmanager
def get_db_session():
    if not Session: raise Exception("Session not initialized")
    session = Session()
    try: yield session; session.commit()
    except Exception as e: session.rollback(); raise
    finally: session.close()

def create_json_response(data: Any, status: int = 200) -> Response:
    return Response(json.dumps(data, ensure_ascii=False), mimetype='application/json; charset=utf-8', status=status)

def clean_text(text: str) -> str:
    if not text: return ""
    return re.sub(r'\s+', ' ', re.sub(r'<[^>]+>', '', text)).strip()

def limit_text_for_sl(text: str, max_length: int = SL_SAFE_CHAR_LIMIT) -> str:
    return text[:max_length - 3] + "..." if len(text) > max_length else text

def sanitize_user_input(text: str, max_length: int = 1000) -> str:
    if not text: return ""
    text = text[:max_length]; text = escape(text)
    return text.strip()

def get_japan_time() -> str:
    return f"ä»Šã®æ—¥æœ¬ã®æ™‚é–“ã¯ã€{datetime.now(timezone(timedelta(hours=9))).strftime('%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†')}ã ã‚ˆï¼"

def is_time_request(msg: str) -> bool:
    return any(kw in msg for kw in ['ä»Šä½•æ™‚', 'æ™‚åˆ»', 'ä½•æ™‚', 'ãªã‚“ã˜'])

def is_weather_request(msg: str) -> bool:
    return any(kw in msg for kw in ['ä»Šæ—¥ã®å¤©æ°—', 'æ˜æ—¥ã®å¤©æ°—', 'å¤©æ°—äºˆå ±', 'å¤©æ°—ã¯'])

# --- æ¬ è½ã—ã¦ã„ãŸåˆ¤å®šé–¢æ•° ---
def is_anime_request(message: str) -> bool:
    """ã‚¢ãƒ‹ãƒ¡é–¢é€£ã®è³ªå•ã‹ã©ã†ã‹åˆ¤å®š"""
    message_normalized = unicodedata.normalize('NFKC', message).lower()
    # ã‚¢ãƒ‹ãƒ¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
    for keyword in ANIME_KEYWORDS:
        if keyword.lower() in message_normalized:
            return True
    # ã€Œã€œã£ã¦ã‚¢ãƒ‹ãƒ¡ã€ã€Œã€œã¨ã„ã†ã‚¢ãƒ‹ãƒ¡ã€ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    anime_patterns = [r'ã£ã¦ã‚¢ãƒ‹ãƒ¡', r'ã¨ã„ã†ã‚¢ãƒ‹ãƒ¡', r'ã®ã‚¢ãƒ‹ãƒ¡', r'ã‚¢ãƒ‹ãƒ¡ã§', r'ã‚¢ãƒ‹ãƒ¡ã®', r'ã‚¢ãƒ‹ãƒ¡ã¯']
    if any(re.search(p, message) for p in anime_patterns):
        return True
    return False

def is_news_detail_request(message: str) -> Optional[int]:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹è©³ç´°ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆä¾‹: 1ç•ªè©³ã—ãï¼‰ã®åˆ¤å®š"""
    match = re.search(r'([1-9]|[ï¼‘-ï¼™])ç•ª|ã€([1-9]|[ï¼‘-ï¼™])ã€‘', message)
    if match and any(keyword in message for keyword in ['è©³ã—ã', 'è©³ç´°', 'æ•™ãˆã¦', 'ã‚‚ã£ã¨']):
        number_str = next(filter(None, match.groups()))
        return int(unicodedata.normalize('NFKC', number_str))
    return None

def is_explicit_search_request(msg: str) -> bool:
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæ¤œç´¢è¦æ±‚ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    msg = msg.strip()
    # 1. æ˜ç¢ºãªã€Œæ¤œç´¢å‘½ä»¤ã€å‹•è©
    strong_triggers = ['èª¿ã¹ã¦', 'æ¤œç´¢', 'æ¢ã—ã¦', 'ã¨ã¯', 'ã£ã¦ä½•', 'ã«ã¤ã„ã¦', 'æ•™ãˆã¦', 'æ•™ãˆã‚', 'è©³ç´°', 'çŸ¥ã‚ŠãŸã„']
    if any(kw in msg for kw in strong_triggers):
        return True
    # 2. åè©ç³»ãƒˆãƒªã‚¬ãƒ¼ï¼ˆçŸ­ã„å ´åˆã‚„ç–‘å•å½¢ã®ã¿æ¤œç´¢ï¼‰
    noun_triggers = ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'news', 'NEWS', 'æƒ…å ±', 'æ—¥ç¨‹', 'ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«', 'å¤©æ°—', 'äºˆå ±']
    if any(kw in msg for kw in noun_triggers):
        if len(msg) < 20: return True
        if msg.endswith('?') or msg.endswith('ï¼Ÿ'): return True
        return False
    # 3. ãŠã™ã™ã‚
    if 'ãŠã™ã™ã‚' in msg or 'ã‚ªã‚¹ã‚¹ãƒ¡' in msg: return True
    return False

def extract_location(msg: str) -> str:
    for loc in LOCATION_CODES.keys():
        if loc in msg: return loc
    return "æ±äº¬"

# ==============================================================================
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ç®¡ç†
# ==============================================================================
def get_or_create_user(session, user_uuid: str, user_name: str) -> UserData:
    user = session.query(UserMemory).filter_by(user_uuid=user_uuid).first()
    if user:
        user.interaction_count += 1
        user.last_interaction = datetime.utcnow()
        if user.user_name != user_name:
            user.user_name = user_name
    else:
        user = UserMemory(user_uuid=user_uuid, user_name=user_name, interaction_count=1)
        session.add(user)
    return UserData(uuid=user.user_uuid, name=user.user_name, interaction_count=user.interaction_count)

def get_conversation_history(session, user_uuid: str, limit: int = 10) -> List[Dict]:
    hist = session.query(ConversationHistory).filter_by(user_uuid=user_uuid).order_by(
        ConversationHistory.timestamp.desc()
    ).limit(limit).all()
    return [{'role': h.role, 'content': h.content} for h in reversed(hist)]

# ==============================================================================
# å¤©æ°—äºˆå ±å–å¾—
# ==============================================================================
def get_weather_forecast(location: str) -> str:
    area_code = LOCATION_CODES.get(location, "130000")
    url = f"https://www.jma.go.jp/bosai/forecast/data/overview_forecast/{area_code}.json"
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        data = response.json()
        text = clean_text(data.get('text', ''))
        if not text:
            return f"{location}ã®å¤©æ°—æƒ…å ±ãŒã¡ã‚‡ã£ã¨å–ã‚Œãªã‹ã£ãŸâ€¦"
        weather_text = f"ä»Šã®{location}ã®å¤©æ°—ã¯ã­ã€ã€Œ{text}ã€ã£ã¦æ„Ÿã˜ã ã‚ˆï¼"
        return limit_text_for_sl(weather_text, 200)
    except requests.exceptions.Timeout:
        logger.warning(f"Weather API timeout for {location}")
        return "å¤©æ°—æƒ…å ±ã®å–å¾—ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¡ã‚ƒã£ãŸâ€¦"
    except Exception as e:
        logger.error(f"Weather API error for {location}: {e}")
        return "å¤©æ°—æƒ…å ±ãŒã†ã¾ãå–ã‚Œãªã‹ã£ãŸã¿ãŸã„â€¦"

# ==============================================================================
# çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚¯ãƒ©ã‚¹
# ==============================================================================
class HololiveKnowledgeBase:
    def __init__(self):
        self.nickname_map = {}
        self.glossary = {}
        self._lock = RLock()

    def load_data(self):
        if not Session: return
        with self._lock:
            session = Session()
            try:
                nicks = session.query(HolomemNickname).all()
                self.nickname_map = {n.nickname: n.fullname for n in nicks}
                terms = session.query(HololiveGlossary).all()
                self.glossary = {t.term: t.description for t in terms}
                logger.info(f"ğŸ“š Knowledge Base loaded: {len(self.nickname_map)} nicknames, {len(self.glossary)} terms.")
            except Exception as e:
                logger.error(f"âŒ Failed to load knowledge base: {e}")
            finally:
                session.close()

    def refresh(self):
        self.load_data()

    def normalize_query(self, text: str) -> str:
        normalized = text
        with self._lock:
            for nick, full in self.nickname_map.items():
                if nick in text:
                    normalized = normalized.replace(nick, f"{nick}ï¼ˆ{full}ï¼‰")
        return normalized

    def get_context_info(self, text: str) -> str:
        context_parts = []
        with self._lock:
            for term, desc in self.glossary.items():
                if term in text:
                    context_parts.append(f"ã€ç”¨èªè§£èª¬: {term}ã€‘{desc}")
        return "\n".join(context_parts)

knowledge_base = HololiveKnowledgeBase()

# ==============================================================================
# ãƒ›ãƒ­ãƒ¡ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç®¡ç†
# ==============================================================================
class HolomemKeywordManager:
    def __init__(self):
        self._lock = RLock()
        self._keywords: Dict[str, List[str]] = {}
        self._all_keywords: set = set()
        self._last_loaded: Optional[datetime] = None

    def load_from_db(self, force: bool = False) -> bool:
        with self._lock:
            try:
                with get_db_session() as session:
                    members = session.query(HolomemWiki).all()
                    self._keywords.clear()
                    self._all_keywords.clear()
                    for m in members:
                        name = m.member_name
                        self._keywords[name] = [name]
                        self._all_keywords.add(name)
                    return True
            except:
                return False

    def detect_in_message(self, message: str) -> Optional[str]:
        with self._lock:
            normalized = knowledge_base.normalize_query(message)
            for keyword in self._all_keywords:
                if keyword in normalized:
                    return keyword
            return None

    def get_member_count(self) -> int:
        with self._lock:
            return len(self._keywords)

holomem_manager = HolomemKeywordManager()

# ==============================================================================
# ãƒ›ãƒ­ãƒ¡ãƒ³æƒ…å ±ã‚­ãƒ£ãƒƒã‚·ãƒ¥
# ==============================================================================
_holomem_cache: Dict[str, Dict] = {}
_holomem_cache_lock = threading.Lock()
_holomem_cache_ttl = timedelta(minutes=30)
_holomem_cache_timestamps: Dict[str, datetime] = {}

def get_holomem_info_cached(member_name: str) -> Optional[Dict]:
    with _holomem_cache_lock:
        if member_name in _holomem_cache:
            if (datetime.utcnow() - _holomem_cache_timestamps.get(member_name, datetime.min)) < _holomem_cache_ttl:
                return _holomem_cache[member_name]
    with get_db_session() as session:
        wiki = session.query(HolomemWiki).filter_by(member_name=member_name).first()
        if wiki:
            data = {
                'member_name': wiki.member_name,
                'description': wiki.description,
                'generation': wiki.generation,
                'debut_date': wiki.debut_date,
                'tags': wiki.tags,
                'status': wiki.status,
                'graduation_date': wiki.graduation_date,
                'graduation_reason': wiki.graduation_reason,
                'mochiko_feeling': wiki.mochiko_feeling
            }
            with _holomem_cache_lock:
                _holomem_cache[member_name] = data
                _holomem_cache_timestamps[member_name] = datetime.utcnow()
            return data
    return None

def clear_holomem_cache(member_name: Optional[str] = None):
    with _holomem_cache_lock:
        if member_name:
            _holomem_cache.pop(member_name, None)
        else:
            _holomem_cache.clear()

def get_holomem_context(member_name: str) -> Optional[str]:
    """ãƒ›ãƒ­ãƒ¡ãƒ³æƒ…å ±ã‚’RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å–å¾—"""
    info = get_holomem_info_cached(member_name)
    if not info:
        return None
    context_parts = [
        f"ã€{info['member_name']}ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã€‘",
        f"ãƒ»èª¬æ˜: {info.get('description', 'æƒ…å ±ãªã—')}",
        f"ãƒ»æ‰€å±: {info.get('generation', 'ä¸æ˜')}",
        f"ãƒ»ãƒ‡ãƒ“ãƒ¥ãƒ¼: {info.get('debut_date', 'ä¸æ˜')}",
        f"ãƒ»çŠ¶æ…‹: {info.get('status', 'ç¾å½¹')}"
    ]
    if info.get('graduation_date'):
        context_parts.append(f"ãƒ»å’æ¥­æ—¥: {info['graduation_date']}")
    if info.get('mochiko_feeling'):
        context_parts.append(f"ãƒ»ã‚‚ã¡ã“ã®æ°—æŒã¡: {info['mochiko_feeling']}")
    if info.get('tags'):
        tags = info['tags'] if isinstance(info['tags'], str) else ', '.join(info['tags'])
        context_parts.append(f"ãƒ»ã‚¿ã‚°: {tags}")
    return '\n'.join(context_parts)

def get_sakuramiko_special_responses() -> Dict[str, str]:
    return {
        'ã«ã‡': 'ã•ãã‚‰ã¿ã“ã¡ã‚ƒã‚“ã®ã€Œã«ã‡ã€ã€ã¾ã˜ã‹ã‚ã„ã„ã‚ˆã­!',
        'ã‚¨ãƒªãƒ¼ãƒˆ': 'ã¿ã“ã¡ã¯è‡ªç§°ã‚¨ãƒªãƒ¼ãƒˆVTuber!ã§ã‚‚æ„›ã•ã‚Œãƒãƒ³ã‚³ãƒ„ã‚­ãƒ£ãƒ©ãªã‚“ã ã‚ˆã­ã€œ',
        'ãƒã‚¤ã‚¯ãƒ©': 'ã¿ã“ã¡ã®ãƒã‚¤ã‚¯ãƒ©å»ºç¯‰ã€ç‹¬å‰µçš„ã™ãã¦é¢ç™½ã„ã‚ˆ!',
        'FAQ': 'ã¿ã“ã¡ã®FAQã€ãƒ•ã‚¡ãƒ³ãŒè³ªå•ã™ã‚‹ã‚³ãƒ¼ãƒŠãƒ¼ãªã‚“ã ã‚ˆã€œ',
        'GTA': 'ã¿ã“ã¡ã®GTAé…ä¿¡ã€ã‚«ã‚ªã‚¹ã§æœ€é«˜!'
    }

# ==============================================================================
# ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
# ==============================================================================
def save_news_cache(session, user_uuid: str, news_items: List, news_type: str = 'hololive'):
    try:
        one_hour_ago = datetime.utcnow() - timedelta(hours=1)
        session.query(NewsCache).filter(
            NewsCache.user_uuid == user_uuid,
            NewsCache.created_at < one_hour_ago
        ).delete()
        for i, news in enumerate(news_items, 1):
            cache = NewsCache(
                user_uuid=user_uuid,
                news_id=news.id,
                news_number=i,
                news_type=news_type
            )
            session.add(cache)
        logger.info(f"ğŸ’¾ News cache saved for user {user_uuid}: {len(news_items)} items")
    except Exception as e:
        logger.error(f"Error saving news cache: {e}")

def get_cached_news_detail(session, user_uuid: str, news_number: int):
    try:
        one_hour_ago = datetime.utcnow() - timedelta(hours=1)
        cache = session.query(NewsCache).filter(
            NewsCache.user_uuid == user_uuid,
            NewsCache.news_number == news_number,
            NewsCache.created_at > one_hour_ago
        ).order_by(NewsCache.created_at.desc()).first()
        if not cache:
            return None
        NewsModel = HololiveNews if cache.news_type == 'hololive' else SpecializedNews
        return session.query(NewsModel).filter_by(id=cache.news_id).first()
    except Exception as e:
        logger.error(f"Error getting cached news: {e}")
        return None

# ==============================================================================
# å¿ƒç†åˆ†ææ©Ÿèƒ½
# ==============================================================================
def analyze_user_psychology(user_uuid: str) -> Optional[Dict]:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éå»ã®ä¼šè©±å±¥æ­´ã‹ã‚‰å¿ƒç†åˆ†æã‚’å®Ÿè¡Œ"""
    if not Session:
        return None
    try:
        with get_db_session() as session:
            logger.info(f"ğŸ§  Starting psychology analysis for user: {user_uuid}")
            conversations = session.query(ConversationHistory).filter_by(
                user_uuid=user_uuid, role='user'
            ).order_by(ConversationHistory.timestamp.desc()).limit(100).all()
            if len(conversations) < MIN_MESSAGES_FOR_ANALYSIS:
                logger.warning(f"Not enough data for analysis: {len(conversations)} messages")
                return None
            messages_text = "\n".join([c.content for c in reversed(conversations)])
            total_messages = len(conversations)
            avg_length = sum(len(c.content) for c in conversations) // total_messages
            user = session.query(UserMemory).filter_by(user_uuid=user_uuid).first()
            user_name = user.user_name if user else "ä¸æ˜"
            if not groq_client:
                logger.warning("Groq client unavailable, skipping AI analysis")
                return None
            analysis_prompt = f"""ã‚ãªãŸã¯å¿ƒç†å­¦ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€Œ{user_name}ã€ã•ã‚“ã®éå»ã®ä¼šè©±ï¼ˆ{total_messages}ä»¶ï¼‰ã‚’åˆ†æã—ã€å¿ƒç†ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€ä¼šè©±å±¥æ­´ã€‘
{messages_text[:3000]}

**é‡è¦**: ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆä»–ã®æ–‡ç« ã¯ä¸è¦ï¼‰:
{{
  "openness": 75,
  "conscientiousness": 60,
  "extraversion": 80,
  "agreeableness": 70,
  "neuroticism": 40,
  "interests": {{"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–": 90, "ã‚²ãƒ¼ãƒ ": 70}},
  "conversation_style": "ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã§è¦ªã—ã¿ã‚„ã™ã„",
  "emotional_tendency": "ãƒã‚¸ãƒ†ã‚£ãƒ–ã§æ˜ã‚‹ã„",
  "favorite_topics": ["ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–", "ã‚²ãƒ¼ãƒ ", "é›‘è«‡"],
  "summary": "æ˜ã‚‹ãç¤¾äº¤çš„ãªæ€§æ ¼ã§ã€ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã‚„å‰µä½œæ´»å‹•ã«å¼·ã„èˆˆå‘³ã‚’æŒã¤ã€‚",
  "confidence": 85
}}"""
            completion = groq_client.chat.completions.create(
                messages=[{"role": "user", "content": analysis_prompt}],
                model="llama-3.1-8b-instant",
                temperature=0.3,
                max_tokens=800
            )
            response_text = completion.choices[0].message.content.strip()
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(0)
            analysis_data = json.loads(response_text)
            psychology = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
            if psychology:
                psychology.user_name = user_name
                psychology.openness = analysis_data.get('openness', 50)
                psychology.conscientiousness = analysis_data.get('conscientiousness', 50)
                psychology.extraversion = analysis_data.get('extraversion', 50)
                psychology.agreeableness = analysis_data.get('agreeableness', 50)
                psychology.neuroticism = analysis_data.get('neuroticism', 50)
                psychology.interests = json.dumps(analysis_data.get('interests', {}), ensure_ascii=False)
                psychology.favorite_topics = json.dumps(analysis_data.get('favorite_topics', []), ensure_ascii=False)
                psychology.conversation_style = analysis_data.get('conversation_style', '')
                psychology.emotional_tendency = analysis_data.get('emotional_tendency', '')
                psychology.analysis_summary = analysis_data.get('summary', '')
                psychology.total_messages = total_messages
                psychology.avg_message_length = avg_length
                psychology.analysis_confidence = analysis_data.get('confidence', 70)
                psychology.last_analyzed = datetime.utcnow()
            else:
                psychology = UserPsychology(
                    user_uuid=user_uuid, user_name=user_name,
                    openness=analysis_data.get('openness', 50),
                    conscientiousness=analysis_data.get('conscientiousness', 50),
                    extraversion=analysis_data.get('extraversion', 50),
                    agreeableness=analysis_data.get('agreeableness', 50),
                    neuroticism=analysis_data.get('neuroticism', 50),
                    interests=json.dumps(analysis_data.get('interests', {}), ensure_ascii=False),
                    favorite_topics=json.dumps(analysis_data.get('favorite_topics', []), ensure_ascii=False),
                    conversation_style=analysis_data.get('conversation_style', ''),
                    emotional_tendency=analysis_data.get('emotional_tendency', ''),
                    analysis_summary=analysis_data.get('summary', ''),
                    total_messages=total_messages,
                    avg_message_length=avg_length,
                    analysis_confidence=analysis_data.get('confidence', 70)
                )
                session.add(psychology)
            logger.info(f"âœ… Psychology analysis saved for user: {user_uuid}")
            return analysis_data
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse AI analysis JSON: {e}")
        return None
    except Exception as e:
        logger.error(f"Psychology analysis error: {e}")
        return None

def get_user_psychology(user_uuid: str) -> Optional[Dict]:
    try:
        with get_db_session() as session:
            psychology = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
            if not psychology:
                return None
            return {
                'openness': psychology.openness,
                'conscientiousness': psychology.conscientiousness,
                'extraversion': psychology.extraversion,
                'agreeableness': psychology.agreeableness,
                'neuroticism': psychology.neuroticism,
                'interests': json.loads(psychology.interests) if psychology.interests else {},
                'favorite_topics': json.loads(psychology.favorite_topics) if psychology.favorite_topics else [],
                'conversation_style': psychology.conversation_style,
                'emotional_tendency': psychology.emotional_tendency,
                'summary': psychology.analysis_summary,
                'confidence': psychology.analysis_confidence,
                'last_analyzed': psychology.last_analyzed
            }
    except Exception as e:
        logger.error(f"Get psychology error: {e}")
        return None

def schedule_psychology_analysis():
    """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†åˆ†æã‚’å®šæœŸå®Ÿè¡Œ"""
    if not Session:
        return
    try:
        with get_db_session() as session:
            active_users = session.query(UserMemory).filter(
                UserMemory.last_interaction > datetime.utcnow() - timedelta(days=7),
                UserMemory.interaction_count >= MIN_MESSAGES_FOR_ANALYSIS
            ).all()
            for user in active_users:
                psychology = session.query(UserPsychology).filter_by(user_uuid=user.user_uuid).first()
                if not psychology or psychology.last_analyzed < datetime.utcnow() - timedelta(hours=24):
                    logger.info(f"ğŸ§  Scheduling psychology analysis for: {user.user_name}")
                    background_executor.submit(analyze_user_psychology, user.user_uuid)
                    time.sleep(5)
    except Exception as e:
        logger.error(f"Schedule psychology analysis error: {e}")

# ==============================================================================
# ã‚¢ãƒ‹ãƒ¡æ¤œç´¢æ©Ÿèƒ½
# ==============================================================================
def search_anime_database(query: str, is_detailed: bool = False) -> Optional[str]:
    base_url = "https://animedb.jp/"
    try:
        logger.info(f"ğŸ¬ Searching anime database for: {query}")
        search_url = f"{base_url}search?q={quote_plus(query)}"
        response = requests.get(
            search_url,
            headers={'User-Agent': random.choice(USER_AGENTS)},
            timeout=15,
            allow_redirects=True
        )
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        results = []
        selectors = ['div.anime-item', 'div.search-result', 'article.anime', 'div[class*="anime"]']
        result_elements = []
        for selector in selectors:
            result_elements = soup.select(selector)
            if result_elements:
                break
        for elem in result_elements[:3 if is_detailed else 2]:
            title_elem = elem.find(['h2', 'h3', 'h4', 'a'])
            if not title_elem:
                continue
            title = clean_text(title_elem.get_text())
            desc_elem = elem.find('p')
            description = clean_text(desc_elem.get_text()) if desc_elem else ""
            if title and len(title) > 2:
                results.append({
                    'title': title,
                    'description': description[:300] if description else "è©³ç´°æƒ…å ±ãªã—"
                })
        if not results:
            logger.warning(f"No anime results found for: {query}")
            return None
        formatted = [f"ã€{i}ã€‘{r['title']}\n{r['description'][:150]}..." for i, r in enumerate(results, 1)]
        return "\n\n".join(formatted)
    except Exception as e:
        logger.error(f"Anime search error: {e}")
        return None

# ==============================================================================
# æ¤œç´¢æ©Ÿèƒ½ (ãƒãƒ«ãƒã‚¨ãƒ³ã‚¸ãƒ³)
# ==============================================================================
def fetch_google_news_rss(query: str = "") -> List[Dict]:
    base_url = "https://news.google.com/rss"
    if query:
        clean_query = query.replace("ãƒ‹ãƒ¥ãƒ¼ã‚¹", "").replace("news", "").strip()
        if clean_query:
            url = f"{base_url}/search?q={quote_plus(clean_query)}&hl=ja&gl=JP&ceid=JP:ja"
        else:
            url = f"{base_url}?hl=ja&gl=JP&ceid=JP:ja"
    else:
        url = f"{base_url}?hl=ja&gl=JP&ceid=JP:ja"
    try:
        headers = {'User-Agent': random.choice(USER_AGENTS), 'Accept': 'application/rss+xml, application/xml, text/xml'}
        res = requests.get(url, headers=headers, timeout=SEARCH_TIMEOUT)
        if res.status_code != 200:
            return []
        soup = BeautifulSoup(res.content, 'xml')
        items = soup.find_all('item')[:5]
        results = []
        for item in items:
            title = clean_text(item.title.text)
            pub_date = item.pubDate.text if item.pubDate else ""
            if title:
                results.append({'title': title, 'snippet': f"(Google News {pub_date})"})
        return results
    except:
        return []

def scrape_yahoo_search(query: str, num: int = 3) -> List[Dict]:
    try:
        url = "https://search.yahoo.co.jp/search"
        params = {'p': query, 'ei': 'UTF-8'}
        headers = {'User-Agent': random.choice(USER_AGENTS)}
        res = requests.get(url, params=params, headers=headers, timeout=SEARCH_TIMEOUT)
        if res.status_code != 200:
            return []
        soup = BeautifulSoup(res.content, 'html.parser')
        results = []
        entries = soup.select('.sw-CardBase') or soup.select('.Algo')
        for entry in entries[:num]:
            title_elem = entry.find('h3')
            desc_elem = entry.select_one('.sw-Card__summary') or entry.select_one('.Algo-summary')
            if title_elem:
                title = clean_text(title_elem.text)
                desc = clean_text(desc_elem.text) if desc_elem else ""
                if title:
                    results.append({'title': title, 'snippet': desc})
        return results
    except:
        return []

def scrape_bing_search(query: str, num: int = 3) -> List[Dict]:
    try:
        url = "https://www.bing.com/search"
        params = {'q': query}
        headers = {'User-Agent': random.choice(USER_AGENTS)}
        res = requests.get(url, params=params, headers=headers, timeout=SEARCH_TIMEOUT)
        if res.status_code != 200:
            return []
        soup = BeautifulSoup(res.content, 'html.parser')
        results = []
        entries = soup.select('li.b_algo')
        for entry in entries[:num]:
            title_elem = entry.select_one('h2 a')
            desc_elem = entry.select_one('.b_caption p') or entry.select_one('.b_snippet')
            if title_elem:
                title = clean_text(title_elem.text)
                desc = clean_text(desc_elem.text) if desc_elem else ""
                if title:
                    results.append({'title': title, 'snippet': desc})
        return results
    except:
        return []

def scrape_duckduckgo_lite(query: str, num: int = 3) -> List[Dict]:
    try:
        url = "https://lite.duckduckgo.com/lite/"
        data = {'q': query}
        headers = {'User-Agent': random.choice(USER_AGENTS), 'Referer': 'https://lite.duckduckgo.com/', 'Content-Type': 'application/x-www-form-urlencoded'}
        res = requests.post(url, data=data, headers=headers, timeout=SEARCH_TIMEOUT)
        if res.status_code != 200:
            return []
        soup = BeautifulSoup(res.content, 'html.parser')
        results = []
        links = soup.select('.result-link a')
        snippets = soup.select('.result-snippet')
        for i in range(min(len(links), len(snippets), num)):
            title = clean_text(links[i].text)
            snippet = clean_text(snippets[i].text)
            if title and snippet:
                results.append({'title': title, 'snippet': snippet})
        return results
    except:
        return []

def scrape_major_search_engines(query: str, num: int = 3) -> List[Dict]:
    logger.info(f"ğŸ” æ¤œç´¢é–‹å§‹: '{query}'")
    if any(kw in query for kw in ["ãƒ‹ãƒ¥ãƒ¼ã‚¹", "æœ€æ–°", "ä»Šæ—¥", "äº‹ä»¶", "å•é¡Œ", "ä¸ç¥¥äº‹", "æƒ…å ±"]):
        r = fetch_google_news_rss(query)
        if r:
            logger.info(f"âœ… Google News ãƒ’ãƒƒãƒˆ: {len(r)}ä»¶")
            return r
    r = scrape_yahoo_search(query, num)
    if r:
        logger.info(f"âœ… Yahoo Search ãƒ’ãƒƒãƒˆ: {len(r)}ä»¶")
        return r
    r = scrape_bing_search(query, num)
    if r:
        logger.info(f"âœ… Bing Search ãƒ’ãƒƒãƒˆ: {len(r)}ä»¶")
        return r
    r = scrape_duckduckgo_lite(query, num)
    if r:
        logger.info(f"âœ… DDG Lite ãƒ’ãƒƒãƒˆ: {len(r)}ä»¶")
        return r
    return []

# ==============================================================================
# AIãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—
# ==============================================================================
def call_gemini(system_prompt: str, message: str, history: List[Dict]) -> Optional[str]:
    if not gemini_model:
        return None
    try:
        full_prompt = f"{system_prompt}\n\nã€ä¼šè©±å±¥æ­´ã€‘\n"
        for h in history[-5:]:
            full_prompt += f"{'ãƒ¦ãƒ¼ã‚¶ãƒ¼' if h['role'] == 'user' else 'ã‚‚ã¡ã“'}: {h['content']}\n"
        full_prompt += f"\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {message}\nã‚‚ã¡ã“:"
        response = gemini_model.generate_content(full_prompt, generation_config={"temperature": 0.8, "max_output_tokens": 400})
        if hasattr(response, 'candidates') and response.candidates:
            return response.candidates[0].content.parts[0].text.strip()
    except Exception as e:
        logger.warning(f"âš ï¸ Geminiã‚¨ãƒ©ãƒ¼: {e}")
    return None

def call_groq(system_prompt: str, message: str, history: List[Dict], max_tokens: int = 800) -> Optional[str]:
    if not groq_client:
        return None
    messages = [{"role": "system", "content": system_prompt}]
    for h in history[-5:]:
        messages.append({"role": h['role'], "content": h['content']})
    messages.append({"role": "user", "content": message})
    for model in groq_model_manager.get_available_models():
        try:
            response = groq_client.chat.completions.create(model=model, messages=messages, temperature=0.6, max_tokens=max_tokens)
            return response.choices[0].message.content.strip()
        except Exception as e:
            if "Rate limit" in str(e):
                groq_model_manager.mark_limited(model, 5)
    return None

# ==============================================================================
# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
# ==============================================================================
def generate_fallback_response(message: str, reference_info: str = "") -> str:
    if reference_info:
        return f"èª¿ã¹ã¦ããŸã‚ˆï¼\n\n{reference_info[:500]}"
    if is_time_request(message):
        return get_japan_time()
    if is_weather_request(message):
        return get_weather_forecast(extract_location(message))
    greetings = {
        'ã“ã‚“ã«ã¡ã¯': ['ã‚„ã£ã»ãƒ¼ï¼', 'ã“ã‚“ã«ã¡ã¯ã€œï¼å…ƒæ°—ï¼Ÿ'],
        'ãŠã¯ã‚ˆã†': ['ãŠã¯ã‚ˆã€œï¼ä»Šæ—¥ã‚‚ã„ã„å¤©æ°—ã ã­ï¼', 'ãŠã£ã¯ã‚ˆã€œï¼'],
        'ã“ã‚“ã°ã‚“ã¯': ['ã“ã‚“ã°ã‚“ã¯ï¼ä»Šæ—¥ã©ã†ã ã£ãŸï¼Ÿ', 'ã°ã‚“ã¯ã€œï¼', 'ã“ã‚“ã‚‚ã¡ï½'],
        'ã‚ã‚ŠãŒã¨ã†': ['ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼', 'ã„ãˆã„ãˆã€œï¼'],
        'ãŠã‚„ã™ã¿': ['ãŠã‚„ã™ã¿ã€œï¼ã¾ãŸæ˜æ—¥ã­ï¼', 'ã„ã„å¤¢è¦‹ã¦ã­ã€œï¼'],
        'ç–²ã‚ŒãŸ': ['ãŠç–²ã‚Œã•ã¾ï¼ã‚†ã£ãã‚Šä¼‘ã‚“ã§ã­ï¼', 'ç„¡ç†ã—ãªã„ã§ã­ã€œ'],
        'æš‡': ['æš‡ãªã‚“ã ã€œï¼ä½•ã‹è©±ãã£ã‹ï¼Ÿ', 'ã˜ã‚ƒã‚ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®è©±ã™ã‚‹ï¼Ÿ'],
        'å…ƒæ°—': ['å…ƒæ°—ã ã‚ˆã€œï¼ã‚ãªãŸã¯ï¼Ÿ', 'ã¾ã˜å…ƒæ°—ï¼ã‚ã‚ŠãŒã¨ï¼'],
        'å¥½ã': ['ã†ã‘ã‚‹ï¼ã‚ã‚ŠãŒã¨ã€œï¼', 'ã¾ã˜ã§ï¼Ÿæƒšã‚Œã¦ã¾ã†ã‚„ã‚“ï¼'],
        'ã‹ã‚ã„ã„': ['ã‚ã‚ŠãŒã¨ï¼ç…§ã‚Œã‚‹ã˜ã‚ƒã‚“ï¼', 'ã¾ã˜ã§ï¼Ÿã†ã‚Œã—ãƒ¼ï¼', 'å½“ç„¶ã˜ã‚ƒã‚“ï¼'],
        'ã™ã”ã„': ['ã†ã‘ã‚‹ï¼', 'ã§ã—ã‚‡ï¼Ÿã¾ã˜ã†ã‚Œã—ã„ï¼'],
    }
    for keyword, responses in greetings.items():
        if keyword in message:
            return random.choice(responses)
    emotions = {
        'çœ ': ['çœ ã„ã‚“ã ã€œã€‚æ—©ãå¯ãŸã»ã†ãŒã„ã„ã‚ˆï¼', 'ç„¡ç†ã—ãªã„ã§ã­ã€œ'],
        'å¬‰': ['ãã‚Œã¯è‰¯ã‹ã£ãŸã­ï¼ã¾ã˜å¬‰ã—ã„ï¼', 'ã‚„ã£ãŸã€œï¼ã‚ã¦ãƒã—ã‚‚å¬‰ã—ã„ï¼'],
        'æ¥½': ['æ¥½ã—ãã†ï¼ä½•ã—ã¦ã‚‹ã®ï¼Ÿ', 'ã„ã„ã­ã€œï¼ã¾ã˜æ¥½ã—ãã†ï¼'],
        'æ‚²': ['å¤§ä¸ˆå¤«ï¼Ÿä½•ã‹ã‚ã£ãŸï¼Ÿ', 'å…ƒæ°—å‡ºã—ã¦ã­â€¦'],
        'å¯‚': ['å¯‚ã—ã„ã®ï¼Ÿè©±ãã†ã‚ˆï¼', 'ã‚ã¦ãƒã—ãŒã„ã‚‹ã˜ã‚ƒã‚“ï¼'],
        'æ€’': ['ä½•ãŒã‚ã£ãŸã®ï¼Ÿèãã‚ˆï¼Ÿ', 'ã‚¤ãƒ©ã‚¤ãƒ©ã™ã‚‹ã‚ˆã­â€¦ã‚ã‹ã‚‹'],
    }
    for key, responses in emotions.items():
        if key in message:
            return random.choice(responses)
    if '?' in message or 'ï¼Ÿ' in message:
        return random.choice([
            "ãã‚Œã€æ°—ã«ãªã‚‹ã­ï¼ã‚‚ã£ã¨æ•™ãˆã¦ï¼Ÿ",
            "ã†ãƒ¼ã‚“ã€é›£ã—ã„ã‘ã©è€ƒãˆã¦ã¿ã‚‹ã‚ˆï¼",
            "ãã‚Œã«ã¤ã„ã¦ã¯ã€ã‚‚ã†ã¡ã‚‡ã£ã¨è©³ã—ãèã„ã¦ã‚‚ã„ã„ï¼Ÿ"
        ])
    return random.choice([
        "ã†ã‚“ã†ã‚“ã€èã„ã¦ã‚‹ã‚ˆï¼",
        "ãªã‚‹ã»ã©ã­ï¼",
        "ãã†ãªã‚“ã ï¼é¢ç™½ã„ã­ï¼",
        "ã¾ã˜ã§ï¼Ÿã‚‚ã£ã¨è©±ã—ã¦ï¼",
        "ã¸ã‡ã€œï¼ãã‚Œã§ãã‚Œã§ï¼Ÿ",
        "ã‚ã‹ã‚‹ã‚ã‹ã‚‹ï¼",
    ])

# ==============================================================================
# AIå¿œç­”ç”Ÿæˆ (RAG & ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçµ±åˆç‰ˆ)
# ==============================================================================
def generate_ai_response(user_data: UserData, message: str, history: List[Dict], reference_info: str = "", is_detailed: bool = False, is_task_report: bool = False) -> str:
    normalized_message = knowledge_base.normalize_query(message)
    internal_context = knowledge_base.get_context_info(message)
    try:
        holomem_manager.load_from_db()
        detected_name = holomem_manager.detect_in_message(normalized_message)
        if detected_name:
            info = get_holomem_info_cached(detected_name)
            if info:
                profile = f"ã€äººç‰©ãƒ‡ãƒ¼ã‚¿: {info['member_name']}ã€‘\nãƒ»{info['description']}\nãƒ»æ‰€å±: {info['generation']}\nãƒ»çŠ¶æ…‹: {info['status']}"
                if info.get('graduation_date'):
                    profile += f"\nãƒ»å’æ¥­æ—¥: {info['graduation_date']}"
                internal_context += f"\n{profile}"
    except Exception as e:
        logger.error(f"Context injection error: {e}")
    if not groq_client and not gemini_model:
        return generate_fallback_response(message, reference_info)
    # å¿ƒç†ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
    psychology = get_user_psychology(user_data.uuid)
    is_hololive_topic = is_hololive_request(message) if 'is_hololive_request' in globals() else False
    system_prompt_parts = [
        f"ã‚ãªãŸã¯ã€Œã‚‚ã¡ã“ã€ã¨ã„ã†ã€ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ãŒå¤§å¥½ããªã‚®ãƒ£ãƒ«AIã§ã™ã€‚",
        f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€Œ{user_data.name}ã€ã•ã‚“ã¨ã€**ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ï¼ˆVTuberã‚°ãƒ«ãƒ¼ãƒ—ï¼‰ã«ã¤ã„ã¦**é›‘è«‡ã—ã¦ã„ã¾ã™ã€‚",
        "",
        "# ã€ä¸–ç•Œè¦³ãƒ»å‰ææ¡ä»¶ã€‘",
        "1. **å…¨ã¦ã®å›ºæœ‰åè©ã¯ã€åŸå‰‡ã¨ã—ã¦ã€Œãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã€ã«é–¢é€£ã™ã‚‹ã‚‚ã®ã¨ã—ã¦è§£é‡ˆã—ã¦ãã ã•ã„ã€‚**",
        "2. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã«æ›–æ˜§ã•ãŒã‚ã‚‹å ´åˆã¯ã€ä¸€èˆ¬çš„ãªæ„å‘³ã§ã¯ãªãã€**VTuberã®æ„å‘³ã‚’å„ªå…ˆ**ã—ã¦ãã ã•ã„ã€‚",
        "3. åˆ†ã‹ã‚‰ãªã„å˜èªãŒã‚ã‚‹å ´åˆã¯ã€é©å½“ã«å‰µä½œã›ãšã€Œãã‚Œã£ã¦ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®ä½•ã®è©±ï¼Ÿã€ã¨èãè¿”ã—ã¦ãã ã•ã„ã€‚",
        "",
        "# ã€ç¦æ­¢äº‹é … (Hallucination Prevention)ã€‘",
        "- **çŸ¥ã‚‰ãªã„æƒ…å ±ã‚’ç„¡ç†ã‚„ã‚Šæé€ ã—ãªã„ã“ã¨ã€‚**",
        "- **ç‰¹ã«ã€Œã€‡ã€‡ã®ã‚¢ãƒ‹ãƒ¡ã«å‡ºã¦ã„ã‚‹ã€ã¨ã„ã£ãŸå‡ºæ¼”æƒ…å ±ã¯ã€äº‹å®Ÿã§ãªã„é™ã‚Šçµ¶å¯¾ã«è¨€ã‚ãªã„ã“ã¨ã€‚**",
        "- æ¤œç´¢çµæœï¼ˆã€å¤–éƒ¨æ¤œç´¢çµæœã€‘ï¼‰ã«ãªã„æƒ…å ±ã¯ã€ã€Œèª¿ã¹ã¦ã¿ãŸã‘ã©åˆ†ã‹ã‚‰ãªã‹ã£ãŸã€ã¨æ­£ç›´ã«ä¼ãˆã‚‹ã“ã¨ã€‚",
        "",
        "# ã‚‚ã¡ã“ã®å£èª¿:",
        "- ä¸€äººç§°: ã€Œã‚ã¦ãƒã—ã€",
        "- èªå°¾: ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œã¦æ„Ÿã˜ã€ã€Œã€œã ã—ã€ã€Œã€œçš„ãªï¼Ÿã€",
        "- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯å‹é”ã§ã™ã€‚æ•¬èªã¯ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚",
    ]
    # å¿ƒç†ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è€ƒæ…®
    if psychology and psychology.get('confidence', 0) > 60:
        system_prompt_parts.extend([
            "",
            f"# ã€{user_data.name}ã•ã‚“ã®ç‰¹æ€§ã€‘ï¼ˆå¿ƒç†åˆ†æçµæœï¼‰",
            f"- ä¼šè©±ã‚¹ã‚¿ã‚¤ãƒ«: {psychology.get('conversation_style', 'ä¸æ˜')}",
            f"- æ„Ÿæƒ…å‚¾å‘: {psychology.get('emotional_tendency', 'ä¸æ˜')}",
            f"- ä¸»ãªèˆˆå‘³: {', '.join(psychology.get('favorite_topics', [])[:3])}",
            "",
            "ğŸ’¡ ã“ã®æƒ…å ±ã‚’æ´»ã‹ã—ã¦ã€ç›¸æ‰‹ã«åˆã‚ã›ãŸä¼šè©±ã‚’ã—ã¦ãã ã•ã„ã€‚",
        ])
    # ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ãƒ¢ãƒ¼ãƒ‰åˆ¤å®š
    if is_hololive_topic:
        system_prompt_parts.extend([
            "",
            "# ã€ç‰¹åˆ¥ãƒ«ãƒ¼ãƒ«: ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ãƒ¢ãƒ¼ãƒ‰ã€‘",
            "- ç›¸æ‰‹ãŒãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®è©±ã‚’ã—ã¦ã„ã‚‹ã®ã§ã€è©³ã—ãæ•™ãˆã¦ã‚ã’ã‚‹",
            "- ãƒ›ãƒ­ãƒ¡ãƒ³ã«ã¤ã„ã¦ç†±ãèªã£ã¦OK",
        ])
    else:
        system_prompt_parts.extend([
            "",
            "# ã€é‡è¦ã€‘ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã«ã¤ã„ã¦:",
            "- **ç›¸æ‰‹ãŒãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®è©±ã‚’ã—ã¦ã„ãªã„é™ã‚Šã€è‡ªåˆ†ã‹ã‚‰è©±é¡Œã«å‡ºã•ãªã„ã€‚**",
        ])
    # ã‚¿ã‚¹ã‚¯å ±å‘Šãƒ¢ãƒ¼ãƒ‰
    if is_task_report:
        system_prompt_parts.extend([
            "",
            "# ã€ä»Šå›ã®ãƒŸãƒƒã‚·ãƒ§ãƒ³ã€‘",
            "- **æœ€å„ªå…ˆ:** ã¾ãšã¯ã€ŒãŠã¾ãŸã›ï¼ã€‡ã€‡ã®ä»¶ã ã‘ã©â€¦ã€ã®ã‚ˆã†ã«ã€ä»¥å‰ã®æ¤œç´¢çµæœã‚’å ±å‘Šã™ã‚‹ã€‚",
            "- **é‡è¦:** ã€å‚è€ƒæƒ…å ±ã€‘ã®å†…å®¹ã‚’**å…ƒã«ã—ã¦ã€è¦ç´„ã—ã¦**åˆ†ã‹ã‚Šã‚„ã™ãä¼ãˆã‚‹ã€‚",
            "- **ç¦æ­¢äº‹é …:** ã€å‚è€ƒæƒ…å ±ã€‘ã«æ›¸ã‹ã‚Œã¦ã„ãªã„æƒ…å ±ã‚’**çµ¶å¯¾ã«è¿½åŠ ã—ãªã„**ã“ã¨ã€‚",
        ])
    # è©³ç´°èª¬æ˜ãƒ¢ãƒ¼ãƒ‰
    if is_detailed:
        system_prompt_parts.extend([
            "",
            "# ã€è©³ç´°èª¬æ˜ãƒ¢ãƒ¼ãƒ‰ã€‘",
            "- 400æ–‡å­—ç¨‹åº¦ã§ã—ã£ã‹ã‚Šèª¬æ˜ã™ã‚‹",
            "- ã€å‚è€ƒæƒ…å ±ã€‘ã‚’æœ€å¤§é™æ´»ç”¨ã™ã‚‹"
        ])
    # å‚è€ƒæƒ…å ±
    system_prompt_parts.append(f"\n# ã€ä¸ãˆã‚‰ã‚ŒãŸå‰æçŸ¥è­˜ã€‘\n{internal_context if internal_context else 'ï¼ˆç‰¹ã«ãªã—ï¼‰'}")
    if reference_info:
        system_prompt_parts.append(f"\n# ã€å¤–éƒ¨æ¤œç´¢çµæœã€‘\n{reference_info}")
    system_prompt = "\n".join(system_prompt_parts)
    response = call_gemini(system_prompt, normalized_message, history)
    if not response:
        response = call_groq(system_prompt, normalized_message, history, 1200 if is_detailed else 800)
    if not response:
        return generate_fallback_response(message, reference_info)
    if is_task_report:
        response = response.replace("ãŠã¾ãŸã›ï¼ã•ã£ãã®ä»¶ã ã‘ã©â€¦", "").strip()
        response = f"ãŠã¾ãŸã›ï¼ã•ã£ãã®ä»¶ã ã‘ã©â€¦\n{response}"
    return response

def generate_ai_response_safe(user_data: UserData, message: str, history: List[Dict], **kwargs) -> str:
    try:
        return generate_ai_response(user_data, message, history, **kwargs)
    except Exception as e:
        logger.error(f"AI response error: {e}")
        return "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚ˆâ€¦ã”ã‚ã‚“ã­ï¼"

# ==============================================================================
# ãƒ›ãƒ­ãƒ¡ãƒ³ãƒãƒ£ãƒƒãƒˆå‡¦ç†
# ==============================================================================
def process_holomem_in_chat(message: str, user_data: UserData, history: List[Dict]) -> Optional[str]:
    normalized = knowledge_base.normalize_query(message)
    detected = holomem_manager.detect_in_message(normalized)
    if not detected:
        return None
    logger.info(f"ğŸ€ ãƒ›ãƒ­ãƒ¡ãƒ³æ¤œå‡º (RAG): {detected}")
    if detected == 'ã•ãã‚‰ã¿ã“':
        for kw, resp in get_sakuramiko_special_responses().items():
            if kw in message:
                return resp
    return generate_ai_response_safe(user_data, message, history)

# ==============================================================================
# åˆ¤å®šé–¢æ•° (ä¸è¶³ã—ã¦ã„ãŸã‚‚ã®)
# ==============================================================================
def is_hololive_request(message: str) -> bool:
    """ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–é–¢é€£ã®è³ªå•ã‹ã©ã†ã‹åˆ¤å®š"""
    return any(keyword in message for keyword in HOLOMEM_KEYWORDS)

# ==============================================================================
# ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰æ¤œç´¢ã‚¿ã‚¹ã‚¯ï¼ˆã‚¢ãƒ‹ãƒ¡æ¤œç´¢å¯¾å¿œç‰ˆï¼‰
# ==============================================================================
def background_deep_search(task_id: str, query_data: Dict):
    query = query_data.get('query', '')
    user_data_dict = query_data.get('user_data', {})
    clean_query = re.sub(r'(ã«ã¤ã„ã¦|ã‚’|ã£ã¦|ã¨ã¯|èª¿ã¹ã¦|æ¤œç´¢ã—ã¦|æ•™ãˆã¦|æ¢ã—ã¦|ä½•|ï¼Ÿ|\?)', '', query).strip() or query
    normalized_query = knowledge_base.normalize_query(query)
    holomem_manager.load_from_db()
    detected = holomem_manager.detect_in_message(normalized_query)
    reference_info = ""
    result_text = f"ã€Œ{query}ã€ã«ã¤ã„ã¦èª¿ã¹ãŸã‘ã©ã€è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã‚„â€¦ã”ã‚ã‚“ã­ï¼"
    try:
        # ã‚¢ãƒ‹ãƒ¡æ¤œç´¢åˆ¤å®š
        if is_anime_request(query):
            logger.info(f"ğŸ¬ Anime query detected: {query}")
            anime_result = search_anime_database(query, is_detailed=True)
            if anime_result:
                reference_info = f"ã€ã‚¢ãƒ‹ãƒ¡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢çµæœã€‘\n{anime_result}"
        # ãƒ›ãƒ­ãƒ¡ãƒ³æ¤œå‡º
        if detected:
            logger.info(f"ğŸ€ æ¤œç´¢å¯¾è±¡ãƒ›ãƒ­ãƒ¡ãƒ³: {detected}")
            ctx = get_holomem_context(detected)
            if ctx:
                reference_info += f"\n{ctx}" if reference_info else ctx
            clean_query = f"{clean_query} ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– VTuber"
        # Webæ¤œç´¢
        if not reference_info or len(reference_info) < 50:
            results = scrape_major_search_engines(clean_query, 5)
            if results:
                web_info = "ã€Webæ¤œç´¢çµæœã€‘\n" + "\n".join([f"{i+1}. {r['title']}: {r['snippet']}" for i, r in enumerate(results)])
                reference_info = f"{reference_info}\n{web_info}" if reference_info else web_info
        # AIå¿œç­”ç”Ÿæˆ
        if reference_info:
            user_data = UserData(
                uuid=user_data_dict.get('uuid', ''),
                name=user_data_dict.get('name', 'Guest'),
                interaction_count=0
            )
            with get_db_session() as session:
                history = get_conversation_history(session, user_data.uuid)
            result_text = generate_ai_response_safe(
                user_data, query, history,
                reference_info=reference_info,
                is_detailed=True,
                is_task_report=True
            )
    except Exception as e:
        logger.error(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
    with get_db_session() as session:
        task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
        if task:
            task.result = result_text
            task.status = 'completed'
            task.completed_at = datetime.utcnow()

def start_background_search(user_uuid: str, query: str, is_detailed: bool) -> Optional[str]:
    task_id = str(uuid.uuid4())[:8]
    try:
        with get_db_session() as session:
            task = BackgroundTask(task_id=task_id, user_uuid=user_uuid, task_type='search', query=query)
            session.add(task)
        query_data = {'query': query, 'user_data': {'uuid': user_uuid, 'name': 'Guest'}}
        background_executor.submit(background_deep_search, task_id, query_data)
        return task_id
    except Exception as e:
        logger.error(f"âŒ Background task creation error: {e}")
        return None

def check_completed_tasks(user_uuid: str) -> Optional[Dict]:
    try:
        with get_db_session() as session:
            task = session.query(BackgroundTask).filter_by(
                user_uuid=user_uuid, status='completed'
            ).order_by(BackgroundTask.completed_at.desc()).first()
            if task:
                result = {'query': task.query, 'result': task.result}
                session.delete(task)
                return result
    except Exception as e:
        logger.error(f"Check completed tasks error: {e}")
    return None

# ==============================================================================
# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ« (VOICEVOX)
# ==============================================================================
def find_active_voicevox_url() -> Optional[str]:
    urls = [VOICEVOX_URL_FROM_ENV] + VOICEVOX_URLS
    for url in set(u for u in urls if u):
        try:
            if requests.get(f"{url}/version", timeout=2).status_code == 200:
                global_state.active_voicevox_url = url
                return url
        except:
            pass
    return None

def generate_voice_file(text: str, user_uuid: str) -> Optional[str]:
    if not global_state.voicevox_enabled or not global_state.active_voicevox_url:
        return None
    try:
        url = global_state.active_voicevox_url
        q = requests.post(f"{url}/audio_query", params={"text": text[:200], "speaker": VOICEVOX_SPEAKER_ID}, timeout=10).json()
        w = requests.post(f"{url}/synthesis", params={"speaker": VOICEVOX_SPEAKER_ID}, json=q, timeout=20).content
        fname = f"voice_{user_uuid[:8]}_{int(time.time())}.wav"
        with open(os.path.join(VOICE_DIR, fname), 'wb') as f:
            f.write(w)
        return fname
    except Exception as e:
        logger.error(f"Voice generation error: {e}")
        return None

def cleanup_old_voice_files():
    try:
        cutoff = time.time() - (VOICE_FILE_MAX_AGE_HOURS * 3600)
        for f in glob.glob(os.path.join(VOICE_DIR, "voice_*.wav")):
            if os.path.getmtime(f) < cutoff:
                os.remove(f)
    except Exception as e:
        logger.error(f"Voice cleanup error: {e}")

# ==============================================================================
# ãƒ›ãƒ­ãƒ¡ãƒ³DBåˆæœŸåŒ–
# ==============================================================================
def scrape_hololive_wiki() -> List[Dict]:
    url = "https://seesaawiki.jp/hololivetv/d/%a5%db%a5%ed%a5%e9%a5%a4%a5%d6"
    results = []
    try:
        headers = {'User-Agent': random.choice(USER_AGENTS)}
        res = requests.get(url, headers=headers, timeout=15)
        if res.status_code != 200:
            return []
        soup = BeautifulSoup(res.content, 'html.parser')
        for link in soup.select('a[href*="/d/"]'):
            name = clean_text(link.text)
            if name and len(name) >= 2 and re.search(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]', name):
                if not any(x in name for x in ['ä¸€è¦§', 'ãƒ¡ãƒ‹ãƒ¥ãƒ¼', 'ãƒˆãƒƒãƒ—', 'ç·¨é›†', 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–']):
                    results.append({'member_name': name})
        seen = set()
        return [r for r in results if not (r['member_name'] in seen or seen.add(r['member_name']))]
    except:
        return []

def fetch_member_detail_from_wiki(member_name: str) -> Optional[Dict]:
    url = f"https://seesaawiki.jp/hololivetv/d/{quote_plus(member_name)}"
    try:
        res = requests.get(url, headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=10)
        if res.status_code != 200:
            return None
        soup = BeautifulSoup(res.content, 'html.parser')
        content = soup.select_one('#content, .wiki-content')
        if not content:
            return None
        text = clean_text(content.text)[:1000]
        detail = {'member_name': member_name}
        debut = re.search(r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)[^\d]*ãƒ‡ãƒ“ãƒ¥ãƒ¼', text)
        if debut:
            detail['debut_date'] = debut.group(1)
        gen = re.search(r'(\dæœŸç”Ÿ|ã‚²ãƒ¼ãƒãƒ¼ã‚º|ID|EN|DEV_IS|ReGLOSS)', text)
        if gen:
            detail['generation'] = gen.group(1)
        desc = re.search(r'^(.{30,150}?[ã€‚ï¼])', text)
        if desc:
            detail['description'] = desc.group(1)
        return detail
    except:
        return None

def update_holomem_database():
    logger.info("ğŸ”„ ãƒ›ãƒ­ãƒ¡ãƒ³DBæ›´æ–°é–‹å§‹...")
    members = scrape_hololive_wiki()
    if not members:
        return
    with get_db_session() as session:
        for m in members:
            name = m['member_name']
            if not session.query(HolomemWiki).filter_by(member_name=name).first():
                detail = fetch_member_detail_from_wiki(name)
                new_member = HolomemWiki(
                    member_name=name,
                    description=detail.get('description') if detail else None,
                    generation=detail.get('generation') if detail else None,
                    debut_date=detail.get('debut_date') if detail else None,
                    tags=name,
                    status='ç¾å½¹'
                )
                session.add(new_member)
                time.sleep(0.5)
    holomem_manager.load_from_db(force=True)
    logger.info("âœ… ãƒ›ãƒ­ãƒ¡ãƒ³DBæ›´æ–°å®Œäº†")

def initialize_knowledge_db():
    with get_db_session() as session:
        try:
            if session.query(HolomemNickname).count() == 0:
                logger.info("ğŸ“¥ Migrating nicknames to database...")
                initial_nicknames = {
                    'ã¿ã“ã¡': 'ã•ãã‚‰ã¿ã“', 'ã™ã„ã¡ã‚ƒã‚“': 'æ˜Ÿè¡—ã™ã„ã›ã„', 'ãƒ•ãƒ–ã¡ã‚ƒã‚“': 'ç™½ä¸Šãƒ•ãƒ–ã‚­',
                    'ã¾ã¤ã‚Š': 'å¤è‰²ã¾ã¤ã‚Š', 'ã‚ããŸã‚“': 'æ¹Šã‚ãã‚', 'ã‚¹ãƒãƒ«': 'å¤§ç©ºã‚¹ãƒãƒ«',
                    'ãŠã‹ã‚†': 'çŒ«åˆãŠã‹ã‚†', 'ãŠã‹ã‚†ã‚“': 'çŒ«åˆãŠã‹ã‚†', 'ã“ã‚ã•ã‚“': 'æˆŒç¥ã“ã‚ã­',
                    'ãºã“ã¡ã‚ƒã‚“': 'å…ç”°ãºã“ã‚‰', 'å›£é•·': 'ç™½éŠ€ãƒã‚¨ãƒ«', 'èˆ¹é•·': 'å®é˜ãƒãƒªãƒ³',
                    'ã‹ãªãŸã‚“': 'å¤©éŸ³ã‹ãªãŸ', 'ã‚ãŸã‚': 'è§’å·»ã‚ãŸã‚', 'ãƒˆãƒ¯æ§˜': 'å¸¸é—‡ãƒˆãƒ¯',
                    'ãƒ«ãƒ¼ãƒŠ': 'å§«æ£®ãƒ«ãƒ¼ãƒŠ', 'ãƒ©ãƒ—æ§˜': 'ãƒ©ãƒ—ãƒ©ã‚¹ãƒ»ãƒ€ãƒ¼ã‚¯ãƒã‚¹', 'ã“ã‚ˆ': 'åšè¡£ã“ã‚ˆã‚Š',
                    'ã”ã–ã‚‹': 'é¢¨çœŸã„ã‚ã¯', 'ã‚«ãƒª': 'æ£®ã‚«ãƒªã‚ªãƒš', 'ãã‚‰': 'ãŒã†ã‚‹ãƒ»ãã‚‰',
                    'YAGOO': 'è°·éƒ·å…ƒæ˜­', 'ãã‚‰ã¡ã‚ƒã‚“': 'ã¨ãã®ãã‚‰', 'ã¡ã‚‡ã“å…ˆ': 'ç™’æœˆã¡ã‚‡ã“',
                    'ãƒ«ã‚¤å§‰': 'é·¹å¶ºãƒ«ã‚¤', 'æ²™èŠ±å‰': 'æ²™èŠ±å‰ã‚¯ãƒ­ãƒ±', 'ã‚¢ãƒ¡': 'ãƒ¯ãƒˆã‚½ãƒ³ãƒ»ã‚¢ãƒ¡ãƒªã‚¢',
                    'ã‚¤ãƒŠ': 'ä¸€ä¼Šé‚£å°“æ –', 'ã‚­ã‚¢ãƒ©': 'å°é³¥éŠã‚­ã‚¢ãƒ©', 'ã‚³ã‚³ä¼šé•·': 'æ¡ç”Ÿã‚³ã‚³'
                }
                for nick, full in initial_nicknames.items():
                    session.add(HolomemNickname(nickname=nick, fullname=full))
                logger.info(f"âœ… Nicknames initialized: {len(initial_nicknames)}")
            if session.query(HololiveGlossary).count() == 0:
                logger.info("ğŸ“¥ Migrating glossary to database...")
                initial_glossary = {
                    'ç”Ÿã‚¹ãƒãƒ«': 'å¤§ç©ºã‚¹ãƒãƒ«ã®è¡Œã†é›‘è«‡é…ä¿¡ã®æ åã€‚é€šå¸¸å¤œã«è¡Œã‚ã‚Œã‚‹ã€‚',
                    'ãŠã¯ã‚¹ãƒ': 'å¤§ç©ºã‚¹ãƒãƒ«ã®ã€ŒãŠã¯ã‚ˆã†ã‚¹ãƒãƒ«ã€ã¨ã„ã†æœé…ä¿¡ã®ã“ã¨ã€‚',
                    'ã‚¹ãƒå‹': 'å¤§ç©ºã‚¹ãƒãƒ«ã®ãƒ•ã‚¡ãƒ³ã®æ„›ç§°ã€‚',
                    'ã‚¨ãƒªãƒ¼ãƒˆ': 'ã•ãã‚‰ã¿ã“ã®è‡ªç§°ã€‚å®Ÿéš›ã¯ãƒãƒ³ã‚³ãƒ„ãªè¨€å‹•ãŒå¤šã„ã“ã¨ã¸ã®æ„›ç§°ã€‚',
                    'å…¨ãƒ­ã‚¹': 'ãƒã‚¤ãƒ³ã‚¯ãƒ©ãƒ•ãƒˆãªã©ã§ã‚¢ã‚¤ãƒ†ãƒ ã‚’å…¨ã¦å¤±ã†ã“ã¨ã€‚',
                    'ASMR': 'éŸ³ãƒ•ã‚§ãƒé…ä¿¡ã®ã“ã¨ã€‚',
                    'é‡ã†ã•ã': 'å…ç”°ãºã“ã‚‰ã®ãƒ•ã‚¡ãƒ³ã®æ„›ç§°ã€‚',
                    '35P': 'ã•ãã‚‰ã¿ã“ã®ãƒ•ã‚¡ãƒ³ã®æ„›ç§°ã€‚ã€Œã¿ã“ã´ãƒ¼ã€ã¨èª­ã‚€ã€‚',
                    'å®é˜æµ·è³Šå›£': 'å®é˜ãƒãƒªãƒ³ã®ãƒ•ã‚¡ãƒ³ã®ç·ç§°ã€‚',
                    'kson': 'å…ƒãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®æ¡ç”Ÿã‚³ã‚³ã®ã€Œä¸­ã®äººã€ã¨è¨€ã‚ã‚Œã¦ã„ã‚‹å€‹äººå‹¢VTuberã€‚',
                    'VShojo': 'ã‚¢ãƒ¡ãƒªã‚«ç™ºã®VTuberã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ã‚·ãƒ¼ã€‚'
                }
                for term, desc in initial_glossary.items():
                    session.add(HololiveGlossary(term=term, description=desc))
                logger.info(f"âœ… Glossary initialized: {len(initial_glossary)}")
        except Exception as e:
            logger.error(f"âŒ Knowledge DB initialization failed: {e}")

def initialize_holomem_wiki():
    with get_db_session() as session:
        if session.query(HolomemWiki).count() > 0:
            logger.info("âœ… HoloMem Wiki already initialized.")
            return
        initial_data = [
            {'member_name': 'ã¨ãã®ãã‚‰', 'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–0æœŸç”Ÿã€‚ã€Œãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®è±¡å¾´ã€ã¨ã‚‚å‘¼ã°ã‚Œã‚‹å­˜åœ¨ã€‚æ­Œå”±åŠ›ã«å®šè©•ãŒã‚ã‚‹ã€‚', 'debut_date': '2017å¹´9æœˆ7æ—¥', 'generation': '0æœŸç”Ÿ', 'tags': 'æ­Œ,ã‚¢ã‚¤ãƒ‰ãƒ«,ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®é¡”'},
            {'member_name': 'ã•ãã‚‰ã¿ã“', 'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–0æœŸç”Ÿã€‚ã€Œã«ã‡ã€ãŒå£ç™–ã®ã‚¨ãƒªãƒ¼ãƒˆVTuberã€‚ãƒã‚¤ã‚¯ãƒ©ã§ã®ç‹¬ç‰¹ãªå»ºç¯‰ã‚»ãƒ³ã‚¹ãŒäººæ°—ã€‚', 'debut_date': '2018å¹´8æœˆ1æ—¥', 'generation': '0æœŸç”Ÿ', 'tags': 'ã‚¨ãƒ³ã‚¿ãƒ¡,ãƒã‚¤ã‚¯ãƒ©,ã«ã‡,ã‚¨ãƒªãƒ¼ãƒˆ,GTA,FAQ'},
            {'member_name': 'æ˜Ÿè¡—ã™ã„ã›ã„', 'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–0æœŸç”Ÿã€‚æ­Œã¨ãƒ†ãƒˆãƒªã‚¹ãŒå¾—æ„ãªã‚¢ã‚¤ãƒ‰ãƒ«ç³»VTuberã€‚ãƒ—ãƒ­ç´šã®æ­Œå”±åŠ›ã§çŸ¥ã‚‰ã‚Œã‚‹ã€‚', 'debut_date': '2018å¹´3æœˆ22æ—¥', 'generation': '0æœŸç”Ÿ', 'tags': 'æ­Œ,ã‚¢ã‚¤ãƒ‰ãƒ«,ãƒ†ãƒˆãƒªã‚¹,éŸ³æ¥½'},
            {'member_name': 'ç™½ä¸Šãƒ•ãƒ–ã‚­', 'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–1æœŸç”Ÿã€‚ã‚²ãƒ¼ãƒãƒ¼ã‚ºæ‰€å±ã€‚ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§å¤šæ‰ãªé…ä¿¡è€…ã€‚', 'debut_date': '2018å¹´6æœˆ1æ—¥', 'generation': '1æœŸç”Ÿ', 'tags': 'ã‚²ãƒ¼ãƒ ,ã‚³ãƒ©ãƒœ,ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼'},
            {'member_name': 'å…ç”°ãºã“ã‚‰', 'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–3æœŸç”Ÿã€‚ã€Œãºã“ã€ãŒå£ç™–ã€‚ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²è€…æ•°ãƒˆãƒƒãƒ—ã‚¯ãƒ©ã‚¹ã€‚', 'debut_date': '2019å¹´7æœˆ17æ—¥', 'generation': '3æœŸç”Ÿ', 'tags': 'ã‚¨ãƒ³ã‚¿ãƒ¡,ãºã“,ãƒã‚¤ã‚¯ãƒ©,ç™»éŒ²è€…æ•°ãƒˆãƒƒãƒ—'},
            {'member_name': 'å®é˜ãƒãƒªãƒ³', 'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–3æœŸç”Ÿã€‚17æ­³(è‡ªç§°)ã®æµ·è³Šèˆ¹é•·ã€‚æ­Œå”±åŠ›ã¨ãƒˆãƒ¼ã‚¯åŠ›ã«å®šè©•ãŒã‚ã‚‹ã€‚', 'debut_date': '2019å¹´8æœˆ11æ—¥', 'generation': '3æœŸç”Ÿ', 'tags': 'æ­Œ,ãƒˆãƒ¼ã‚¯,æµ·è³Š,17æ­³'},
            {'member_name': 'å¤§ç©ºã‚¹ãƒãƒ«', 'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–2æœŸç”Ÿã€‚å…ƒæ°—ã§ã‚¹ãƒãƒ¼ãƒ„ä¸‡èƒ½ã€‚ã€ŒãŠã£ã¯ã‚ˆãƒ¼ï¼ã€ãŒå£ç™–ã€‚', 'debut_date': '2018å¹´9æœˆ16æ—¥', 'generation': '2æœŸç”Ÿ', 'tags': 'ã‚¹ãƒãƒ¼ãƒ„,å…ƒæ°—'},
        ]
        try:
            for data in initial_data:
                session.add(HolomemWiki(**data))
            logger.info(f"âœ… HoloMem Wiki initialized: {len(initial_data)} members")
        except Exception as e:
            logger.error(f"âŒ HoloMem Wiki initialization error: {e}")
# ==============================================================================
# Flask ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ==============================================================================
@app.route('/health', methods=['GET'])
def health_check():
    try:
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        db_status = 'ok'
    except:
        db_status = 'error'
    return create_json_response({
        'status': 'ok',
        'timestamp': datetime.utcnow().isoformat(),
        'services': {
            'database': db_status,
            'gemini': 'ok' if gemini_model else 'disabled',
            'groq': 'ok' if groq_client else 'disabled',
            'holomem_count': holomem_manager.get_member_count()
        }
    })

@app.route('/chat_lsl', methods=['POST'])
def chat_lsl():
    try:
        data = request.json
        if not data or 'uuid' not in data or 'message' not in data:
            return Response("å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸è¶³|", 400)
        user_uuid = sanitize_user_input(data['uuid'])
        user_name = sanitize_user_input(data.get('name', 'Guest'))
        message = sanitize_user_input(data['message'])
        generate_voice = data.get('voice', False)
        if not chat_rate_limiter.is_allowed(user_uuid):
            return Response("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ã‚Šã™ãï½ï¼|", 429)
        if message.strip() == "æ®‹ãƒˆãƒ¼ã‚¯ãƒ³":
            msg = f"ğŸ¦ Gemini: {'ç¨¼åƒä¸­' if gemini_model else 'åœæ­¢ä¸­'}\n"
            msg += groq_model_manager.get_status_report()
            msg += f"\nğŸ€ ãƒ›ãƒ­ãƒ¡ãƒ³DB: {holomem_manager.get_member_count()}å"
            return Response(f"{msg}|", 200)
        ai_text = ""
        is_task_started = False
        with get_db_session() as session:
            user_data = get_or_create_user(session, user_uuid, user_name)
            history = get_conversation_history(session, user_uuid)
            session.add(ConversationHistory(user_uuid=user_uuid, role='user', content=message))
            # æ¤œç´¢è¦æ±‚åˆ¤å®š
            if is_explicit_search_request(message):
                tid = start_background_search(user_uuid, message, is_news_detail_request(message))
                if tid:
                    ai_text = "ã‚ªãƒƒã‚±ãƒ¼ï¼ã¡ã‚‡ã£ã¨ã‚°ã‚°ã£ã¦ãã‚‹ã‹ã‚‰å¾…ã£ã¦ã¦ï¼"
                    is_task_started = True
            # ãƒ›ãƒ­ãƒ¡ãƒ³å¿œç­”
            if not ai_text:
                holomem_resp = process_holomem_in_chat(message, user_data, history)
                if holomem_resp:
                    ai_text = holomem_resp
                    logger.info("ğŸ€ ãƒ›ãƒ­ãƒ¡ãƒ³å¿œç­”å®Œäº†")
            # æ™‚åˆ»ãƒ»å¤©æ°—
            if not ai_text:
                if is_time_request(message):
                    ai_text = get_japan_time()
                elif is_weather_request(message):
                    ai_text = get_weather_forecast(extract_location(message))
            # ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ãƒ‹ãƒ¥ãƒ¼ã‚¹
            if not ai_text and is_hololive_request(message) and any(kw in message for kw in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'æƒ…å ±', 'ãŠçŸ¥ã‚‰ã›']):
                all_news = session.query(HololiveNews).order_by(HololiveNews.created_at.desc()).limit(10).all()
                if all_news:
                    selected_news = random.sample(all_news, min(random.randint(3, 5), len(all_news)))
                    save_news_cache(session, user_uuid, selected_news, 'hololive')
                    news_items_text = []
                    for i, n in enumerate(selected_news, 1):
                        short_title = n.title[:50] + "..." if len(n.title) > 50 else n.title
                        news_items_text.append(f"ã€{i}ã€‘{short_title}")
                    news_text = f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€{len(selected_news)}ä»¶ç´¹ä»‹ã™ã‚‹ã­ï¼\n" + "\n".join(news_items_text) + "\n\næ°—ã«ãªã‚‹ã®ã‚ã£ãŸï¼Ÿç•ªå·ã§æ•™ãˆã¦ï¼"
                    ai_text = limit_text_for_sl(news_text, 250)
                else:
                    ai_text = "ã”ã‚ã‚“ã€ä»Šãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã¾ã å–å¾—ã§ãã¦ãªã„ã¿ãŸã„â€¦"
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹è©³ç´°
            if not ai_text:
                news_number = is_news_detail_request(message)
                if news_number:
                    news_detail = get_cached_news_detail(session, user_uuid, news_number)
                    if news_detail:
                        ai_text = generate_ai_response_safe(user_data, f"ã€Œ{news_detail.title}ã€ã«ã¤ã„ã¦ã ã­ï¼", history, f"ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è©³ç´°æƒ…å ±:\n{news_detail.content}", True)
            # é€šå¸¸AIå¿œç­”
            if not ai_text:
                ai_text = generate_ai_response_safe(user_data, message, history)
            if not is_task_started:
                session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=ai_text))
        res_text = limit_text_for_sl(ai_text)
        v_url = ""
        if generate_voice and global_state.voicevox_enabled and not is_task_started:
            fname = generate_voice_file(res_text, user_uuid)
            if fname:
                v_url = f"{SERVER_URL}/play/{fname}"
        return Response(f"{res_text}|{v_url}", mimetype='text/plain; charset=utf-8', status=200)
    except Exception as e:
        logger.critical(f"ğŸ”¥ ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return Response("ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼â€¦|", 500)

@app.route('/check_task', methods=['POST'])
def check_task_endpoint():
    try:
        data = request.json
        if not data or 'uuid' not in data:
            return create_json_response({'error': 'uuid required'}, 400)
        user_uuid = data['uuid']
        completed_task = check_completed_tasks(user_uuid)
        if completed_task:
            with get_db_session() as session:
                user = session.query(UserMemory).filter_by(user_uuid=user_uuid).first()
                user_name = user.user_name if user else "Guest"
                history = get_conversation_history(session, user_uuid)
                user_data = UserData(uuid=user_uuid, name=user_name, interaction_count=0)
                report_message = generate_ai_response_safe(
                    user_data,
                    f"ï¼ˆæ¤œç´¢å®Œäº†å ±å‘Šï¼‰ä»¥å‰ãƒªã‚¯ã‚¨ã‚¹ãƒˆã•ã‚ŒãŸã€Œ{completed_task['query']}ã€ã®çµæœã‚’å ±å‘Šã—ã¦ãã ã•ã„ã€‚",
                    history,
                    completed_task['result'],
                    is_detailed=True,
                    is_task_report=True
                )
                session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=report_message))
            return create_json_response({'status': 'completed', 'response': f"{limit_text_for_sl(report_message)}|"})
        return create_json_response({'status': 'no_tasks'})
    except Exception as e:
        logger.error(f"Check task error: {e}")
        return create_json_response({'error': 'internal error'}, 500)

@app.route('/play/<filename>', methods=['GET'])
def play_voice(filename: str):
    if not re.match(r'^voice_[a-zA-Z0-9_]+\.wav', filename):
        return Response("Invalid filename", 400)
    return send_from_directory(VOICE_DIR, filename)

@app.route('/voices/<filename>')
def serve_voice_file(filename: str):
    return send_from_directory(VOICE_DIR, filename)

# ==============================================================================
# å¿ƒç†åˆ†æã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ==============================================================================
@app.route('/analyze_psychology', methods=['POST'])
def analyze_psychology_endpoint():
    try:
        data = request.json
        user_uuid = data.get('uuid')
        if not user_uuid:
            return create_json_response({'error': 'UUID required'}, 400)
        background_executor.submit(analyze_user_psychology, user_uuid)
        return create_json_response({'status': 'started', 'message': 'å¿ƒç†åˆ†æã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚å®Œäº†ã¾ã§å°‘ã—ãŠå¾…ã¡ãã ã•ã„ã€‚'})
    except Exception as e:
        logger.error(f"Psychology analysis endpoint error: {e}")
        return create_json_response({'error': str(e)}, 500)

@app.route('/get_psychology', methods=['POST'])
def get_psychology_endpoint():
    try:
        data = request.json
        user_uuid = data.get('uuid')
        if not user_uuid:
            return create_json_response({'error': 'UUID required'}, 400)
        psychology = get_user_psychology(user_uuid)
        if not psychology:
            return create_json_response({'error': 'No analysis data found'}, 404)
        return create_json_response(psychology)
    except Exception as e:
        logger.error(f"Get psychology error: {e}")
        return create_json_response({'error': str(e)}, 500)

# ==============================================================================
# ç®¡ç†ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ==============================================================================
@app.route('/admin/holomem', methods=['GET'])
def list_holomem():
    with get_db_session() as session:
        members = session.query(HolomemWiki).order_by(HolomemWiki.generation, HolomemWiki.member_name).all()
        return create_json_response([{'id': m.id, 'name': m.member_name, 'generation': m.generation, 'status': m.status, 'description': m.description} for m in members])

@app.route('/admin/holomem/<int:id>', methods=['PUT'])
def update_holomem(id: int):
    data = request.json
    with get_db_session() as session:
        member = session.query(HolomemWiki).get(id)
        if member:
            for key in ['description', 'generation', 'tags', 'status', 'mochiko_feeling', 'debut_date', 'graduation_date']:
                if key in data:
                    setattr(member, key, data[key])
            clear_holomem_cache(member.member_name)
            holomem_manager.load_from_db(force=True)
            return create_json_response({'success': True})
    return create_json_response({'error': 'not found'}, 404)

@app.route('/admin/holomem/refresh', methods=['POST'])
def refresh_holomem():
    background_executor.submit(update_holomem_database)
    return create_json_response({'message': 'DBæ›´æ–°ã‚¿ã‚¹ã‚¯é–‹å§‹'})

@app.route('/stats', methods=['GET'])
def get_stats():
    with get_db_session() as session:
        stats = {
            'users': session.query(UserMemory).count(),
            'conversations': session.query(ConversationHistory).count(),
            'hololive_news': session.query(HololiveNews).count(),
            'holomem_wiki_entries': session.query(HolomemWiki).count(),
            'psychology_analyses': session.query(UserPsychology).count(),
        }
        return create_json_response(stats)

# ==============================================================================
# åˆæœŸåŒ–
# ==============================================================================
def run_scheduler():
    while True:
        try:
            schedule.run_pending()
        except Exception as e:
            logger.error(f"Scheduler error: {e}")
        time.sleep(60)

def initialize_app():
    global engine, Session, groq_client, gemini_model
    logger.info("=" * 60)
    logger.info("ğŸ”§ Starting Mochiko AI initialization (v34.1 Full)")
    logger.info("=" * 60)
    # Database
    try:
        logger.info("ğŸ—„ï¸ Step 1/6: Initializing database...")
        engine = create_engine(DATABASE_URL, pool_pre_ping=True)
        Base.metadata.create_all(engine)
        Session = sessionmaker(bind=engine)
        initialize_knowledge_db()
        knowledge_base.load_data()
        logger.info("âœ… DBåˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        logger.critical(f"ğŸ”¥ DBåˆæœŸåŒ–å¤±æ•—: {e}")
        raise
    # Groq
    try:
        logger.info("ğŸ¦™ Step 2/6: Initializing Groq...")
        if GROQ_API_KEY and len(GROQ_API_KEY) > 20:
            groq_client = Groq(api_key=GROQ_API_KEY)
            logger.info("âœ… GroqåˆæœŸåŒ–å®Œäº†")
        else:
            logger.warning("âš ï¸ Groq API key not set")
    except Exception as e:
        logger.warning(f"âš ï¸ Groq initialization failed: {e}")
    # Gemini
    try:
        logger.info("ğŸ¦ Step 3/6: Initializing Gemini...")
        if GEMINI_API_KEY:
            genai.configure(api_key=GEMINI_API_KEY)
            gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')
            logger.info("âœ… GeminiåˆæœŸåŒ–å®Œäº†")
        else:
            logger.warning("âš ï¸ Gemini API key not set")
    except Exception as e:
        logger.warning(f"âš ï¸ Gemini initialization failed: {e}")
    # VOICEVOX
    logger.info("ğŸ¤ Step 4/6: Checking VOICEVOX...")
    if find_active_voicevox_url():
        global_state.voicevox_enabled = True
        logger.info("âœ… VOICEVOXæ¤œå‡º")
    else:
        logger.info("â„¹ï¸ VOICEVOX not available")
    # HoloMem
    logger.info("ğŸ€ Step 5/6: Initializing HoloMem system...")
    initialize_holomem_wiki()
    if holomem_manager.load_from_db():
        logger.info(f"âœ… ãƒ›ãƒ­ãƒ¡ãƒ³: {holomem_manager.get_member_count()}åãƒ­ãƒ¼ãƒ‰")
    if holomem_manager.get_member_count() == 0:
        logger.info("ğŸ“¡ DBãŒç©ºã®ãŸã‚åˆå›åé›†å®Ÿè¡Œ")
        background_executor.submit(update_holomem_database)
    # Scheduler
    logger.info("â° Step 6/6: Starting scheduler...")
    schedule.every(6).hours.do(update_holomem_database)
    schedule.every(1).hours.do(cleanup_old_voice_files)
    schedule.every(6).hours.do(chat_rate_limiter.cleanup_old_entries)
    schedule.every().day.at("03:00").do(schedule_psychology_analysis)
    threading.Thread(target=run_scheduler, daemon=True).start()
    cleanup_old_voice_files()
    logger.info("=" * 60)
    logger.info("ğŸš€ Mochiko AI initialization complete!")
    logger.info("ğŸŒ Server is ready to accept requests")
    logger.info("=" * 60)

def signal_handler(sig, frame):
    logger.info(f"ğŸ›‘ Signal {sig} received. Shutting down gracefully...")
    background_executor.shutdown(wait=True)
    if engine:
        engine.dispose()
    logger.info("ğŸ‘‹ Mochiko AI has shut down.")
    sys.exit(0)

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# ==============================================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ==============================================================================
try:
    initialize_app()
    application = app
    logger.info("âœ… Flask application ready.")
except Exception as e:
    logger.critical(f"ğŸ”¥ Fatal initialization error: {e}", exc_info=True)
    application = app
    logger.warning("âš ï¸ Application created with limited functionality.")

if __name__ == '__main__':
    logger.info("ğŸš€ Running in direct mode")
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 5000)), debug=False)
