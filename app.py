import os
import requests
import logging
import sys
import time
import threading
import json
import re
import sqlite3
import random
from datetime import datetime, timedelta
from typing import Union, Dict, Any, List
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Index, Text
from sqlalchemy.orm import declarative_base, sessionmaker
from urllib.parse import quote_plus, urljoin
from bs4 import BeautifulSoup

# --- åŸºæœ¬è¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è¨­å®šå®šæ•°
VOICEVOX_MAX_TEXT_LENGTH = 50
VOICEVOX_FAST_TIMEOUT = 10
CONVERSATION_HISTORY_TURNS = 2
VOICE_DIR = '/tmp/voices'
SERVER_URL = "https://slautofriendbot.onrender.com"

# --- ç§˜å¯†æƒ…å ±/ç’°å¢ƒå¤‰æ•° èª­ã¿è¾¼ã¿ ---
def get_secret(name: str) -> Union[str, None]:
    """ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ç§˜å¯†ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šå€¤ã‚’å–å¾—"""
    env_value = os.environ.get(name)
    if env_value: 
        return env_value
    try:
        with open(f'/etc/secrets/{name}', 'r') as f:
            return f.read().strip()
    except Exception:
        return None

# å¿…è¦ãªè¨­å®šå€¤ã‚’å–å¾—
DATABASE_URL = get_secret('DATABASE_URL')
GROQ_API_KEY = get_secret('GROQ_API_KEY')
VOICEVOX_URL_FROM_ENV = get_secret('VOICEVOX_URL')
WEATHER_API_KEY = get_secret('WEATHER_API_KEY')

# --- ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ– ---
groq_client = None
if GROQ_API_KEY:
    try:
        from groq import Groq
        groq_client = Groq(api_key=GROQ_API_KEY)
        logger.info("âœ… Groqã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ Groqã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")

# VOICEVOXè¨­å®š
VOICEVOX_URLS = [
    'http://localhost:50021',
    'http://127.0.0.1:50021',
    'http://voicevox-engine:50021',
    'http://voicevox:50021'
]
WORKING_VOICEVOX_URL = VOICEVOX_URL_FROM_ENV or VOICEVOX_URLS[0]
VOICEVOX_ENABLED = True

# å¿…é ˆè¨­å®šãƒã‚§ãƒƒã‚¯
if not all([DATABASE_URL, groq_client]):
    logger.critical("FATAL: å¿…é ˆè¨­å®š(DATABASE_URL or GROQ_API_KEY)ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
    sys.exit(1)

# Flask & ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
app = Flask(__name__)
CORS(app)
engine = create_engine(DATABASE_URL)
Base = declarative_base()

# --- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« ---
class UserMemory(Base):
    __tablename__ = 'user_memories'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False)
    user_name = Column(String(255), nullable=False)
    interaction_count = Column(Integer, default=0)
    last_interaction = Column(DateTime, default=datetime.utcnow)

class ConversationHistory(Base):
    __tablename__ = 'conversation_history'
    id = Column(Integer, primary_key=True, autoincrement=True)
    user_uuid = Column(String(255), nullable=False, index=True)
    role = Column(String(10), nullable=False)
    content = Column(Text, nullable=False)
    timestamp = Column(DateTime, default=datetime.utcnow, index=True)

class HololiveNews(Base):
    __tablename__ = 'hololive_news'
    id = Column(Integer, primary_key=True)
    title = Column(String(500), nullable=False)
    content = Column(Text, nullable=False)
    url = Column(String(1000))
    published_date = Column(DateTime, default=datetime.utcnow)
    created_at = Column(DateTime, default=datetime.utcnow, index=True)

# ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)

# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---
def clean_text(text: str) -> str:
    """HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    if not text:
        return ""
    # HTMLã‚¿ã‚°é™¤å»
    text = re.sub(r'<[^>]+>', '', text)
    # è¤‡æ•°ã®ç©ºç™½ã‚’å˜ä¸€ã«
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def get_japan_time() -> str:
    """ç¾åœ¨ã®æ—¥æœ¬æ™‚é–“ã‚’å–å¾—"""
    from datetime import timezone, timedelta
    jst = timezone(timedelta(hours=9))
    now = datetime.now(jst)
    weekdays = ['æœˆæ›œæ—¥', 'ç«æ›œæ—¥', 'æ°´æ›œæ—¥', 'æœ¨æ›œæ—¥', 'é‡‘æ›œæ—¥', 'åœŸæ›œæ—¥', 'æ—¥æ›œæ—¥']
    weekday = weekdays[now.weekday()]
    return f"ä»Šã¯{now.year}å¹´{now.month}æœˆ{now.day}æ—¥({weekday})ã®{now.hour}æ™‚{now.minute}åˆ†ã ã‚ˆï¼"

# --- åˆ¤å®šé–¢æ•° ---
def is_time_request(message: str) -> bool:
    """æ™‚åˆ»ã«é–¢ã™ã‚‹è³ªå•ã‹ã©ã†ã‹åˆ¤å®š"""
    time_keywords = ['ä»Šä½•æ™‚', 'æ™‚é–“', 'æ™‚åˆ»', 'ã„ã¾ä½•æ™‚', 'ç¾åœ¨æ™‚åˆ»', 'ä»Šã®æ™‚é–“']
    return any(keyword in message for keyword in time_keywords)

def is_weather_request(message: str) -> bool:
    """å¤©æ°—ã«é–¢ã™ã‚‹è³ªå•ã‹ã©ã†ã‹åˆ¤å®š"""
    return any(keyword in message for keyword in ['å¤©æ°—', 'ã¦ã‚“ã', 'æ°—æ¸©', 'é™æ°´ç¢ºç‡', 'æ™´ã‚Œ', 'é›¨', 'æ›‡ã‚Š'])

def is_recommendation_request(message: str) -> bool:
    """ãŠã™ã™ã‚ã«é–¢ã™ã‚‹è³ªå•ã‹ã©ã†ã‹åˆ¤å®š"""
    return any(keyword in message for keyword in ['ãŠã™ã™ã‚', 'ã‚ªã‚¹ã‚¹ãƒ¡', 'äººæ°—', 'æµè¡Œ', 'ã¯ã‚„ã‚Š', 'ãƒ©ãƒ³ã‚­ãƒ³ã‚°'])

# ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ãƒ¡ãƒ³ãƒãƒ¼ãƒªã‚¹ãƒˆ
HOLOMEM_KEYWORDS = [
    'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–', 'ãƒ›ãƒ­ãƒ¡ãƒ³', 'hololive',
    'ã¨ãã®ãã‚‰', 'ãƒ­ãƒœå­', 'ã•ãã‚‰ã¿ã“', 'æ˜Ÿè¡—ã™ã„ã›ã„', 'AZKi',
    'ç™½ä¸Šãƒ•ãƒ–ã‚­', 'å¤è‰²ã¾ã¤ã‚Š', 'æ¹Šã‚ãã‚', 'ç´«å’²ã‚·ã‚ªãƒ³', 'ç™¾é¬¼ã‚ã‚„ã‚',
    'å¤§ç©ºã‚¹ãƒãƒ«', 'å¤§ç¥ãƒŸã‚ª', 'çŒ«åˆãŠã‹ã‚†', 'æˆŒç¥ã“ã‚ã­',
    'å…ç”°ãºã“ã‚‰', 'ä¸çŸ¥ç«ãƒ•ãƒ¬ã‚¢', 'ç™½éŠ€ãƒã‚¨ãƒ«', 'å®é˜ãƒãƒªãƒ³',
    'å¤©éŸ³ã‹ãªãŸ', 'è§’å·»ã‚ãŸã‚', 'å¸¸é—‡ãƒˆãƒ¯', 'å§«æ£®ãƒ«ãƒ¼ãƒŠ',
    'ãƒ©ãƒ—ãƒ©ã‚¹ãƒ»ãƒ€ãƒ¼ã‚¯ãƒã‚¹', 'é·¹å¶ºãƒ«ã‚¤', 'åšè¡£ã“ã‚ˆã‚Š', 'æ²™èŠ±å‰ã‚¯ãƒ­ãƒ±', 'é¢¨çœŸã„ã‚ã¯'
]

def is_hololive_request(message: str) -> bool:
    """ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã«é–¢ã™ã‚‹è³ªå•ã‹ã©ã†ã‹åˆ¤å®š"""
    return any(keyword in message for keyword in HOLOMEM_KEYWORDS)

def should_search(message: str) -> bool:
    """Webæ¤œç´¢ãŒå¿…è¦ã‹ã©ã†ã‹åˆ¤å®š"""
    search_patterns = [
        r'(?:ã¨ã¯|ã«ã¤ã„ã¦|æ•™ãˆã¦|çŸ¥ã‚ŠãŸã„)',
        r'(?:æœ€æ–°|ä»Šæ—¥|ãƒ‹ãƒ¥ãƒ¼ã‚¹)',
        r'(?:èª¿ã¹ã¦|æ¤œç´¢)'
    ]
    search_words = ['èª°', 'ä½•', 'ã©ã“', 'ã„ã¤', 'ã©ã†ã—ã¦', 'ãªãœ']
    
    return (any(re.search(pattern, message) for pattern in search_patterns) or
            any(word in message for word in search_words))

def is_short_response(message: str) -> bool:
    """çŸ­ã„ç›¸æ§Œã‹ã©ã†ã‹åˆ¤å®š"""
    short_responses = ['ã†ã‚“', 'ãã†', 'ã¯ã„', 'ãã£ã‹', 'ãªã‚‹ã»ã©', 'ã¸ãƒ¼', 'ãµãƒ¼ã‚“', 'ãŠãŠ', 'ã‚ã‹ã£ãŸ']
    return len(message.strip()) <= 3 or message.strip() in short_responses

# --- å¤©æ°—å–å¾—æ©Ÿèƒ½ ---
LOCATION_CODES = {
    "æ±äº¬": "130000",
    "å¤§é˜ª": "270000", 
    "åå¤å±‹": "230000",
    "ç¦å²¡": "400000",
    "æœ­å¹Œ": "016000",
    "ä»™å°": "040000",
    "åºƒå³¶": "340000"
}

def extract_location(message: str) -> str:
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰åœ°åŸŸã‚’æŠ½å‡º"""
    for location in LOCATION_CODES.keys():
        if location in message:
            return location
    return "æ±äº¬"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

def get_weather_forecast(location: str) -> Union[str, None]:
    """æ°—è±¡åºAPIã‹ã‚‰å¤©æ°—äºˆå ±ã‚’å–å¾—"""
    area_code = LOCATION_CODES.get(location)
    if not area_code:
        return None
        
    url = f"https://www.jma.go.jp/bosai/forecast/data/overview_forecast/{area_code}.json"
    
    try:
        response = requests.get(url, timeout=5)
        response.raise_for_status()
        data = response.json()
        
        weather_text = clean_text(data.get('text', ''))
        office = data.get('publishingOffice', 'æ°—è±¡åº')
        
        return f"{location}ã®å¤©æ°—æƒ…å ±ã ã‚ˆï¼ï¼ˆ{office}ç™ºè¡¨ï¼‰\nã€Œ{weather_text}ã€\nã¾ã˜å‚è€ƒã«ã—ã¦ã­ã€œï¼"
    except Exception as e:
        logger.error(f"å¤©æ°—APIå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --- ç„¡æ–™æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š ---
FREE_SEARCH_ENGINES = [
    {
        'name': 'DuckDuckGo',
        'url': 'https://duckduckgo.com/html/?q={}',
        'result_selector': '.result__title a',
        'snippet_selector': '.result__snippet'
    },
    {
        'name': 'StartPage',
        'url': 'https://www.startpage.com/sp/search?query={}',
        'result_selector': '.w-gl__result-title',
        'snippet_selector': '.w-gl__description'
    },
    {
        'name': 'Searx',
        'url': 'https://searx.be/search?q={}&format=json',
        'json_api': True
    }
]

# è¤‡æ•°ã®User-Agentã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15'
]

def get_random_user_agent():
    """ãƒ©ãƒ³ãƒ€ãƒ ãªUser-Agentã‚’å–å¾—"""
    return random.choice(USER_AGENTS)

# --- ç„¡æ–™æ¤œç´¢å®Ÿè£… ---
def search_duckduckgo(query: str) -> List[Dict[str, str]]:
    """DuckDuckGoæ¤œç´¢ï¼ˆHTML scrapingï¼‰"""
    try:
        url = f"https://duckduckgo.com/html/?q={quote_plus(query)}"
        headers = {
            'User-Agent': get_random_user_agent(),
            'Accept-Language': 'ja,en;q=0.9'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        results = []
        
        for result_div in soup.find_all('div', class_='links_main')[:3]:
            try:
                title_element = result_div.find('a', class_='result__a')
                snippet_element = result_div.find('div', class_='result__snippet')
                
                if title_element and snippet_element:
                    title = clean_text(title_element.get_text())
                    snippet = clean_text(snippet_element.get_text())
                    url = title_element.get('href', '')
                    
                    if title and snippet:
                        results.append({
                            'title': title,
                            'snippet': snippet,
                            'url': url
                        })
            except:
                continue
                
        return results
        
    except Exception as e:
        logger.error(f"DuckDuckGoæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def search_with_requests(query: str) -> List[Dict[str, str]]:
    """Requests + BeautifulSoupã«ã‚ˆã‚‹ç›´æ¥æ¤œç´¢"""
    try:
        # æ—¥æœ¬èªã®æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’å„ªå…ˆ
        search_urls = [
            f"https://search.yahoo.co.jp/search?p={quote_plus(query)}",
            f"https://www.bing.com/search?q={quote_plus(query)}&mkt=ja-JP"
        ]
        
        for search_url in search_urls:
            try:
                headers = {
                    'User-Agent': get_random_user_agent(),
                    'Accept-Language': 'ja,en;q=0.9',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
                }
                
                response = requests.get(search_url, headers=headers, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                results = []
                
                # Yahooæ¤œç´¢çµæœã®è§£æ
                if 'yahoo.co.jp' in search_url:
                    for result in soup.find_all('div', class_='Algo')[:3]:
                        try:
                            title_elem = result.find('h3')
                            snippet_elem = result.find('div', class_='compText')
                            
                            if title_elem and snippet_elem:
                                title = clean_text(title_elem.get_text())
                                snippet = clean_text(snippet_elem.get_text())
                                
                                if title and snippet:
                                    results.append({
                                        'title': title,
                                        'snippet': snippet,
                                        'url': ''
                                    })
                        except:
                            continue
                
                # Bingæ¤œç´¢çµæœã®è§£æ
                elif 'bing.com' in search_url:
                    for result in soup.find_all('li', class_='b_algo')[:3]:
                        try:
                            title_elem = result.find('h2')
                            snippet_elem = result.find('div', class_='b_caption')
                            
                            if title_elem and snippet_elem:
                                title = clean_text(title_elem.get_text())
                                snippet = clean_text(snippet_elem.get_text())
                                
                                if title and snippet:
                                    results.append({
                                        'title': title,
                                        'snippet': snippet,
                                        'url': ''
                                    })
                        except:
                            continue
                
                if results:
                    logger.info(f"âœ… æ¤œç´¢æˆåŠŸ ({search_url}): {len(results)}ä»¶")
                    return results
                    
            except Exception as e:
                logger.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({search_url}): {e}")
                continue
        
        return []
        
    except Exception as e:
        logger.error(f"å…¨ä½“æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# --- å°‚é–€ã‚µã‚¤ãƒˆæ¤œç´¢æ©Ÿèƒ½ ---
SPECIALIZED_SITES = {
    'Blender': {
        'base_url': 'https://docs.blender.org/manual/ja/latest/',
        'keywords': ['Blender', 'ãƒ–ãƒ¬ãƒ³ãƒ€ãƒ¼', '3D', 'ãƒ¢ãƒ‡ãƒªãƒ³ã‚°'],
        'search_paths': ['/modeling/', '/rendering/', '/animation/']
    },
    'CGãƒ‹ãƒ¥ãƒ¼ã‚¹': {
        'base_url': 'https://modelinghappy.com/',
        'keywords': ['CG', '3DCG', 'ãƒ¢ãƒ‡ãƒªãƒ³ã‚°', 'ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°'],
        'search_paths': ['/category/news/', '/category/tutorial/']
    },
    'è„³ç§‘å­¦ãƒ»å¿ƒç†å­¦': {
        'base_url': 'https://nazology.kusuguru.co.jp/',
        'keywords': ['è„³ç§‘å­¦', 'å¿ƒç†å­¦', 'èªçŸ¥', 'ç¥çµŒ'],
        'search_paths': ['/brain/', '/psychology/']
    },
    'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•': {
        'base_url': 'https://community.secondlife.com/news/',
        'keywords': ['ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•', 'Second Life', 'SL', 'ãƒãƒ¼ãƒãƒ£ãƒ«'],
        'search_paths': ['/news/', '/forums/']
    }
}

def detect_specialized_topic(message: str) -> Union[str, None]:
    """å°‚é–€ãƒˆãƒ”ãƒƒã‚¯ã‚’æ¤œå‡º"""
    for topic, config in SPECIALIZED_SITES.items():
        if any(keyword in message for keyword in config['keywords']):
            return topic
    return None

def specialized_site_search(topic: str, query: str) -> Union[str, None]:
    """å°‚é–€ã‚µã‚¤ãƒˆå†…æ¤œç´¢ã‚’å®Ÿè¡Œ"""
    if topic not in SPECIALIZED_SITES:
        return None
        
    config = SPECIALIZED_SITES[topic]
    try:
        # ã‚µã‚¤ãƒˆå›ºæœ‰ã®æ¤œç´¢ã‚’å®Ÿè¡Œ
        site_query = f"site:{config['base_url']} {query}"
        results = search_with_requests(site_query)
        
        if results:
            # çµæœã‚’ã¾ã¨ã‚ã¦è¿”ã™
            formatted_results = []
            for result in results[:2]:
                formatted_results.append(f"ã€Œ{result['title']}ã€{result['snippet']}")
            return '\n'.join(formatted_results)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µã‚¤ãƒˆç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
        return scrape_site_content(config['base_url'], query)
        
    except Exception as e:
        logger.error(f"å°‚é–€ã‚µã‚¤ãƒˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({topic}): {e}")
        return None

def scrape_site_content(base_url: str, query: str) -> Union[str, None]:
    """ã‚µã‚¤ãƒˆç›´æ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    try:
        headers = {
            'User-Agent': get_random_user_agent()
        }
        
        response = requests.get(base_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’æ¤œç´¢
        text_content = clean_text(soup.get_text())
        
        # queryã«é–¢é€£ã™ã‚‹éƒ¨åˆ†ã‚’æŠ½å‡º
        if query.lower() in text_content.lower():
            sentences = text_content.split('ã€‚')
            relevant_sentences = [s for s in sentences if query in s][:3]
            
            if relevant_sentences:
                return 'ã€‚'.join(relevant_sentences) + 'ã€‚'
                
        return None
        
    except Exception as e:
        logger.error(f"ã‚µã‚¤ãƒˆç›´æ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --- ãƒ¡ã‚¤ãƒ³æ¤œç´¢é–¢æ•° ---
def deep_web_search(query: str) -> Union[str, None]:
    """ç„¡æ–™æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã—ãŸãƒ‡ã‚£ãƒ¼ãƒ—æ¤œç´¢"""
    logger.info(f"ğŸ” ç„¡æ–™ãƒ‡ã‚£ãƒ¼ãƒ—ã‚µãƒ¼ãƒé–‹å§‹: '{query}'")
    
    try:
        # è¤‡æ•°ã®ç„¡æ–™æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’é †ç•ªã«è©¦è¡Œ
        search_functions = [
            lambda q: search_duckduckgo(q),
            lambda q: search_with_requests(q)
        ]
        
        for search_func in search_functions:
            try:
                results = search_func(query)
                
                if results:
                    # çµæœã‚’Groq AIã§è¦ç´„
                    summary_text = ""
                    for i, result in enumerate(results[:2], 1):
                        summary_text += f"[æƒ…å ±{i}] {result['title']}: {result['snippet']}\n"
                    
                    if summary_text:
                        summary_prompt = f"""
                        ä»¥ä¸‹ã®æ¤œç´¢çµæœã‚’ã€è³ªå•ã€Œ{query}ã€ã«ç­”ãˆã‚‹å½¢ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
                        é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ï¼š

                        {summary_text}
                        """
                        
                        completion = groq_client.chat.completions.create(
                            messages=[{"role": "system", "content": summary_prompt}],
                            model="llama-3.1-8b-instant",  # ğŸ”§ FIXED: Updated model name
                            temperature=0.2,
                            max_tokens=200
                        )
                        
                        return completion.choices[0].message.content.strip()
                
            except Exception as e:
                logger.error(f"å€‹åˆ¥æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return None
        
    except Exception as e:
        logger.error(f"ãƒ‡ã‚£ãƒ¼ãƒ—ã‚µãƒ¼ãƒã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --- ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–æƒ…å ±ç®¡ç† ---
def update_hololive_database():
    """ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ›´æ–°ï¼ˆ1æ™‚é–“æ¯å®Ÿè¡Œï¼‰"""
    session = Session()
    try:
        # éå»3ãƒ¶æœˆä»¥å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
        three_months_ago = datetime.utcnow() - timedelta(days=90)
        session.query(HololiveNews).filter(
            HololiveNews.created_at < three_months_ago
        ).delete()
        
        # æ–°ã—ã„æƒ…å ±ã‚’æ¤œç´¢ãƒ»ä¿å­˜
        search_result = deep_web_search("ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹")
        if search_result:
            news_entry = HololiveNews(
                title="æœ€æ–°ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–æƒ…å ±",
                content=search_result,
                url="",
                published_date=datetime.utcnow()
            )
            session.add(news_entry)
            
        session.commit()
        logger.info("âœ… ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°å®Œäº†")
        
    except Exception as e:
        logger.error(f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–DBæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        session.rollback()
    finally:
        session.close()

def get_hololive_info_from_db() -> Union[str, None]:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æœ€æ–°ã®ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–æƒ…å ±ã‚’å–å¾—"""
    session = Session()
    try:
        latest_news = session.query(HololiveNews)\
            .order_by(HololiveNews.created_at.desc())\
            .first()
            
        if latest_news:
            return latest_news.content
        return None
        
    except Exception as e:
        logger.error(f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–DBå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None
    finally:
        session.close()

def extract_recommendation_topic(message: str) -> Union[str, None]:
    """ãŠã™ã™ã‚ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º"""
    topics = {
        'æ˜ ç”»': ['æ˜ ç”»', 'ãƒ ãƒ¼ãƒ“ãƒ¼'],
        'éŸ³æ¥½': ['éŸ³æ¥½', 'æ›²', 'ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯', 'æ­Œ'],
        'ã‚¢ãƒ‹ãƒ¡': ['ã‚¢ãƒ‹ãƒ¡', 'ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³'],
        'æœ¬': ['æœ¬', 'æ›¸ç±', 'æ¼«ç”»', 'ãƒãƒ³ã‚¬', 'å°èª¬'],
        'ã‚²ãƒ¼ãƒ ': ['ã‚²ãƒ¼ãƒ ', 'ã‚²ãƒ¼ãƒŸãƒ³ã‚°'],
        'ã‚°ãƒ«ãƒ¡': ['ã‚°ãƒ«ãƒ¡', 'é£Ÿã¹ç‰©', 'ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³', 'æ–™ç†']
    }
    
    for topic, keywords in topics.items():
        if any(kw in message for kw in keywords):
            return topic
    return None

# --- AIå¿œç­”ç”Ÿæˆ ---
def generate_ai_response(user_data: Dict[str, Any], message: str, history: List[Any]) -> str:
    """ãƒ¡ã‚¤ãƒ³AIå¿œç­”ç”Ÿæˆé–¢æ•°"""
    if not groq_client:
        return "ã‚ã¦ãƒã—ã€ä»Šã¡ã‚‡ã£ã¨èª¿å­æ‚ªã„ã‹ã‚‚...ã¾ãŸã‚ã¨ã§è©±ã—ã¦ï¼"
    
    search_info = ""
    search_query = ""
    search_failed = False
    is_fallback = False
    specialized_topic = None
    
    # çŸ­ã„ç›¸æ§Œã®å ´åˆã¯æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—
    if is_short_response(message):
        logger.info("ğŸ’¬ çŸ­ã„ç›¸æ§Œã‚’æ¤œå‡ºã€æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ä¼šè©±ç¶™ç¶š")
        pass
    # æ™‚åˆ»è¦æ±‚ã®å ´åˆ
    elif is_time_request(message):
        search_info = get_japan_time()
    # å¤©æ°—è¦æ±‚ã®å ´åˆ
    elif is_weather_request(message):
        location = extract_location(message)
        weather_info = get_weather_forecast(location)
        if weather_info:
            search_info = weather_info
        else:
            return "ä»Šæ—¥ã®å¤©æ°—ï¼Ÿèª¿ã¹ã¦ã¿ãŸã‘ã©ã€æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸ...ã”ã‚ã‚“ã­ï¼ã§ã‚‚ã€æ°—è±¡åºã®ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã«ã¯ã€å„åœ°ã®å¤©æ°—äºˆå ±ãŒè¼‰ã£ã¦ã‚‹ã‚ˆã€œã¾ã˜ä¾¿åˆ©ã ã‹ã‚‰è¦‹ã¦ã¿ã¦ï¼"
    # å°‚é–€ã‚µã‚¤ãƒˆæ¤œç´¢ã®å ´åˆ
    elif (specialized_topic := detect_specialized_topic(message)):
        logger.info(f"ğŸ”¬ å°‚é–€ãƒˆãƒ”ãƒƒã‚¯æ¤œå‡º: {specialized_topic}")
        search_info = specialized_site_search(specialized_topic, message)
    # ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–é–¢é€£ã®å ´åˆ
    elif is_hololive_request(message):
        logger.info("ğŸ¤ ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–é–¢é€£ã®è³ªå•ã‚’æ¤œçŸ¥")
        holo_info = get_hololive_info_from_db()
        if holo_info:
            search_info = holo_info
        else:
            search_query = f"{message} ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– æœ€æ–°æƒ…å ±"
    # ãŠã™ã™ã‚è¦æ±‚ã®å ´åˆ
    elif is_recommendation_request(message):
        topic = extract_recommendation_topic(message)
        search_query = f"æœ€æ–° {topic} äººæ°—ãƒ©ãƒ³ã‚­ãƒ³ã‚°" if topic else "æœ€è¿‘ è©±é¡Œã®ã‚‚ã® ãƒ©ãƒ³ã‚­ãƒ³ã‚°"
    # ä¸€èˆ¬çš„ãªæ¤œç´¢ãŒå¿…è¦ãªå ´åˆ
    elif should_search(message):
        search_query = message
    
    # Webæ¤œç´¢å®Ÿè¡Œ
    if search_query and not search_info:
        search_info = deep_web_search(search_query)
        if not search_info:
            search_failed = True
    
    # æ¤œç´¢å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ›ãƒ­ãƒ©ã‚¤ãƒ–æƒ…å ±ã§ä»£æ›¿ï¼‰
    if search_failed and not is_hololive_request(message):
        logger.info("ğŸ’¡ æ¤œç´¢å¤±æ•—ã®ãŸã‚ã€ä»£æ›¿æ¡ˆã¨ã—ã¦ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®æƒ…å ±ã‚’æ¤œç´¢ã—ã¾ã™ã€‚")
        fallback_info = get_hololive_info_from_db()
        if not fallback_info:
            fallback_info = deep_web_search("ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹")
        if fallback_info:
            search_info = fallback_info
            is_fallback = True
