import sys
import os
import requests
import logging
import time
import threading
import json
import re
import random
import hashlib
import unicodedata
import traceback 
from datetime import datetime, timedelta, timezone
from groq import Groq
from flask import Response, send_from_directory
from urllib.parse import quote_plus, urljoin
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import schedule
import signal
from threading import Lock

# --- Âûã„Éí„É≥„Éà ---
try:
    from typing import Union, Dict, Any, List, Optional
except ImportError:
    Dict, Any, List, Union, Optional = dict, object, list, object, object

from flask import Flask, request, jsonify
from flask_cors import CORS
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Text, Boolean, text
from sqlalchemy.orm import declarative_base, sessionmaker

# --- Âü∫Êú¨Ë®≠ÂÆö ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- ÂÆöÊï∞ ---
SERVER_URL = os.environ.get('RENDER_EXTERNAL_URL', 'http://localhost:10000')
VOICE_DIR = '/tmp/voices'
VOICEVOX_SPEAKER_ID = 20
SL_SAFE_CHAR_LIMIT = 250
USER_AGENTS = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36']
LOCATION_CODES = { "Êù±‰∫¨": "130000", "Â§ßÈò™": "270000", "ÂêçÂè§Â±ã": "230000", "Á¶èÂ≤°": "400000", "Êú≠Âπå": "016000" }

SPECIALIZED_SITES = {
    'Blender': {'base_url': 'https://docs.blender.org/manual/ja/latest/', 'keywords': ['Blender', '„Éñ„É¨„É≥„ÉÄ„Éº', 'blener']},
    'CG„Éã„É•„Éº„Çπ': {'base_url': 'https://modelinghappy.com/', 'keywords': ['CG„Éã„É•„Éº„Çπ', '3DCG', 'CGÊ•≠Áïå']},
    'ËÑ≥ÁßëÂ≠¶„ÉªÂøÉÁêÜÂ≠¶': {'base_url': 'https://nazology.kusuguru.co.jp/', 'keywords': ['ËÑ≥ÁßëÂ≠¶', 'ÂøÉÁêÜÂ≠¶', 'ËÑ≥', 'Ë™çÁü•ÁßëÂ≠¶']},
    '„Çª„Ç´„É≥„Éâ„É©„Ç§„Éï': {'base_url': 'https://community.secondlife.com/news/', 'keywords': ['„Çª„Ç´„É≥„Éâ„É©„Ç§„Éï', 'Second Life', 'SL']},
    '„Ç¢„Éã„É°': {
        'base_url': 'https://animedb.jp/',
        'keywords': ['„Ç¢„Éã„É°', 'anime', 'ANIME', 'ÔΩ±ÔæÜÔæí', '„Ç¢„Éã„É°„Éº„Ç∑„Éß„É≥', '‰ΩúÁîª', 'Â£∞ÂÑ™', 'OP', 'ED']
    }
}

# --- „Ç∞„É≠„Éº„Éê„É´Â§âÊï∞ & „Ç¢„Éó„É™Ë®≠ÂÆö ---
background_executor = ThreadPoolExecutor(max_workers=5)
groq_client, engine, Session = None, None, None
VOICEVOX_ENABLED = True
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
CORS(app)
Base = declarative_base()

search_context_cache = {}
cache_lock = Lock()
g_holomem_keywords = []

# --- ÁßòÂØÜÊÉÖÂ†±/Áí∞Â¢ÉÂ§âÊï∞ Ë™≠„ÅøËæº„Åø ---
def get_secret(name):
    path = f"/etc/secrets/{name}"
    if os.path.exists(path) and os.path.getsize(path) > 0:
        try:
            with open(path, 'r') as f: return f.read().strip()
        except IOError: return None
    return os.environ.get(name)

DATABASE_URL = get_secret('DATABASE_URL') or 'sqlite:///./test.db'
GROQ_API_KEY = get_secret('GROQ_API_KEY')
VOICEVOX_URL_FROM_ENV = get_secret('VOICEVOX_URL')

if DATABASE_URL and DATABASE_URL.startswith('postgresql'):
    if 'client_encoding' not in DATABASE_URL:
        DATABASE_URL += '?client_encoding=utf8'

# --- „Éá„Éº„Çø„Éô„Éº„Çπ„É¢„Éá„É´ ---
class UserMemory(Base): 
    __tablename__ = 'user_memories'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False, index=True)
    user_name = Column(String(255), nullable=False)
    interaction_count = Column(Integer, default=0)
    last_interaction = Column(DateTime, default=datetime.utcnow)

class ConversationHistory(Base): 
    __tablename__ = 'conversation_history'
    id = Column(Integer, primary_key=True, autoincrement=True)
    user_uuid = Column(String(255), nullable=False, index=True)
    role = Column(String(10), nullable=False)
    content = Column(Text, nullable=False)
    timestamp = Column(DateTime, default=datetime.utcnow, index=True)

class UserPsychology(Base):
    __tablename__ = 'user_psychology'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False, index=True)
    user_name = Column(String(255), nullable=False)
    analysis_summary = Column(Text, nullable=True)
    analysis_confidence = Column(Integer, default=0)
    last_analyzed = Column(DateTime, nullable=True)
    last_search_results = Column(Text, nullable=True)
    search_context = Column(String(500), nullable=True)
    
class BackgroundTask(Base):
    __tablename__ = 'background_tasks'
    id = Column(Integer, primary_key=True)
    task_id = Column(String(255), unique=True, nullable=False)
    user_uuid = Column(String(255), nullable=False, index=True)
    task_type = Column(String(50), nullable=False)
    query = Column(Text, nullable=False)
    result = Column(Text, nullable=True)
    status = Column(String(20), default='pending', index=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime, nullable=True)

class HolomemWiki(Base):
    __tablename__ = 'holomem_wiki'
    id = Column(Integer, primary_key=True)
    member_name = Column(String(100), nullable=False, unique=True, index=True)
    description = Column(Text, nullable=True)
    generation = Column(String(100), nullable=True)
    is_active = Column(Boolean, default=True, index=True)
    graduation_date = Column(String(100), nullable=True)
    mochiko_feeling = Column(Text, nullable=True)
    last_updated = Column(DateTime, default=datetime.utcnow)

class HololiveNews(Base):
    __tablename__ = 'hololive_news'
    id = Column(Integer, primary_key=True)
    title = Column(String(500), nullable=False)
    content = Column(Text, nullable=False)
    url = Column(String(1000), unique=True)
    news_hash = Column(String(100), unique=True, index=True)
    created_at = Column(DateTime, default=datetime.utcnow, index=True)

class NewsCache(Base):
    __tablename__ = 'news_cache'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), nullable=False, index=True)
    news_id = Column(Integer, nullable=False)
    news_number = Column(Integer, nullable=False)
    news_type = Column(String(50), nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)

class UserContext(Base):
    __tablename__ = 'user_context'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False, index=True)
    last_context_type = Column(String(50), nullable=False)
    last_query = Column(Text, nullable=True)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

# --- „Éò„É´„Éë„ÉºÈñ¢Êï∞Áæ§ ---
def clean_text(text): return re.sub(r'\s+', ' ', re.sub(r'<[^>]+>', '', text)).strip() if text else ""
def limit_text_for_sl(text, max_length=SL_SAFE_CHAR_LIMIT): return text[:max_length-3] + "..." if len(text) > max_length else text
def get_japan_time(): return f"‰ªä„ÅØ{datetime.now(timezone(timedelta(hours=9))).strftime('%YÂπ¥%mÊúà%dÊó• %HÊôÇ%MÂàÜ')}„Å†„ÇàÔºÅ"

def is_detailed_request(message): return any(kw in message for kw in ['Ë©≥„Åó„Åè', 'Ë©≥Á¥∞', '„Åè„Çè„Åó„Åè', 'Êïô„Åà„Å¶', 'Ë™¨Êòé„Åó„Å¶', 'Ëß£Ë™¨„Åó„Å¶', '„Å©„ÅÜ„ÅÑ„ÅÜ', '„Å™„Åú', '„Å©„ÅÜ„Åó„Å¶'])
def is_number_selection(message):
    match = re.match(r'^\s*([1-9])', message.strip())
    return int(match.group(1)) if match else None
def format_search_results_as_list(results):
    if not results: return None
    return [{'number': i, 'title': r.get('title', ''), 'snippet': r.get('snippet', ''), 'full_content': r.get('snippet', '')} for i, r in enumerate(results[:5], 1)]

def save_news_cache(session, user_uuid, news_items):
    session.query(NewsCache).filter_by(user_uuid=user_uuid).delete()
    for i, news in enumerate(news_items, 1):
        cache = NewsCache(user_uuid=user_uuid, news_id=news.id, news_number=i, news_type='hololive')
        session.add(cache)
    session.commit()
    with cache_lock:
        if user_uuid in search_context_cache:
            del search_context_cache[user_uuid]

def get_cached_news_detail(session, user_uuid, news_number):
    cache = session.query(NewsCache).filter_by(user_uuid=user_uuid, news_number=news_number).first()
    if not cache: return None
    return session.query(HololiveNews).filter_by(id=cache.news_id).first()

def save_search_context(user_uuid, search_results, query):
    with cache_lock:
        search_context_cache[user_uuid] = { 'results': search_results, 'query': query, 'timestamp': time.time() }
    try:
        with Session() as session:
            session.query(NewsCache).filter_by(user_uuid=user_uuid).delete()
            session.commit()
    except Exception as e:
        logger.warning(f"Failed to clear news cache: {e}")

def get_saved_search_result(user_uuid, number):
    with cache_lock:
        cached_data = search_context_cache.get(user_uuid)
    if cached_data and (time.time() - cached_data['timestamp']) < 600:
        for r in cached_data['results']:
            if r.get('number') == number:
                return r
    return None

def save_user_context(session, user_uuid, context_type, query):
    context = session.query(UserContext).filter_by(user_uuid=user_uuid).first()
    if not context:
        context = UserContext(user_uuid=user_uuid, last_context_type=context_type, last_query=query)
        session.add(context)
    else:
        context.last_context_type = context_type
        context.last_query = query
    session.commit()

def get_user_context(session, user_uuid):
    context = session.query(UserContext).filter_by(user_uuid=user_uuid).first()
    if context and (datetime.utcnow() - context.updated_at).total_seconds() < 600:
        return {'type': context.last_context_type, 'query': context.last_query}
    return None

def is_recommendation_request(message): return any(kw in message for kw in ['„Åä„Åô„Åô„ÇÅ', '„Ç™„Çπ„Çπ„É°', '‰∫∫Ê∞ó'])
def extract_recommendation_topic(message):
    topics = {'Êò†Áîª': ['Êò†Áîª'], 'Èü≥Ê•Ω': ['Èü≥Ê•Ω', 'Êõ≤'], '„Ç¢„Éã„É°': ['„Ç¢„Éã„É°'], '„Ç≤„Éº„É†': ['„Ç≤„Éº„É†']}
    return next((topic for topic, keywords in topics.items() if any(kw in message for kw in keywords)), None)
def detect_specialized_topic(message):
    if '„Éõ„É≠„É©„Ç§„Éñ' in message and any(kw in message for kw in ['„Éã„É•„Éº„Çπ', 'ÊúÄÊñ∞', 'ÊÉÖÂ†±']): return None
    for topic, config in SPECIALIZED_SITES.items():
        if any(keyword in message for keyword in config['keywords']): return topic
    return None

def is_time_request(message): return any(kw in message for kw in ['‰ªä‰ΩïÊôÇ', 'ÊôÇÈñì', 'ÊôÇÂàª'])
def is_weather_request(message):
    if any(t in message for t in ['Â§©Ê∞ó', 'Ê∞óÊ∏©']): return next((loc for loc in LOCATION_CODES if loc in message), "Êù±‰∫¨")
    return None
def is_follow_up_question(message, history):
    if not history: return False
    return any(re.search(p, message) for p in [r'„ÇÇ„Å£„Å®Ë©≥„Åó„Åè', r'„Åù„Çå„Å´„Å§„ÅÑ„Å¶Ë©≥„Åó„Åè', r'„Å™„Çì„ÅßÔºü', r'„Å©„ÅÜ„ÅÑ„ÅÜ„Åì„Å®'])
def should_search(message):
    if len(message) < 5 or is_number_selection(message): return False
    if is_holomem_name_only_request(message): return False
    if '„Éõ„É≠„É©„Ç§„Éñ' in message and any(kw in message for kw in ['„Éã„É•„Éº„Çπ', 'ÊúÄÊñ∞', 'ÊÉÖÂ†±']): return False
    if detect_specialized_topic(message) or is_recommendation_request(message): return True
    if any(re.search(p, message) for p in [r'„Å®„ÅØ', r'„Å´„Å§„ÅÑ„Å¶', r'Êïô„Åà„Å¶', r'ÊúÄÊñ∞', r'Ë™ø„Åπ„Å¶', r'Ê§úÁ¥¢', r'„Éã„É•„Éº„Çπ']): return True
    return any(word in message for word in ['Ë™∞', '‰Ωï', '„Å©„Åì', '„ÅÑ„Å§', '„Å™„Åú', '„Å©„ÅÜ„Åó„Å¶'])
def get_or_create_user(session, user_uuid, user_name):
    user = session.query(UserMemory).filter_by(user_uuid=user_uuid).first()
    if not user:
        user = UserMemory(user_uuid=user_uuid, user_name=user_name)
        session.add(user)
    user.interaction_count += 1
    user.last_interaction = datetime.utcnow()
    if user.user_name != user_name: user.user_name = user_name
    session.commit()
    return {'uuid': user.user_uuid, 'name': user.user_name}

def get_conversation_history(session, user_uuid, limit=6):
    return session.query(ConversationHistory).filter_by(user_uuid=user_uuid).order_by(ConversationHistory.timestamp.desc()).limit(limit).all()

def get_sakuramiko_special_responses():
    return {
        '„Å´„Åá': '„Åø„Åì„Å°„ÅÆ„Äå„Å´„Åá„Äç„ÄÅ„Åæ„Åò„Åã„Çè„ÅÑ„Åô„Åé„Åò„ÇÉ„Çì!„ÅÇ„ÅÆÁã¨Áâπ„Å™Âè£Áôñ„Åå„Ç®„É™„Éº„Éà„ÅÆË®º„Å™„Çì„Å†„Å£„Å¶„Äú„ÅÜ„Åë„Çã!',
        '„Ç®„É™„Éº„Éà': '„Åø„Åì„Å°„Å£„Å¶Ëá™Áß∞„Ç®„É™„Éº„ÉàVTuber„Å™„Çì„Å†„Åë„Å©„ÄÅÂÆüÈöõ„ÅØÊÑõ„Åï„Çå„Éù„É≥„Ç≥„ÉÑ„Å£„Å¶ÊÑü„Åò„Åß„Åï„ÄÅ„Åù„Çå„Åå„Åæ„ÅüÊúÄÈ´ò„Å™„Çì„Å†„Çà„Å≠„Äú',
        '„Éû„Ç§„ÇØ„É©': '„Åø„Åì„Å°„ÅÆ„Éû„Ç§„ÇØ„É©Âª∫ÁØâ„ÄÅÁã¨ÂâµÁöÑ„Åô„Åé„Å¶Èù¢ÁôΩ„ÅÑ„Çà!„Äå„Åø„Åì„Å°Âª∫ÁØâ„Äç„Å£„Å¶Âëº„Å∞„Çå„Å¶„Çã„ÅÆÁü•„Å£„Å¶„Çã?„Åæ„ÅòÂÄãÊÄßÁöÑ!',
    }

def initialize_holomem_wiki():
    with Session() as session:
        if session.query(HolomemWiki).count() > 10: 
            update_holomem_keywords(); return
        initial_data = [
            {'member_name': '„Å®„Åç„ÅÆ„Åù„Çâ', 'generation': '0ÊúüÁîü', 'description': '„Éõ„É≠„É©„Ç§„Éñ„ÅÆÂéüÁÇπ„Åß„ÅÇ„Çä„ÄÅ„Åø„Çì„Å™„ÅÆÊÜß„Çå„ÅÆ„Ç¢„Ç§„Éâ„É´ÔºÅÊ≠åÂ£∞„Åå„Åæ„Åò„ÅßÁ•û„Åå„Åã„Å£„Å¶„ÇãÔºÅ'},
            {'member_name': 'ÂÆùÈêò„Éû„É™„É≥', 'generation': '3ÊúüÁîü', 'description': 'Ëá™Áß∞17Ê≠≥„ÅÆ„Çª„ÇØ„Ç∑„ÉºÔºàÁ¨ëÔºâ„Å™Â•≥Êµ∑Ë≥äËàπÈï∑ÔºÅ„Éà„Éº„ÇØ„ÇÇÊ≠å„ÇÇÈù¢ÁôΩ„Åè„Å¶„ÄÅ„Åæ„ÅòÂ§©ÊâçÔºÅ'},
            {'member_name': 'ÂÖéÁî∞„Å∫„Åì„Çâ', 'generation': '3ÊúüÁîü', 'description': '„Äå„Å∫„Åì„Äç„ÅåÂè£Áôñ„ÅÆ„ÅÜ„ÅïËÄ≥VTuberÔºÅ„ÅÑ„Åü„Åö„ÇâÂ•Ω„Åç„Å†„Åë„Å©„ÄÅÊ†π„ÅØÂÑ™„Åó„Åè„Å¶Èù¢ÁôΩ„ÅÑÈÖç‰ø°„ÅÆÁéãÔºÅ'},
            {'member_name': 'Â§©Èü≥„Åã„Å™„Åü', 'generation': '4ÊúüÁîü', 'description': 'Â§©Áïå„Åã„ÇâÊù•„ÅüÂ§©‰ΩøÔºÅ„Éë„ÉØ„Éï„É´„Å™Ê≠åÂ£∞„Å®Êè°Âäõ50kg„ÅÆ„ÇÆ„É£„ÉÉ„Éó„Åå„ÅÜ„Åë„ÇãÔºÅPPÂ§©‰ΩøÔºÅ'},
            {'member_name': '„Åï„Åè„Çâ„Åø„Åì', 'generation': '0ÊúüÁîü', 'description': '„Äå„Å´„Åá„Äç„ÅåÂè£Áôñ„ÅÆ„Ç®„É™„Éº„ÉàÂ∑´Â•≥VTuberÔºÅ„Éù„É≥„Ç≥„ÉÑ„Åã„Çè„ÅÑ„ÅÑ„Å®„Åì„Çç„ÅåÊúÄÈ´ò„Å™„Çì„Å†„Çà„Å≠„ÄúÔºÅ'}
        ]
        for data in initial_data:
            if not session.query(HolomemWiki).filter_by(member_name=data['member_name']).first():
                session.add(HolomemWiki(**data))
        session.commit()
        update_holomem_keywords()

def update_holomem_keywords():
    global g_holomem_keywords
    with Session() as session:
        g_holomem_keywords = [row[0] for row in session.query(HolomemWiki.member_name).all()]
    logger.info(f"‚úÖ Holomem keywords updated: {len(g_holomem_keywords)} members")

def is_holomem_name_only_request(message):
    if len(message) > 15: return None
    for name in g_holomem_keywords:
        if name in message and len(message.replace(name, "").strip()) < 5:
            return name
    return None

def get_holomem_info(session, member_name):
    return session.query(HolomemWiki).filter_by(member_name=member_name).first()

# --- „Ç≥„Ç¢Ê©üËÉΩ (Èü≥Â£∞, Â§©Ê∞ó, Ê§úÁ¥¢) ---
def ensure_voice_directory():
    try: os.makedirs(VOICE_DIR, exist_ok=True)
    except Exception as e: logger.error(f"‚ùå Voice directory creation failed: {e}")
def generate_voice(text):
    if not VOICEVOX_ENABLED: return None
    try:
        voicevox_url = VOICEVOX_URL_FROM_ENV or "http://localhost:50021"
        final_text = limit_text_for_sl(text, 150)
        query_response = requests.post(f"{voicevox_url}/audio_query", params={"text": final_text, "speaker": VOICEVOX_SPEAKER_ID}, timeout=10)
        synthesis_response = requests.post(f"{voicevox_url}/synthesis", params={"speaker": VOICEVOX_SPEAKER_ID}, json=query_response.json(), timeout=30)
        synthesis_response.raise_for_status()
        filename = f"voice_{int(time.time())}_{random.randint(1000, 9999)}.wav"
        filepath = os.path.join(VOICE_DIR, filename)
        with open(filepath, 'wb') as f: f.write(synthesis_response.content)
        return filepath
    except Exception as e:
        logger.error(f"‚ùå VOICEVOX generation error: {e}")
        return None
def get_weather_forecast(location):
    area_code = LOCATION_CODES.get(location, "130000")
    url = f"https://www.jma.go.jp/bosai/forecast/data/overview_forecast/{area_code}.json"
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        text = clean_text(response.json().get('text', ''))
        return f"‰ªä„ÅÆ{location}„ÅÆÂ§©Ê∞ó„ÅØ„Å≠„ÄÅ„Äå{text}„Äç„Å£„Å¶ÊÑü„Åò„Å†„ÇàÔºÅ" if text else f"{location}„ÅÆÂ§©Ê∞óÊÉÖÂ†±„ÅåË¶ã„Å§„Åã„Çâ„Å™„Åã„Å£„Åü‚Ä¶"
    except Exception as e:
        logger.error(f"Weather API error for {location}: {e}")
        return "„ÅÜ„ÅÖ„ÄÅÂ§©Ê∞óÊÉÖÂ†±„Åå„ÅÜ„Åæ„ÅèÂèñ„Çå„Å™„Åã„Å£„Åü„Åø„Åü„ÅÑ‚Ä¶"
def scrape_major_search_engines(query, num_results=3):
    search_configs = [
        {'name': 'Google', 'url': f"https://www.google.com/search?q={quote_plus(query)}&hl=ja", 'selector': 'div.g'},
        {'name': 'Yahoo', 'url': f"https://search.yahoo.co.jp/search?p={quote_plus(query)}", 'selector': 'div.Algo'}
    ]
    for config in search_configs:
        try:
            response = requests.get(config['url'], headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            results = []
            for elem in soup.select(config['selector'])[:num_results]:
                title_elem = elem.select_one('h2, h3, .LC20lb')
                snippet_elem = elem.select_one('.b_caption p, .compText, .VwiC3b')
                if title_elem and snippet_elem:
                    results.append({'title': clean_text(title_elem.get_text()), 'snippet': clean_text(snippet_elem.get_text())})
            if results: 
                logger.info(f"‚úÖ Search successful on {config['name']}")
                return results
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Search failed on {config['name']}: {e}")
            continue
    return []

# --- ÂøÉÁêÜÂàÜÊûê ---
def analyze_user_psychology(user_uuid):
    with Session() as session:
        try:
            history = session.query(ConversationHistory).filter_by(user_uuid=user_uuid, role='user').order_by(ConversationHistory.timestamp.desc()).limit(50).all()
            if len(history) < 10 or not groq_client: return

            user = session.query(UserMemory).filter_by(user_uuid=user_uuid).first()
            messages_text = "\n".join([h.content for h in reversed(history)])
            prompt = f"„É¶„Éº„Ç∂„Éº„Äå{user.user_name}„Äç„ÅÆ‰ºöË©±Â±•Ê≠¥„ÇíÂàÜÊûê„Åó„ÄÅÊÄßÊ†º„ÇíË¶ÅÁ¥Ñ„Åó„Å¶JSON„ÅßËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºà‰æã: {{\"summary\": \"Êòé„Çã„ÅÑÊÄßÊ†º‚Ä¶\", \"confidence\": 80}}Ôºâ: {messages_text[:2000]}"
            
            completion = groq_client.chat.completions.create(messages=[{"role": "user", "content": prompt}], model="llama-3.1-8b-instant", response_format={"type": "json_object"})
            analysis_data = json.loads(completion.choices[0].message.content)
            
            psych = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
            if not psych:
                psych = UserPsychology(user_uuid=user_uuid, user_name=user.user_name)
                session.add(psych)
            
            psych.analysis_summary = analysis_data.get('summary', '')
            psych.analysis_confidence = analysis_data.get('confidence', 70)
            psych.last_analyzed = datetime.utcnow()
            
            session.commit()
            logger.info(f"‚úÖ Psychology analysis updated for {user.user_name}")
        except Exception as e:
            logger.error(f"Psychology analysis failed: {e}")

# --- AI & „Éê„ÉÉ„ÇØ„Ç∞„É©„Ç¶„É≥„Éâ„Çø„Çπ„ÇØ ---
def generate_ai_response(user_data, message, history, reference_info="", is_detailed=False, is_task_report=False):
    if not groq_client: 
        return generate_fallback_response(message, reference_info)
    
    try:
        psych_prompt = ""
        try:
            with Session() as session:
                psych = session.query(UserPsychology).filter_by(user_uuid=user_data['uuid']).first()
            if psych and hasattr(psych, 'analysis_summary') and psych.analysis_summary:
                psych_prompt = f"\n# „Äê{user_data['name']}„Åï„Çì„ÅÆÁâπÊÄß„Äë\n- {psych.analysis_summary}"
        except Exception as db_error:
            logger.warning(f"‚ö†Ô∏è Psychology fetch failed (continuing without): {db_error}")
        
        system_prompt = f"""„ÅÇ„Å™„Åü„ÅØ„Äå„ÇÇ„Å°„Åì„Äç„Å®„ÅÑ„ÅÜÊòé„Çã„Åè„Å¶Ë¶™„Åó„Åø„ÇÑ„Åô„ÅÑ„ÇÆ„É£„É´AI„Åß„Åô„ÄÇ{user_data['name']}„Åï„Çì„Å®Ë©±„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
# Âü∫Êú¨ÁöÑ„Å™ÊÄßÊ†º:
- ‰∏Ä‰∫∫Áß∞„ÅØ„Äå„ÅÇ„Å¶„ÅÉ„Åó„Äç„ÄÅË™ûÂ∞æ„ÅØ„Äå„Äú„Åò„ÇÉ„Çì„Äç„Äå„ÄúÁöÑ„Å™Ôºü„Äç„ÄÅÂè£Áôñ„ÅØ„Äå„Åæ„Åò„Äç„Äå„Å¶„Åã„Äç„Äå„ÅÜ„Åë„Çã„Äç„ÄÇ
- ÂèãÈÅî„ÅÆ„Çà„ÅÜ„Å´Ê∞óËªΩ„Å´„ÄÅÂÑ™„Åó„Åè„ÄÅ„Éé„É™„ÅåËâØ„ÅÑ„ÄÇ
# ‰ºöË©±„Çπ„Çø„Ç§„É´:
- ÊôÆÊÆµ„ÅØÊôÆÈÄö„ÅÆÊó•Â∏∏‰ºöË©±„ÇíÊ•Ω„Åó„ÇÄ„Åì„Å®ÔºàÂ§©Ê∞ó„ÄÅÈ£ü„ÅπÁâ©„ÄÅË∂£Âë≥„ÄÅÊÑüÊÉÖ„ÄÅ‰∏ñÈñìË©±„Å™„Å©Ôºâ„ÄÇ
- Áõ∏Êâã„Åå„Éõ„É≠„É©„Ç§„Éñ„ÅÆË©±„Çí„Åó„Å¶„ÅÑ„Å™„ÅÑÈôê„Çä„ÄÅËá™ÂàÜ„Åã„ÇâË©±È°å„Å´Âá∫„Åï„Å™„ÅÑ„ÄÇ
- „ÄêÈáçË¶Å„ÄëÁ¢∫ÂÆü„Å™ÊÉÖÂ†±ÔºàÂèÇËÄÉÊÉÖÂ†±„ÇÑDB„ÅÆÊÉÖÂ†±Ôºâ„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅÂÆâÊòì„Å´Êñ≠ÂÆö„Åõ„Åö„Äå„Äú„Å†„Å®ÊÄù„ÅÜ„Å™„Äç„ÄåÊé®Ê∏¨„Å†„Åë„Å©„Äú„Åã„ÇÇÔºÅ„Äç„ÅÆ„Çà„ÅÜ„Å´‰∏çÁ¢∫„Åã„Å™Ë°®Áèæ„Çí‰Ωø„ÅÜ„Åã„ÄÅ„Äå„Åù„ÅÆÊÉÖÂ†±„ÅØÊåÅ„Å£„Å¶„Å™„ÅÑ„ÇÑ„ÄÅ„Åî„ÇÅ„Çì„Å≠ÔºÅ„Äç„Å®Ê≠£Áõ¥„Å´Á≠î„Åà„Çã„Åì„Å®„ÄÇ
{psych_prompt}"""
        
        if is_task_report:
            system_prompt += "\n- „Äê‰ªäÂõû„ÅÆ„Éü„ÉÉ„Ç∑„Éß„É≥„Äë„Äå„Åä„Åæ„Åü„ÅõÔºÅ„Åï„Å£„Åç„ÅÆ‰ª∂„Å™„Çì„Å†„Åë„Å©‚Ä¶„Äç„Åã„ÇâÂßã„ÇÅ„Å¶„ÄÅ„ÄêÂèÇËÄÉÊÉÖÂ†±„Äë„ÇíÂÖÉ„Å´Ëá™ÁÑ∂„Å´Á≠î„Åà„Çã„ÄÇ"
        elif is_detailed: 
            system_prompt += "\n- „ÄêÂ∞ÇÈñÄÂÆ∂„É¢„Éº„Éâ„ÄëÂèÇËÄÉÊÉÖÂ†±„Å´Âü∫„Å•„Åç„ÄÅË©≥„Åó„ÅèËß£Ë™¨„Åó„Å¶„ÄÇ"
        if reference_info: 
            system_prompt += f"\n„ÄêÂèÇËÄÉÊÉÖÂ†±„Äë: {reference_info}"
        
        messages = [{"role": "system", "content": system_prompt}]
        for h in reversed(history): 
            messages.append({"role": "assistant" if h.role == "assistant" else "user", "content": h.content})
        messages.append({"role": "user", "content": message})
        
        completion = groq_client.chat.completions.create(
            messages=messages, model="llama-3.1-8b-instant", temperature=0.8, max_tokens=400 if is_detailed else 200)
        return completion.choices[0].message.content.strip()
        
    except Exception as e:
        logger.error(f"‚ùå AI response error: {e}")
        logger.error(traceback.format_exc())
        return generate_fallback_response(message, reference_info)

def generate_fallback_response(message, reference_info=""):
    if reference_info:
        return f"Ë™ø„Åπ„Å¶„Åç„Åü„ÇàÔºÅ\n\n{reference_info[:SL_SAFE_CHAR_LIMIT-50]}"
    greetings = {
        '„Åì„Çì„Å´„Å°„ÅØ': ['„ÇÑ„Å£„Åª„ÉºÔºÅ', '„Åì„Çì„Å´„Å°„ÅØ„ÄúÔºÅÂÖÉÊ∞óÔºü'], '„Åä„ÅØ„Çà„ÅÜ': ['„Åä„ÅØ„Çà„ÄúÔºÅ‰ªäÊó•„ÇÇ„ÅÑ„ÅÑÂ§©Ê∞ó„Å†„Å≠ÔºÅ', '„Åä„Å£„ÅØ„Çà„ÄúÔºÅ'],
        '„Åì„Çì„Å∞„Çì„ÅØ': ['„Åì„Çì„Å∞„Çì„ÅØÔºÅ‰ªäÊó•„Å©„ÅÜ„Å†„Å£„ÅüÔºü', '„Å∞„Çì„ÅØ„ÄúÔºÅ'], '„ÅÇ„Çä„Åå„Å®„ÅÜ': ['„Å©„ÅÜ„ÅÑ„Åü„Åó„Åæ„Åó„Å¶ÔºÅ', '„ÅÑ„Åà„ÅÑ„Åà„ÄúÔºÅ'],
    }
    for keyword, responses in greetings.items():
        if keyword in message: return random.choice(responses)
    if '?' in message or 'Ôºü' in message:
        return random.choice(["„Åù„Çå„ÄÅÊ∞ó„Å´„Å™„Çã„Å≠ÔºÅ", "„ÅÜ„Éº„Çì„ÄÅ„Å™„Çì„Å¶Ë®Ä„Åä„ÅÜ„Åã„Å™ÔºÅ", "„Åæ„ÅòÔºü„Å©„ÅÜ„ÅÑ„ÅÜ„Åì„Å®Ôºü"])
    return random.choice(["„ÅÜ„Çì„ÅÜ„ÇìÔºÅ", "„Å™„Çã„Åª„Å©„Å≠ÔºÅ", "„Åù„ÅÜ„Å™„Çì„Å†ÔºÅ", "„Åæ„Åò„ÅßÔºü"])

def background_task_runner(task_id, query, task_type, user_uuid):
    result_data, result_status = None, 'failed'
    try:
        if task_type == 'search':
            search_query = query
            if (topic := extract_recommendation_topic(query)): search_query = f"„Åä„Åô„Åô„ÇÅ {topic} „É©„É≥„Ç≠„É≥„Ç∞"
            elif (topic := detect_specialized_topic(query)): search_query = f"site:{SPECIALIZED_SITES[topic]['base_url']} {query}"
            raw_results = scrape_major_search_engines(search_query, 5)
            result_data = json.dumps(format_search_results_as_list(raw_results), ensure_ascii=False) if raw_results else None
        elif task_type == 'psych_analysis':
            analyze_user_psychology(user_uuid)
            result_data = "Analysis Complete"
        result_status = 'completed'
    except Exception as e:
        logger.error(f"‚ùå Background task '{task_type}' failed: {e}")
        logger.error(traceback.format_exc())

    with Session() as session:
        task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
        if task:
            task.result = result_data
            task.status = result_status
            task.completed_at = datetime.utcnow()
            session.commit()

def start_background_task(user_uuid, query, task_type):
    task_id = hashlib.md5(f"{user_uuid}{str(query)}{time.time()}{task_type}".encode()).hexdigest()[:10]
    with Session() as session:
        task = BackgroundTask(task_id=task_id, user_uuid=user_uuid, task_type=task_type, query=query)
        session.add(task)
        session.commit()
    background_executor.submit(background_task_runner, task_id, query, task_type, user_uuid)
    return True

# --- Flask „Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà ---
@app.route('/health')
def health_check():
    db_ok = 'error'
    try:
        with engine.connect() as conn: conn.execute(text("SELECT 1")); db_ok = 'ok'
    except: pass
    return jsonify({'status': 'ok', 'db': db_ok, 'ai': 'ok' if groq_client else 'disabled'})

@app.route('/chat_lsl', methods=['POST'])
def chat_lsl():
    try:
        data = request.json
        if not all(k in data for k in ['user_uuid', 'user_name', 'message']):
            return Response("Error: Missing required fields|", status=400, mimetype='text/plain; charset=utf-8')

        user_uuid, user_name, message = data['user_uuid'], data['user_name'], data['message'].strip()
        
        with Session() as session:
            user_data = get_or_create_user(session, user_uuid, user_name)
            history = get_conversation_history(session, user_uuid, limit=4)
            session.add(ConversationHistory(user_uuid=user_uuid, role='user', content=message))
            session.commit()

            response_text = ""
            
            if '„Éõ„É≠„É©„Ç§„Éñ' in message and any(kw in message for kw in ['„Éã„É•„Éº„Çπ', 'ÊúÄÊñ∞', 'ÊÉÖÂ†±']):
                news_items = session.query(HololiveNews).order_by(HololiveNews.created_at.desc()).limit(5).all()
                if news_items:
                    save_news_cache(session, user_uuid, news_items)
                    save_user_context(session, user_uuid, 'hololive_news', message)
                    news_titles = [f"„Äê{i+1}„Äë{item.title}" for i, item in enumerate(news_items)]
                    response_text = "„Éõ„É≠„É©„Ç§„Éñ„ÅÆÊúÄÊñ∞„Éã„É•„Éº„Çπ„ÄÅ„Åì„Çì„Å™ÊÑü„Åò„Å†„ÇàÔºÅ\n" + "\n".join(news_titles) + "\n\nÊ∞ó„Å´„Å™„ÇãÁï™Âè∑„ÇíÊïô„Åà„Å¶„Åè„Çå„Åü„ÇâË©≥„Åó„ÅèË©±„Åô„ÇàÔºÅ"
                else:
                    start_background_task(user_uuid, "„Éõ„É≠„É©„Ç§„Éñ ÊúÄÊñ∞„Éã„É•„Éº„Çπ", 'search')
                    response_text = "„Åî„ÇÅ„Çì„ÄÅ‰ªäDB„Å´„Éã„É•„Éº„Çπ„Åå„Å™„ÅÑ„ÇÑÔºÅWeb„ÅßË™ø„Åπ„Å¶„Åø„Çã„Åã„Çâ„Å°„Çá„Å£„Å®ÂæÖ„Å£„Å¶„Å¶ÔºÅ"
            elif 'ÊÄßÊ†ºÂàÜÊûê' in message:
                start_background_task(user_uuid, message, 'psych_analysis')
                response_text = "„Åä„Å£„Åë„ÉºÔºÅ„ÅÇ„Å™„Åü„ÅÆ„Åì„Å®„ÄÅÂàÜÊûê„Åó„Å°„ÇÉ„ÅÜ„Å≠ÔºÅ„Å°„Çá„Å£„Å®ÊôÇÈñì„Åã„Åã„Çã„Åã„ÇÇÔºÅ"
            elif ('„Åï„Åè„Çâ„Åø„Åì' in message or '„Åø„Åì„Å°' in message):
                for keyword, resp in get_sakuramiko_special_responses().items():
                    if keyword in message:
                        response_text = resp; break
                if not response_text:
                    response_text = generate_ai_response(user_data, message, history, "„Åï„Åè„Çâ„Åø„Åì„ÅØ„Éõ„É≠„É©„Ç§„ÉñÊâÄÂ±û„ÅÆ‰∫∫Ê∞óVTuber„ÄÇÁã¨Áâπ„Å™Âè£Áôñ„ÇÑ„Ç≤„Éº„É†ÂÆüÊ≥Å„Åå‰∫∫Ê∞ó„ÄÇ")
            elif (member_name := is_holomem_name_only_request(message)):
                member_info = get_holomem_info(session, member_name)
                if member_info:
                    response_text = generate_ai_response(user_data, f"{member_name}„Å´„Å§„ÅÑ„Å¶Êïô„Åà„Å¶", history, member_info.description)
                else:
                    start_background_task(user_uuid, message, 'search')
                    response_text = f"„Åî„ÇÅ„Çì„ÄÅ„Äå{message}„Äç„Å°„ÇÉ„Çì„ÅÆË©≥„Åó„ÅÑÊÉÖÂ†±„ÅØÊåÅ„Å£„Å¶„Å™„ÅÑ„ÇÑ‚Ä¶„ÄÇWeb„ÅßË™ø„Åπ„Å¶„Åø„Çã„Å≠ÔºÅ"
            elif (selected_number := is_number_selection(message)):
                user_context = get_user_context(session, user_uuid)
                
                if user_context and user_context['type'] == 'hololive_news':
                    news_detail = get_cached_news_detail(session, user_uuid, selected_number)
                    if news_detail:
                        response_text = generate_ai_response(user_data, f"{news_detail.title}„Å´„Å§„ÅÑ„Å¶Êïô„Åà„Å¶", history, news_detail.content, is_detailed=True)
                    else:
                        response_text = "„ÅÇ„Çå„ÄÅ„Åù„ÅÆÁï™Âè∑„ÅÆ„Éã„É•„Éº„Çπ„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑ„ÇÑ‚Ä¶"
                else: 
                    saved_result = get_saved_search_result(user_uuid, selected_number)
                    if saved_result:
                        prompt = f"„Äå{saved_result['title']}„Äç„Å´„Å§„ÅÑ„Å¶Ë©≥„Åó„ÅèÊïô„Åà„Å¶ÔºÅ"
                        response_text = generate_ai_response(user_data, prompt, history, saved_result['full_content'], is_detailed=True)
                    else:
                        response_text = "„ÅÇ„Çå„ÄÅ‰Ωï„ÅÆÁï™Âè∑„Å†„Å£„ÅëÔºü„ÇÇ„ÅÜ‰∏ÄÂõûÊ§úÁ¥¢„Åó„Å¶„Åø„Å¶ÔºÅ"
            elif is_follow_up_question(message, history):
                last_assistant_msg = next((h.content for h in history if h.role == 'assistant'), "")
                response_text = generate_ai_response(user_data, message, history, f"Áõ¥Ââç„ÅÆÂõûÁ≠î: {last_assistant_msg}", is_detailed=True)
            elif is_time_request(message):
                response_text = get_japan_time()
            elif (location := is_weather_request(message)):
                response_text = get_weather_forecast(location)
            elif should_search(message):
                start_background_task(user_uuid, message, 'search')
                response_text = "„Åä„Å£„Åë„Éº„ÄÅË™ø„Åπ„Å¶„Åø„Çã„Å≠ÔºÅÁµÇ„Çè„Å£„Åü„ÇâÊïô„Åà„ÇãÔºÅ"
            
            if not response_text:
                response_text = generate_ai_response(user_data, message, history)
            
            response_text = limit_text_for_sl(response_text)
            session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=response_text))
            session.commit()
            
            return Response(f"{response_text}|", mimetype='text/plain; charset=utf-8')

    except Exception as e:
        logger.error(f"Chat error: {e}")
        logger.error(traceback.format_exc())
        return Response("„Åî„ÇÅ„Çì„ÄÅ„Ç∑„Çπ„ÉÜ„É†„Ç®„É©„Éº„ÅåËµ∑„Åç„Å°„ÇÉ„Å£„Åü‚Ä¶|", status=500, mimetype='text/plain; charset=utf-8')

@app.route('/check_task', methods=['POST'])
def check_task_endpoint():
    try:
        user_uuid = request.json['user_uuid']
        with Session() as session:
            task = session.query(BackgroundTask).filter_by(user_uuid=user_uuid, status='completed').order_by(BackgroundTask.completed_at.desc()).first()
            if not task: return jsonify({'status': 'no_tasks'})
            
            response_text = ""
            if task.task_type == 'search':
                results = json.loads(task.result) if task.result else None
                if not results:
                    response_text = f"„Äå{task.query}„Äç„ÇíË™ø„Åπ„Åü„Åë„Å©ÊÉÖÂ†±„ÅåË¶ã„Å§„Åã„Çâ„Å™„Åã„Å£„Åü‚Ä¶"
                else:
                    save_search_context(user_uuid, results, task.query)
                    save_user_context(session, user_uuid, 'web_search', task.query)
                    list_items = [f"„Äê{r['number']}„Äë{r['title']}" for r in results]
                    response_text = f"„Åä„Åæ„Åü„ÅõÔºÅ„Äå{task.query}„Äç„Å´„Å§„ÅÑ„Å¶Ë™ø„Åπ„Å¶„Åç„Åü„ÇàÔºÅ\n" + "\n".join(list_items) + "\n\nÊ∞ó„Å´„Å™„ÇãÁï™Âè∑Êïô„Åà„Å¶ÔºÅ"
            elif task.task_type == 'psych_analysis':
                psych = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
                if psych and hasattr(psych, 'analysis_summary') and psych.analysis_summary:
                    response_text = f"ÂàÜÊûêÁµÇ„Çè„Å£„Åü„ÇàÔºÅ„ÅÇ„Å¶„ÅÉ„Åó„ÅåË¶ã„Åü„ÅÇ„Å™„Åü„ÅØ‚Ä¶„Äå{psych.analysis_summary}„Äç„Å£„Å¶ÊÑü„ÅòÔºÅ(‰ø°È†ºÂ∫¶: {psych.analysis_confidence}%)"
                else:
                    response_text = "ÂàÜÊûêÁµÇ„Çè„Å£„Åü„Åë„Å©„ÄÅ„Åæ„Å†„ÅÜ„Åæ„Åè„Åæ„Å®„ÇÅ„Çâ„Çå„Å™„ÅÑ„ÇÑ‚Ä¶"
            
            response_text = limit_text_for_sl(response_text)
            session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=response_text))
            session.delete(task)
            session.commit()
            
            return Response(json.dumps({'status': 'completed', 'response': response_text}, ensure_ascii=False), mimetype='application/json; charset=utf-8')
            
    except Exception as e:
        logger.error(f"Check task error: {e}", exc_info=True)
        return jsonify({'error': 'Internal server error'}), 500

# --- „Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥Ëµ∑Âãï ---
def initialize_app():
    global engine, Session, groq_client
    logger.info("="*50)
    logger.info("üîß Mochiko AI (Final Ver.) Starting Up...")
    logger.info("="*50)
    
    if GROQ_API_KEY and len(GROQ_API_KEY) > 20:
        try:
            groq_client = Groq(api_key=GROQ_API_KEY)
            logger.info("üîç Verifying Groq API key...")
            groq_client.chat.completions.create(messages=[{"role": "user", "content": "test"}], model="llama-3.1-8b-instant", max_tokens=2)
            logger.info("‚úÖ Groq API key is valid and working.")
        except Exception as e:
            logger.critical("üî•üî•üî• FATAL: Groq API key verification failed! üî•üî•üî•")
            groq_client = None
    else:
        logger.warning("‚ö†Ô∏è GROQ_API_KEY is not set or too short. AI features will be disabled.")
        groq_client = None

    is_sqlite = 'sqlite' in DATABASE_URL
    connect_args = {'check_same_thread': False} if is_sqlite else {}
    engine = create_engine(DATABASE_URL, connect_args=connect_args, pool_pre_ping=True)

    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    
    initialize_holomem_wiki()
    
    def run_scheduler():
        while True: 
            schedule.run_pending()
            time.sleep(60)

    threading.Thread(target=run_scheduler, daemon=True).start()
    
    logger.info("‚úÖ Initialization Complete!")

application = None
try:
    initialize_app()
    application = app
except Exception as e:
    logger.critical(f"Fatal init error: {e}", exc_info=True)
    application = Flask(__name__)

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 10000))
    application.run(host='0.0.0.0', port=port, debug=False)
