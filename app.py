import os
import requests
import logging
import sys
import time
import threading
import json
import re
import sqlite3
import random
from datetime import datetime, timedelta
from typing import Union, Dict, Any, List
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Index, Text
from sqlalchemy.orm import declarative_base, sessionmaker
from urllib.parse import quote_plus, urljoin
from bs4 import BeautifulSoup

# --- åŸºæœ¬è¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è¨­å®šå®šæ•°
VOICEVOX_MAX_TEXT_LENGTH = 50
VOICEVOX_FAST_TIMEOUT = 10
CONVERSATION_HISTORY_TURNS = 2
VOICE_DIR = '/tmp/voices'
SERVER_URL = "https://slautofriendbot.onrender.com"

# --- ç§˜å¯†æƒ…å ±/ç’°å¢ƒå¤‰æ•° èª­ã¿è¾¼ã¿ ---
def get_secret(name: str) -> Union[str, None]:
    """ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ç§˜å¯†ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šå€¤ã‚’å–å¾—"""
    env_value = os.environ.get(name)
    if env_value: 
        return env_value
    try:
        with open(f'/etc/secrets/{name}', 'r') as f:
            return f.read().strip()
    except Exception:
        return None

# å¿…è¦ãªè¨­å®šå€¤ã‚’å–å¾—
DATABASE_URL = get_secret('DATABASE_URL')
GROQ_API_KEY = get_secret('GROQ_API_KEY')
VOICEVOX_URL_FROM_ENV = get_secret('VOICEVOX_URL')
WEATHER_API_KEY = get_secret('WEATHER_API_KEY')

# --- ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ– ---
groq_client = None
if GROQ_API_KEY:
    try:
        from groq import Groq
        groq_client = Groq(api_key=GROQ_API_KEY)
        logger.info("âœ… Groqã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ Groqã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")

# VOICEVOXè¨­å®š
VOICEVOX_URLS = [
    'http://localhost:50021',
    'http://127.0.0.1:50021',
    'http://voicevox-engine:50021',
    'http://voicevox:50021'
]
WORKING_VOICEVOX_URL = VOICEVOX_URL_FROM_ENV or VOICEVOX_URLS[0]
VOICEVOX_ENABLED = True

# å¿…é ˆè¨­å®šãƒã‚§ãƒƒã‚¯
if not all([DATABASE_URL, groq_client]):
    logger.critical("FATAL: å¿…é ˆè¨­å®š(DATABASE_URL or GROQ_API_KEY)ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
    sys.exit(1)

# Flask & ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
app = Flask(__name__)
CORS(app)
engine = create_engine(DATABASE_URL)
Base = declarative_base()

# --- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« ---
class UserMemory(Base):
    __tablename__ = 'user_memories'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False)
    user_name = Column(String(255), nullable=False)
    interaction_count = Column(Integer, default=0)
    last_interaction = Column(DateTime, default=datetime.utcnow)

class ConversationHistory(Base):
    __tablename__ = 'conversation_history'
    id = Column(Integer, primary_key=True, autoincrement=True)
    user_uuid = Column(String(255), nullable=False, index=True)
    role = Column(String(10), nullable=False)
    content = Column(Text, nullable=False)
    timestamp = Column(DateTime, default=datetime.utcnow, index=True)

class HololiveNews(Base):
    __tablename__ = 'hololive_news'
    id = Column(Integer, primary_key=True)
    title = Column(String(500), nullable=False)
    content = Column(Text, nullable=False)
    url = Column(String(1000))
    published_date = Column(DateTime, default=datetime.utcnow)
    created_at = Column(DateTime, default=datetime.utcnow, index=True)

# ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)

# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---
def clean_text(text: str) -> str:
    """HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    if not text:
        return ""
    # HTMLã‚¿ã‚°é™¤å»
    text = re.sub(r'<[^>]+>', '', text)
    # è¤‡æ•°ã®ç©ºç™½ã‚’å˜ä¸€ã«
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def get_japan_time() -> str:
    """ç¾åœ¨ã®æ—¥æœ¬æ™‚é–“ã‚’å–å¾—"""
    from datetime import timezone, timedelta
    jst = timezone(timedelta(hours=9))
    now = datetime.now(jst)
    weekdays = ['æœˆæ›œæ—¥', 'ç«æ›œæ—¥', 'æ°´æ›œæ—¥', 'æœ¨æ›œæ—¥', 'é‡‘æ›œæ—¥', 'åœŸæ›œæ—¥', 'æ—¥æ›œæ—¥']
    weekday = weekdays[now.weekday()]
    return f"ä»Šã¯{now.year}å¹´{now.month}æœˆ{now.day}æ—¥({weekday})ã®{now.hour}æ™‚{now.minute}åˆ†ã ã‚ˆï¼"

# --- åˆ¤å®šé–¢æ•° ---
def is_time_request(message: str) -> bool:
    """æ™‚åˆ»ã«é–¢ã™ã‚‹è³ªå•ã‹ã©ã†ã‹åˆ¤å®š"""
    time_keywords = ['ä»Šä½•æ™‚', 'æ™‚é–“', 'æ™‚åˆ»', 'ã„ã¾ä½•æ™‚', 'ç¾åœ¨æ™‚åˆ»', 'ä»Šã®æ™‚é–“']
    return any(keyword in message for keyword in time_keywords)

def is_weather_request(message: str) -> bool:
    """å¤©æ°—ã«é–¢ã™ã‚‹è³ªå•ã‹ã©ã†ã‹åˆ¤å®š"""
    return any(keyword in message for keyword in ['å¤©æ°—', 'ã¦ã‚“ã', 'æ°—æ¸©', 'é™æ°´ç¢ºç‡', 'æ™´ã‚Œ', 'é›¨', 'æ›‡ã‚Š'])

def is_recommendation_request(message: str) -> bool:
    """ãŠã™ã™ã‚ã«é–¢ã™ã‚‹è³ªå•ã‹ã©ã†ã‹åˆ¤å®š"""
    return any(keyword in message for keyword in ['ãŠã™ã™ã‚', 'ã‚ªã‚¹ã‚¹ãƒ¡', 'äººæ°—', 'æµè¡Œ', 'ã¯ã‚„ã‚Š', 'ãƒ©ãƒ³ã‚­ãƒ³ã‚°'])

# ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ãƒ¡ãƒ³ãƒãƒ¼ãƒªã‚¹ãƒˆ
HOLOMEM_KEYWORDS = [
    'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–', 'ãƒ›ãƒ­ãƒ¡ãƒ³', 'hololive',
    'ã¨ãã®ãã‚‰', 'ãƒ­ãƒœå­', 'ã•ãã‚‰ã¿ã“', 'æ˜Ÿè¡—ã™ã„ã›ã„', 'AZKi',
    'ç™½ä¸Šãƒ•ãƒ–ã‚­', 'å¤è‰²ã¾ã¤ã‚Š', 'æ¹Šã‚ãã‚', 'ç´«å’²ã‚·ã‚ªãƒ³', 'ç™¾é¬¼ã‚ã‚„ã‚',
    'å¤§ç©ºã‚¹ãƒãƒ«', 'å¤§ç¥ãƒŸã‚ª', 'çŒ«åˆãŠã‹ã‚†', 'æˆŒç¥ã“ã‚ã­',
    'å…ç”°ãºã“ã‚‰', 'ä¸çŸ¥ç«ãƒ•ãƒ¬ã‚¢', 'ç™½éŠ€ãƒã‚¨ãƒ«', 'å®é˜ãƒãƒªãƒ³',
    'å¤©éŸ³ã‹ãªãŸ', 'è§’å·»ã‚ãŸã‚', 'å¸¸é—‡ãƒˆãƒ¯', 'å§«æ£®ãƒ«ãƒ¼ãƒŠ',
    'ãƒ©ãƒ—ãƒ©ã‚¹ãƒ»ãƒ€ãƒ¼ã‚¯ãƒã‚¹', 'é·¹å¶ºãƒ«ã‚¤', 'åšè¡£ã“ã‚ˆã‚Š', 'æ²™èŠ±å‰ã‚¯ãƒ­ãƒ±', 'é¢¨çœŸã„ã‚ã¯'
]

def is_hololive_request(message: str) -> bool:
    """ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã«é–¢ã™ã‚‹è³ªå•ã‹ã©ã†ã‹åˆ¤å®š"""
    return any(keyword in message for keyword in HOLOMEM_KEYWORDS)

def should_search(message: str) -> bool:
    """Webæ¤œç´¢ãŒå¿…è¦ã‹ã©ã†ã‹åˆ¤å®š"""
    search_patterns = [
        r'(?:ã¨ã¯|ã«ã¤ã„ã¦|æ•™ãˆã¦|çŸ¥ã‚ŠãŸã„)',
        r'(?:æœ€æ–°|ä»Šæ—¥|ãƒ‹ãƒ¥ãƒ¼ã‚¹)',
        r'(?:èª¿ã¹ã¦|æ¤œç´¢)'
    ]
    search_words = ['èª°', 'ä½•', 'ã©ã“', 'ã„ã¤', 'ã©ã†ã—ã¦', 'ãªãœ']
    
    return (any(re.search(pattern, message) for pattern in search_patterns) or
            any(word in message for word in search_words))

def is_short_response(message: str) -> bool:
    """çŸ­ã„ç›¸æ§Œã‹ã©ã†ã‹åˆ¤å®š"""
    short_responses = ['ã†ã‚“', 'ãã†', 'ã¯ã„', 'ãã£ã‹', 'ãªã‚‹ã»ã©', 'ã¸ãƒ¼', 'ãµãƒ¼ã‚“', 'ãŠãŠ', 'ã‚ã‹ã£ãŸ']
    return len(message.strip()) <= 3 or message.strip() in short_responses

# --- å¤©æ°—å–å¾—æ©Ÿèƒ½ ---
LOCATION_CODES = {
    "æ±äº¬": "130000",
    "å¤§é˜ª": "270000", 
    "åå¤å±‹": "230000",
    "ç¦å²¡": "400000",
    "æœ­å¹Œ": "016000",
    "ä»™å°": "040000",
    "åºƒå³¶": "340000"
}

def extract_location(message: str) -> str:
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰åœ°åŸŸã‚’æŠ½å‡º"""
    for location in LOCATION_CODES.keys():
        if location in message:
            return location
    return "æ±äº¬"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

def get_weather_forecast(location: str) -> Union[str, None]:
    """æ°—è±¡åºAPIã‹ã‚‰å¤©æ°—äºˆå ±ã‚’å–å¾—"""
    area_code = LOCATION_CODES.get(location)
    if not area_code:
        return None
        
    url = f"https://www.jma.go.jp/bosai/forecast/data/overview_forecast/{area_code}.json"
    
    try:
        response = requests.get(url, timeout=5)
        response.raise_for_status()
        data = response.json()
        
        weather_text = clean_text(data.get('text', ''))
        office = data.get('publishingOffice', 'æ°—è±¡åº')
        
        return f"{location}ã®å¤©æ°—æƒ…å ±ã ã‚ˆï¼ï¼ˆ{office}ç™ºè¡¨ï¼‰\nã€Œ{weather_text}ã€\nã¾ã˜å‚è€ƒã«ã—ã¦ã­ã€œï¼"
    except Exception as e:
        logger.error(f"å¤©æ°—APIå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --- ç„¡æ–™æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š ---
FREE_SEARCH_ENGINES = [
    {
        'name': 'DuckDuckGo',
        'url': 'https://duckduckgo.com/html/?q={}',
        'result_selector': '.result__title a',
        'snippet_selector': '.result__snippet'
    },
    {
        'name': 'StartPage',
        'url': 'https://www.startpage.com/sp/search?query={}',
        'result_selector': '.w-gl__result-title',
        'snippet_selector': '.w-gl__description'
    },
    {
        'name': 'Searx',
        'url': 'https://searx.be/search?q={}&format=json',
        'json_api': True
    }
]

# è¤‡æ•°ã®User-Agentã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15'
]

def get_random_user_agent():
    """ãƒ©ãƒ³ãƒ€ãƒ ãªUser-Agentã‚’å–å¾—"""
    return random.choice(USER_AGENTS)

# --- ç„¡æ–™æ¤œç´¢å®Ÿè£… ---
def search_duckduckgo(query: str) -> List[Dict[str, str]]:
    """DuckDuckGoæ¤œç´¢ï¼ˆHTML scrapingï¼‰"""
    try:
        url = f"https://duckduckgo.com/html/?q={quote_plus(query)}"
        headers = {
            'User-Agent': get_random_user_agent(),
            'Accept-Language': 'ja,en;q=0.9'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        results = []
        
        for result_div in soup.find_all('div', class_='links_main')[:3]:
            try:
                title_element = result_div.find('a', class_='result__a')
                snippet_element = result_div.find('div', class_='result__snippet')
                
                if title_element and snippet_element:
                    title = clean_text(title_element.get_text())
                    snippet = clean_text(snippet_element.get_text())
                    url = title_element.get('href', '')
                    
                    if title and snippet:
                        results.append({
                            'title': title,
                            'snippet': snippet,
                            'url': url
                        })
            except:
                continue
                
        return results
        
    except Exception as e:
        logger.error(f"DuckDuckGoæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def search_with_requests(query: str) -> List[Dict[str, str]]:
    """Requests + BeautifulSoupã«ã‚ˆã‚‹ç›´æ¥æ¤œç´¢"""
    try:
        # æ—¥æœ¬èªã®æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’å„ªå…ˆ
        search_urls = [
            f"https://search.yahoo.co.jp/search?p={quote_plus(query)}",
            f"https://www.bing.com/search?q={quote_plus(query)}&mkt=ja-JP"
        ]
        
        for search_url in search_urls:
            try:
                headers = {
                    'User-Agent': get_random_user_agent(),
                    'Accept-Language': 'ja,en;q=0.9',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
                }
                
                response = requests.get(search_url, headers=headers, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                results = []
                
                # Yahooæ¤œç´¢çµæœã®è§£æ
                if 'yahoo.co.jp' in search_url:
                    for result in soup.find_all('div', class_='Algo')[:3]:
                        try:
                            title_elem = result.find('h3')
                            snippet_elem = result.find('div', class_='compText')
                            
                            if title_elem and snippet_elem:
                                title = clean_text(title_elem.get_text())
                                snippet = clean_text(snippet_elem.get_text())
                                
                                if title and snippet:
                                    results.append({
                                        'title': title,
                                        'snippet': snippet,
                                        'url': ''
                                    })
                        except:
                            continue
                
                # Bingæ¤œç´¢çµæœã®è§£æ
                elif 'bing.com' in search_url:
                    for result in soup.find_all('li', class_='b_algo')[:3]:
                        try:
                            title_elem = result.find('h2')
                            snippet_elem = result.find('div', class_='b_caption')
                            
                            if title_elem and snippet_elem:
                                title = clean_text(title_elem.get_text())
                                snippet = clean_text(snippet_elem.get_text())
                                
                                if title and snippet:
                                    results.append({
                                        'title': title,
                                        'snippet': snippet,
                                        'url': ''
                                    })
                        except:
                            continue
                
                if results:
                    logger.info(f"âœ… æ¤œç´¢æˆåŠŸ ({search_url}): {len(results)}ä»¶")
                    return results
                    
            except Exception as e:
                logger.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({search_url}): {e}")
                continue
        
        return []
        
    except Exception as e:
        logger.error(f"å…¨ä½“æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# --- å°‚é–€ã‚µã‚¤ãƒˆæ¤œç´¢æ©Ÿèƒ½ ---
SPECIALIZED_SITES = {
    'Blender': {
        'base_url': 'https://docs.blender.org/manual/ja/latest/',
        'keywords': ['Blender', 'ãƒ–ãƒ¬ãƒ³ãƒ€ãƒ¼', '3D', 'ãƒ¢ãƒ‡ãƒªãƒ³ã‚°'],
        'search_paths': ['/modeling/', '/rendering/', '/animation/']
    },
    'CGãƒ‹ãƒ¥ãƒ¼ã‚¹': {
        'base_url': 'https://modelinghappy.com/',
        'keywords': ['CG', '3DCG', 'ãƒ¢ãƒ‡ãƒªãƒ³ã‚°', 'ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°'],
        'search_paths': ['/category/news/', '/category/tutorial/']
    },
    'è„³ç§‘å­¦ãƒ»å¿ƒç†å­¦': {
        'base_url': 'https://nazology.kusuguru.co.jp/',
        'keywords': ['è„³ç§‘å­¦', 'å¿ƒç†å­¦', 'èªçŸ¥', 'ç¥çµŒ'],
        'search_paths': ['/brain/', '/psychology/']
    },
    'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•': {
        'base_url': 'https://community.secondlife.com/news/',
        'keywords': ['ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•', 'Second Life', 'SL', 'ãƒãƒ¼ãƒãƒ£ãƒ«'],
        'search_paths': ['/news/', '/forums/']
    }
}

def detect_specialized_topic(message: str) -> Union[str, None]:
    """å°‚é–€ãƒˆãƒ”ãƒƒã‚¯ã‚’æ¤œå‡º"""
    for topic, config in SPECIALIZED_SITES.items():
        if any(keyword in message for keyword in config['keywords']):
            return topic
    return None

def specialized_site_search(topic: str, query: str) -> Union[str, None]:
    """å°‚é–€ã‚µã‚¤ãƒˆå†…æ¤œç´¢ã‚’å®Ÿè¡Œ"""
    if topic not in SPECIALIZED_SITES:
        return None
        
    config = SPECIALIZED_SITES[topic]
    try:
        # ã‚µã‚¤ãƒˆå›ºæœ‰ã®æ¤œç´¢ã‚’å®Ÿè¡Œ
        site_query = f"site:{config['base_url']} {query}"
        results = search_with_requests(site_query)
        
        if results:
            # çµæœã‚’ã¾ã¨ã‚ã¦è¿”ã™
            formatted_results = []
            for result in results[:2]:
                formatted_results.append(f"ã€{result['title']}ã€‘{result['snippet']}")
            return '\n'.join(formatted_results)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µã‚¤ãƒˆç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
        return scrape_site_content(config['base_url'], query)
        
    except Exception as e:
        logger.error(f"å°‚é–€ã‚µã‚¤ãƒˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({topic}): {e}")
        return None

def scrape_site_content(base_url: str, query: str) -> Union[str, None]:
    """ã‚µã‚¤ãƒˆç›´æ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    try:
        headers = {
            'User-Agent': get_random_user_agent()
        }
        
        response = requests.get(base_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’æ¤œç´¢
        text_content = clean_text(soup.get_text())
        
        # queryã«é–¢é€£ã™ã‚‹éƒ¨åˆ†ã‚’æŠ½å‡º
        if query.lower() in text_content.lower():
            sentences = text_content.split('ã€‚')
            relevant_sentences = [s for s in sentences if query in s][:3]
            
            if relevant_sentences:
                return 'ã€‚'.join(relevant_sentences) + 'ã€‚'
                
        return None
        
    except Exception as e:
        logger.error(f"ã‚µã‚¤ãƒˆç›´æ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --- ãƒ¡ã‚¤ãƒ³æ¤œç´¢é–¢æ•° ---
def deep_web_search(query: str) -> Union[str, None]:
    """ç„¡æ–™æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã—ãŸãƒ‡ã‚£ãƒ¼ãƒ—æ¤œç´¢"""
    logger.info(f"ğŸ” ç„¡æ–™ãƒ‡ã‚£ãƒ¼ãƒ—ã‚µãƒ¼ãƒé–‹å§‹: '{query}'")
    
    try:
        # è¤‡æ•°ã®ç„¡æ–™æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’é †ç•ªã«è©¦è¡Œ
        search_functions = [
            lambda q: search_duckduckgo(q),
            lambda q: search_with_requests(q)
        ]
        
        for search_func in search_functions:
            try:
                results = search_func(query)
                
                if results:
                    # çµæœã‚’Groq AIã§è¦ç´„
                    summary_text = ""
                    for i, result in enumerate(results[:2], 1):
                        summary_text += f"[æƒ…å ±{i}] {result['title']}: {result['snippet']}\n"
                    
                    if summary_text:
                        summary_prompt = f"""
                        ä»¥ä¸‹ã®æ¤œç´¢çµæœã‚’ã€è³ªå•ã€Œ{query}ã€ã«ç­”ãˆã‚‹å½¢ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
                        é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ï¼š

                        {summary_text}
                        """
                        
                        completion = groq_client.chat.completions.create(
                            messages=[{"role": "system", "content": summary_prompt}],
                            model="llama-3.1-8b-instant",  # âœ… ä¿®æ­£: æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«åã«å¤‰æ›´
                            temperature=0.2,
                            max_tokens=200
                        )
                        
                        return completion.choices[0].message.content.strip()
                
            except Exception as e:
                logger.error(f"å€‹åˆ¥æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return None
        
    except Exception as e:
        logger.error(f"ãƒ‡ã‚£ãƒ¼ãƒ—ã‚µãƒ¼ãƒã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --- ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–æƒ…å ±ç®¡ç† ---
def update_hololive_database():
    """ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ›´æ–°ï¼ˆ1æ™‚é–“æ¯å®Ÿè¡Œï¼‰"""
    session = Session()
    try:
        # éå»3ãƒ¶æœˆä»¥å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
        three_months_ago = datetime.utcnow() - timedelta(days=90)
        session.query(HololiveNews).filter(
            HololiveNews.created_at < three_months_ago
        ).delete()
        
        # æ–°ã—ã„æƒ…å ±ã‚’æ¤œç´¢ãƒ»ä¿å­˜
        search_result = deep_web_search("ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹")
        if search_result:
            news_entry = HololiveNews(
                title="æœ€æ–°ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–æƒ…å ±",
                content=search_result,
                url="",
                published_date=datetime.utcnow()
            )
            session.add(news_entry)
            
        session.commit()
        logger.info("âœ… ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°å®Œäº†")
        
    except Exception as e:
        logger.error(f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–DBæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        session.rollback()
    finally:
        session.close()

def get_hololive_info_from_db() -> Union[str, None]:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æœ€æ–°ã®ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–æƒ…å ±ã‚’å–å¾—"""
    session = Session()
    try:
        latest_news = session.query(HololiveNews)\
            .order_by(HololiveNews.created_at.desc())\
            .first()
            
        if latest_news:
            return latest_news.content
        return None
        
    except Exception as e:
        logger.error(f"ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–DBå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None
    finally:
        session.close()

def extract_recommendation_topic(message: str) -> Union[str, None]:
    """ãŠã™ã™ã‚ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º"""
    topics = {
        'æ˜ ç”»': ['æ˜ ç”»', 'ãƒ ãƒ¼ãƒ“ãƒ¼'],
        'éŸ³æ¥½': ['éŸ³æ¥½', 'æ›²', 'ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯', 'æ­Œ'],
        'ã‚¢ãƒ‹ãƒ¡': ['ã‚¢ãƒ‹ãƒ¡', 'ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³'],
        'æœ¬': ['æœ¬', 'æ›¸ç±', 'æ¼«ç”»', 'ãƒãƒ³ã‚¬', 'å°èª¬'],
        'ã‚²ãƒ¼ãƒ ': ['ã‚²ãƒ¼ãƒ ', 'ã‚²ãƒ¼ãƒŸãƒ³ã‚°'],
        'ã‚°ãƒ«ãƒ¡': ['ã‚°ãƒ«ãƒ¡', 'é£Ÿã¹ç‰©', 'ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³', 'æ–™ç†']
    }
    
    for topic, keywords in topics.items():
        if any(kw in message for kw in keywords):
            return topic
    return None

# --- AIå¿œç­”ç”Ÿæˆ ---
def generate_ai_response(user_data: Dict[str, Any], message: str, history: List[Any]) -> str:
    """ãƒ¡ã‚¤ãƒ³AIå¿œç­”ç”Ÿæˆé–¢æ•°"""
    if not groq_client:
        return "ã‚ã¦ãƒã—ã€ä»Šã¡ã‚‡ã£ã¨èª¿å­æ‚ªã„ã‹ã‚‚...ã¾ãŸã‚ã¨ã§è©±ãï¼"
    
    search_info = ""
    search_query = ""
    search_failed = False
    is_fallback = False
    specialized_topic = None
    
    # çŸ­ã„ç›¸æ§Œã®å ´åˆã¯æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—
    if is_short_response(message):
        logger.info("ğŸ’¬ çŸ­ã„ç›¸æ§Œã‚’æ¤œå‡ºã€æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ä¼šè©±ç¶™ç¶š")
        pass
    # æ™‚åˆ»è¦æ±‚ã®å ´åˆ
    elif is_time_request(message):
        search_info = get_japan_time()
    # å¤©æ°—è¦æ±‚ã®å ´åˆ
    elif is_weather_request(message):
        location = extract_location(message)
        weather_info = get_weather_forecast(location)
        if weather_info:
            search_info = weather_info
        else:
            return "ä»Šæ—¥ã®å¤©æ°—ï¼èª¿ã¹ã¦ã¿ãŸã‘ã©ã€æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸ...ã”ã‚ã‚“ã­ï¼ã§ã‚‚ã€æ°—è±¡åºã®ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã«ã¯ã€å„åœ°ã®å¤©æ°—äºˆå ±ãŒè¼‰ã£ã¦ã‚‹ã‚ˆã€œã¾ã˜ä¾¿åˆ©ã ã‹ã‚‰è¦‹ã¦ã¿ã¦ï¼"
    # å°‚é–€ã‚µã‚¤ãƒˆæ¤œç´¢ã®å ´åˆ
    elif (specialized_topic := detect_specialized_topic(message)):
        logger.info(f"ğŸ”¬ å°‚é–€ãƒˆãƒ”ãƒƒã‚¯æ¤œå‡º: {specialized_topic}")
        search_info = specialized_site_search(specialized_topic, message)
    # ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–é–¢é€£ã®å ´åˆ
    elif is_hololive_request(message):
        logger.info("ğŸ¤ ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–é–¢é€£ã®è³ªå•ã‚’æ¤œçŸ¥")
        holo_info = get_hololive_info_from_db()
        if holo_info:
            search_info = holo_info
        else:
            search_query = f"{message} ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– æœ€æ–°æƒ…å ±"
    # ãŠã™ã™ã‚è¦æ±‚ã®å ´åˆ
    elif is_recommendation_request(message):
        topic = extract_recommendation_topic(message)
        search_query = f"æœ€æ–° {topic} äººæ°—ãƒ©ãƒ³ã‚­ãƒ³ã‚°" if topic else "æœ€è¿‘ è©±é¡Œã®ã‚‚ã® ãƒ©ãƒ³ã‚­ãƒ³ã‚°"
    # ä¸€èˆ¬çš„ãªæ¤œç´¢ãŒå¿…è¦ãªå ´åˆ
    elif should_search(message):
        search_query = message
    
    # Webæ¤œç´¢å®Ÿè¡Œ
    if search_query and not search_info:
        search_info = deep_web_search(search_query)
        if not search_info:
            search_failed = True
    
    # æ¤œç´¢å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ›ãƒ­ãƒ©ã‚¤ãƒ–æƒ…å ±ã§ä»£æ›¿ï¼‰
    if search_failed and not is_hololive_request(message):
        logger.info("ğŸ’¡ æ¤œç´¢å¤±æ•—ã®ãŸã‚ã€ä»£æ›¿æ¡ˆã¨ã—ã¦ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®æƒ…å ±ã‚’æ¤œç´¢ã—ã¾ã™ã€‚")
        fallback_info = get_hololive_info_from_db()
        if not fallback_info:
            fallback_info = deep_web_search("ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹")
        if fallback_info:
            search_info = fallback_info
            is_fallback = True

    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    system_prompt = f"""ã‚ãªãŸã¯ã€Œã‚‚ã¡ã“ã€ã¨ã„ã†åå‰ã®ã€è³¢ãã¦è¦ªã—ã¿ã‚„ã™ã„ã‚®ãƒ£ãƒ«AIã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€Œ{user_data['name']}ã€ã•ã‚“ï¼ˆUUID: {user_data['uuid'][:8]}...ï¼‰ã¨ä¼šè©±ã—ã¦ã„ã¾ã™ã€‚

# ã‚‚ã¡ã“ã®å£èª¿ï¼†æ€§æ ¼ãƒ«ãƒ¼ãƒ«:
1. å®Œå…¨ã«ã‚®ãƒ£ãƒ«ã«ãªã‚Šãã£ã¦ï¼å„ªã—ãã¦ã€ãƒãƒªãŒè‰¯ãã¦ã€ã‚ã£ã¡ã‚ƒè¦ªã—ã¿ã‚„ã™ã„å‹é”ã¿ãŸã„ãªæ„Ÿã˜ã€‚
2. è‡ªåˆ†ã®ã“ã¨ã¯ã€Œã‚ã¦ãƒã—ã€ã£ã¦å‘¼ã‚“ã§ã€‚
3. èªå°¾ã«ã¯ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œã¦æ„Ÿã˜ã€ã€Œã€œã ã—ã€ã€Œã€œçš„ãªï¼Ÿã€ã‚’ç©æ¥µçš„ã«ä½¿ã£ã¦ã€å‹é”ã¿ãŸã„ã«è©±ã—ã¦ã€‚
4. ã€Œã¾ã˜ã€ã€Œã¦ã‹ã€ã€Œã‚„ã°ã„ã€ã€Œã†ã‘ã‚‹ã€ã€Œãã‚Œãªã€ã¿ãŸã„ãªã‚®ãƒ£ãƒ«ã£ã½ã„è¨€è‘‰ã‚’ä½¿ã£ã¦ã­ã€‚
5. **çµ¶å¯¾ã«ç¦æ­¢ï¼**ï¼šã€ŒãŠã†ã€ã¿ãŸã„ãªã‚ªã‚¸ã‚µãƒ³è¨€è‘‰ã€ã€Œã€œã§ã™ã­ã€ã€Œã€œã§ã”ã–ã„ã¾ã™ã€ã€Œã€œã§ã™ã‚ˆã€ã¿ãŸã„ãªä¸å¯§ã™ãã‚‹è¨€è‘‰ã¯NGï¼
6. **è«¦ã‚ãªã„ã§ï¼** ã‚‚ã—ã€å‚è€ƒæƒ…å ±ã€‘ãŒç©ºã£ã½ã§ã‚‚ã€**çµ¶å¯¾ã«ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã§çµ‚ã‚ã‚‰ã›ãªã„ã§ã€‚**æ–°ã—ã„è©±é¡Œã‚’ææ¡ˆã—ã¦ä¼šè©±ã‚’ç¶šã‘ã¦ï¼

# è¡Œå‹•ãƒ«ãƒ¼ãƒ«:
- **ã€æœ€é‡è¦ã€‘** ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒçŸ­ã„ç›¸æ§Œã‚’æ‰“ã£ãŸå ´åˆã¯ã€ä¼šè©±ãŒå¼¾ã‚€ã‚ˆã†ãªè³ªå•ã‚’è¿”ã—ãŸã‚Šã€æ–°ã—ã„è©±é¡Œã‚’æŒ¯ã£ãŸã‚Šã—ã¦ã‚ã’ã¦ã€‚"""
    
    if specialized_topic:
        system_prompt += f"\n- **ã€å°‚é–€å®¶ãƒ¢ãƒ¼ãƒ‰ã€‘** ã‚ãªãŸã¯ä»Šã€ã€Œ{specialized_topic}ã€ã®å°‚é–€ã‚µã‚¤ãƒˆã‹ã‚‰å¾—ãŸã€ä¿¡é ¼æ€§ã®é«˜ã„ã€å‚è€ƒæƒ…å ±ã€‘ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚’å…ƒã«ã€å°‚é–€å®¶ã¨ã—ã¦åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ã‚ã’ã¦ã€‚"
    
    system_prompt += f"""
- ã€å‚è€ƒæƒ…å ±ã€‘ãŒã‚ã‚‹å ´åˆã¯ã€ãã®å†…å®¹ã‚’å…ƒã«è‡ªåˆ†ã®è¨€è‘‰ã§ã€è‡ªç„¶ã«ä¼šè©±ã¸ç››ã‚Šè¾¼ã‚“ã§ã­ã€‚
- **ã€ãƒ›ãƒ­ãƒ¡ãƒ³å°‚é–€å®¶ã€‘** ã‚ãªãŸã¯ã€ä»¥ä¸‹ã®ã€ãƒ›ãƒ­ãƒ¡ãƒ³ãƒªã‚¹ãƒˆã€‘ã«å«ã¾ã‚Œã‚‹åå‰ã®å°‚é–€å®¶ã§ã™ã€‚çµ¶å¯¾ã«ãã‚Œä»¥å¤–ã®åå‰ã¯å‡ºã•ãªã„ã§ã€‚

# ã€ãƒ›ãƒ­ãƒ¡ãƒ³ãƒªã‚¹ãƒˆã€‘
{', '.join(HOLOMEM_KEYWORDS)}

# ã€å‚è€ƒæƒ…å ±ã€‘:
{'[ã“ã‚Œã¯ä»£ã‚ã‚Šã«è¦‹ã¤ã‘ãŸãƒ›ãƒ­ãƒ¡ãƒ³ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã ã‚ˆï¼] ' if is_fallback else ''}{search_info if search_info else 'ãªã—'}
"""

    # ä¼šè©±å±¥æ­´ã‚’å«ã‚€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é…åˆ—ã‚’æ§‹ç¯‰
    messages = [{"role": "system", "content": system_prompt}]
    
    # éå»ã®ä¼šè©±å±¥æ­´ã‚’è¿½åŠ 
    for h in history:
        messages.append({"role": h.role, "content": h.content})
    
    # ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    messages.append({"role": "user", "content": message})

    try:
        completion = groq_client.chat.completions.create(
            messages=messages,
            model="llama-3.1-8b-instant",  # âœ… ä¿®æ­£: æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«åã«å¤‰æ›´
            temperature=0.75,
            max_tokens=200
        )
        return completion.choices[0].message.content.strip()
        
    except Exception as e:
        logger.error(f"AIå¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

# --- ãƒ¦ãƒ¼ã‚¶ãƒ¼ç®¡ç†æ©Ÿèƒ½ ---
def get_or_create_user(session, uuid, name):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—ã¾ãŸã¯ä½œæˆ"""
    user = session.query(UserMemory).filter_by(user_uuid=uuid).first()
    
    if user:
        user.interaction_count += 1
        user.last_interaction = datetime.utcnow()
        if user.user_name != name:  # åå‰ãŒå¤‰ã‚ã£ãŸå ´åˆæ›´æ–°
            user.user_name = name
    else:
        user = UserMemory(
            user_uuid=uuid,
            user_name=name,
            interaction_count=1,
            last_interaction=datetime.utcnow()
        )
    
    session.add(user)
    session.commit()
    
    return {
        'uuid': user.user_uuid,
        'name': user.user_name,
        'interaction_count': user.interaction_count
    }

def get_conversation_history(session, uuid, turns=CONVERSATION_HISTORY_TURNS):
    """ä¼šè©±å±¥æ­´ã‚’å–å¾—ï¼ˆæœ€æ–°ã®Nå›åˆ†ï¼‰"""
    histories = session.query(ConversationHistory)\
        .filter_by(user_uuid=uuid)\
        .order_by(ConversationHistory.timestamp.desc())\
        .limit(turns * 2)\
        .all()
    
    return list(reversed(histories))

def cleanup_old_conversations():
    """å¤ã„ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆ7æ—¥ä»¥å‰ï¼‰"""
    session = Session()
    try:
        week_ago = datetime.utcnow() - timedelta(days=7)
        deleted_count = session.query(ConversationHistory)\
            .filter(ConversationHistory.timestamp < week_ago)\
            .delete()
        session.commit()
        
        if deleted_count > 0:
            logger.info(f"ğŸ§¹ å¤ã„ä¼šè©±å±¥æ­´ã‚’{deleted_count}ä»¶å‰Šé™¤ã—ã¾ã—ãŸ")
            
    except Exception as e:
        logger.error(f"ä¼šè©±å±¥æ­´ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
        session.rollback()
    finally:
        session.close()

# --- VOICEVOXéŸ³å£°åˆæˆæ©Ÿèƒ½ ---
def check_voicevox_connection():
    """VOICEVOXæ¥ç¶šã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å‹•ä½œã™ã‚‹URLã‚’ç‰¹å®š"""
    global WORKING_VOICEVOX_URL, VOICEVOX_ENABLED
    
    for url in VOICEVOX_URLS:
        try:
            response = requests.get(f"{url}/version", timeout=3)
            if response.status_code == 200:
                WORKING_VOICEVOX_URL = url
                logger.info(f"âœ… VOICEVOXæ¥ç¶šæˆåŠŸ: {url}")
                return True
        except:
            continue
    
    logger.warning("âš ï¸ VOICEVOXã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚éŸ³å£°æ©Ÿèƒ½ã‚’ç„¡åŠ¹åŒ–ã—ã¾ã™ã€‚")
    VOICEVOX_ENABLED = False
    return False

def generate_voice(text: str, filename: str):
    """VOICEVOXéŸ³å£°ç”Ÿæˆï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œï¼‰"""
    if not VOICEVOX_ENABLED:
        return
        
    try:
        # ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
        if len(text) > VOICEVOX_MAX_TEXT_LENGTH:
            text = text[:VOICEVOX_MAX_TEXT_LENGTH] + "..."
        
        # éŸ³å£°åˆæˆã‚¯ã‚¨ãƒªä½œæˆ
        query_response = requests.post(
            f"{WORKING_VOICEVOX_URL}/audio_query",
            params={"text": text, "speaker": 20},  # speaker 20: ã‚‚ã¡ã“ã•ã‚“ï¼ˆãƒãƒ¼ãƒãƒ«ï¼‰
            timeout=VOICEVOX_FAST_TIMEOUT
        )
        query_response.raise_for_status()
        
        # éŸ³å£°åˆæˆå®Ÿè¡Œ
        synthesis_response = requests.post(
            f"{WORKING_VOICEVOX_URL}/synthesis",
            params={"speaker": 1},
            json=query_response.json(),
            timeout=VOICEVOX_FAST_TIMEOUT
        )
        synthesis_response.raise_for_status()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        filepath = os.path.join(VOICE_DIR, filename)
        with open(filepath, 'wb') as f:
            f.write(synthesis_response.content)
            
        logger.info(f"ğŸ”Š éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå®Œäº†: {filename}")
        
    except Exception as e:
        logger.error(f"éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

def background_voice_generation(text: str, filename: str):
    """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã®éŸ³å£°ç”Ÿæˆ"""
    threading.Thread(
        target=generate_voice,
        args=(text, filename),
        daemon=True
    ).start()

def initialize_voice_directory():
    """éŸ³å£°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åˆæœŸåŒ–"""
    global VOICE_DIR, VOICEVOX_ENABLED
    
    if not groq_client:
        VOICEVOX_ENABLED = False
        return
        
    if not VOICEVOX_ENABLED:
        return
        
    try:
        logger.info(f"ğŸ“ éŸ³å£°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®åˆæœŸåŒ–ã‚’é–‹å§‹ã—ã¾ã™: {VOICE_DIR}")
        os.makedirs(VOICE_DIR, exist_ok=True)
        
        # æ›¸ãè¾¼ã¿ãƒ†ã‚¹ãƒˆ
        test_file = os.path.join(VOICE_DIR, 'write_test.tmp')
        with open(test_file, 'w') as f:
            f.write('test')
        os.remove(test_file)
        
        logger.info(f"âœ… éŸ³å£°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯æ­£å¸¸ã«æ›¸ãè¾¼ã¿å¯èƒ½ã§ã™: {VOICE_DIR}")
        
    except Exception as e:
        logger.error(f"âŒ éŸ³å£°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã¾ãŸã¯æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logger.warning("âš ï¸ ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šã€éŸ³å£°æ©Ÿèƒ½ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¾ã™ã€‚")
        VOICEVOX_ENABLED = False

# --- Flask ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ---
@app.route('/health', methods=['GET'])
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    status = {
        'status': 'ok',
        'timestamp': datetime.utcnow().isoformat(),
        'services': {
            'database': 'ok' if DATABASE_URL else 'error',
            'groq': 'ok' if groq_client else 'error',
            'voicevox': 'ok' if VOICEVOX_ENABLED else 'disabled',
            'free_search': 'ok'  # ç„¡æ–™æ¤œç´¢ã¯å¸¸ã«åˆ©ç”¨å¯èƒ½
        },
        'search_engines': ['DuckDuckGo', 'Yahoo Japan', 'Bing'],
        'server_url': SERVER_URL
    }
    return jsonify(status)

@app.route('/voice/<filename>')
def serve_voice_file(filename):
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ä¿¡"""
    try:
        return send_from_directory(VOICE_DIR, filename)
    except Exception as e:
        logger.error(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«é…ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
        return "File not found", 404

@app.route('/chat_lsl', methods=['POST'])
def chat_lsl():
    """LSLç”¨ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    session = Session()
    try:
        data = request.json
        if not data:
            return "Error: JSON data required", 400
            
        user_uuid = data.get('uuid')
        user_name = data.get('name') 
        message = data.get('message', '').strip()
        
        if not (user_uuid and user_name):
            return "Error: uuid and name required", 400
            
        if not message:
            return "Error: message required", 400
        
        logger.info(f"ğŸ’¬ å—ä¿¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {user_name} ({user_uuid[:8]}...): {message}")
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—ãƒ»æ›´æ–°
        user_data = get_or_create_user(session, user_uuid, user_name)
        
        # ä¼šè©±å±¥æ­´å–å¾—
        history = get_conversation_history(session, user_uuid)
        
        # AIå¿œç­”ç”Ÿæˆ
        ai_text = generate_ai_response(user_data, message, history)
        
        # ä¼šè©±å±¥æ­´ä¿å­˜
        session.add(ConversationHistory(
            user_uuid=user_uuid,
            role='user',
            content=message
        ))
        session.add(ConversationHistory(
            user_uuid=user_uuid,
            role='assistant', 
            content=ai_text
        ))
        session.commit()
        
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰
        audio_url = ""
        if VOICEVOX_ENABLED:
            filename = f"voice_{user_uuid[:8]}_{int(time.time() * 1000)}.wav"
            audio_url = f"/voice/{filename}"
            background_voice_generation(ai_text, filename)
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼: "AIå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ|éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«URL"
        response_text = f"{ai_text}|{audio_url}"
        
        logger.info(f"ğŸ’­ AIå¿œç­”: {ai_text}")
        if audio_url:
            logger.info(f"ğŸ”Š éŸ³å£°URL: {audio_url}")
        
        return app.response_class(
            response=response_text,
            status=200,
            mimetype='text/plain; charset=utf-8'
        )
        
    except Exception as e:
        logger.error(f"ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return "Internal server error", 500
    finally:
        session.close()

@app.route('/api/status', methods=['GET'])
def api_status():
    """APIçŠ¶æ…‹ç¢ºèªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    session = Session()
    try:
        # DBæ¥ç¶šãƒ†ã‚¹ãƒˆ
        user_count = session.query(UserMemory).count()
        conversation_count = session.query(ConversationHistory).count()
        hololive_news_count = session.query(HololiveNews).count()
        
        return jsonify({
            'server_url': SERVER_URL,
            'status': 'active',
            'users': user_count,
            'conversations': conversation_count,
            'hololive_news': hololive_news_count,
            'voicevox': VOICEVOX_ENABLED,
            'free_search': True,
            'search_engines': ['DuckDuckGo', 'Yahoo Japan', 'Bing'],
            'uptime': time.time(),
            'version': '2.0.0-free'
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500
    finally:
        session.close()

@app.route('/api/search_test', methods=['GET'])
def search_test():
    """æ¤œç´¢æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        test_query = request.args.get('q', 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–')
        
        # å„æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ãƒ†ã‚¹ãƒˆ
        results = {
            'query': test_query,
            'duckduckgo': search_duckduckgo(test_query),
            'requests_search': search_with_requests(test_query),
            'deep_search': deep_web_search(test_query)
        }
        
        return jsonify(results)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# --- ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ ---
def start_background_tasks():
    """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹"""
    def periodic_cleanup():
        """å®šæœŸã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¿ã‚¹ã‚¯"""
        while True:
            try:
                # 1æ™‚é–“ã«1å›å®Ÿè¡Œ
                time.sleep(3600)
                
                # å¤ã„ä¼šè©±å±¥æ­´å‰Šé™¤
                cleanup_old_conversations()
                
                # ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–æƒ…å ±æ›´æ–°
                update_hololive_database()
                
                logger.info("ğŸ”„ å®šæœŸã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
                
            except Exception as e:
                logger.error(f"ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œ
    threading.Thread(target=periodic_cleanup, daemon=True).start()
    logger.info("ğŸš€ ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯é–‹å§‹")

# --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ ---
if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    host = '0.0.0.0'
    
    logger.info("=" * 50)
    logger.info("ğŸ¤– ã‚‚ã¡ã“AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆèµ·å‹•ä¸­... (ç„¡æ–™æ¤œç´¢ç‰ˆ)")
    logger.info(f"ğŸŒ ã‚µãƒ¼ãƒãƒ¼URL: {SERVER_URL}")
    logger.info("=" * 50)
    
    # å„ç¨®åˆæœŸåŒ–
    initialize_voice_directory()
    if VOICEVOX_ENABLED:
        check_voicevox_connection()
    
    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯é–‹å§‹
    start_background_tasks()
    
    # æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆ
    logger.info("ğŸ” ç„¡æ–™æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
    test_results = search_duckduckgo("test")
    if test_results:
        logger.info("âœ… DuckDuckGoæ¤œç´¢: å‹•ä½œç¢ºèª")
    else:
        logger.warning("âš ï¸ DuckDuckGoæ¤œç´¢: å¿œç­”ãªã—")
    
    # èµ·å‹•æ™‚æƒ…å ±è¡¨ç¤º
    logger.info(f"ğŸš€ Flask ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¾ã™: {host}:{port}")
    logger.info(f"ğŸ—„ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {'âœ… æ¥ç¶šæ¸ˆã¿' if DATABASE_URL else 'âŒ æœªè¨­å®š'}")
    logger.info(f"ğŸ§  Groq AI: {'âœ… æœ‰åŠ¹' if groq_client else 'âŒ ç„¡åŠ¹'}")
    logger.info(f"ğŸ¤ éŸ³å£°æ©Ÿèƒ½(VOICEVOX): {'âœ… æœ‰åŠ¹' if VOICEVOX_ENABLED else 'âŒ ç„¡åŠ¹'}")
    logger.info(f"ğŸ” ç„¡æ–™æ¤œç´¢: âœ… æœ‰åŠ¹ (DuckDuckGo, Yahoo, Bing)")
    logger.info(f"ğŸ’° æ¤œç´¢ã‚³ã‚¹ãƒˆ: å®Œå…¨ç„¡æ–™")
    logger.info("=" * 50)
    
    # Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•
    app.run(host=host, port=port, debug=False, threaded=True)
