import sys
import os
import requests
import logging
import time
import threading
import json
import re
import random
import hashlib
import unicodedata
import traceback 
from datetime import datetime, timedelta, timezone
from groq import Groq
from flask import Response, send_from_directory
from urllib.parse import quote_plus, urljoin
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import schedule
import signal
from threading import Lock

# --- å‹ãƒ’ãƒ³ãƒˆ ---
try:
    from typing import Union, Dict, Any, List, Optional
except ImportError:
    Dict, Any, List, Union, Optional = dict, object, list, object, object

from flask import Flask, request, jsonify
from flask_cors import CORS
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Text, Boolean, text
from sqlalchemy.orm import declarative_base, sessionmaker

# --- åŸºæœ¬è¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- å®šæ•° ---
SERVER_URL = os.environ.get('RENDER_EXTERNAL_URL', 'http://localhost:10000')
VOICE_DIR = '/tmp/voices'
VOICEVOX_SPEAKER_ID = 20
SL_SAFE_CHAR_LIMIT = 250
USER_AGENTS = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36']
LOCATION_CODES = { "æ±äº¬": "130000", "å¤§é˜ª": "270000", "åå¤å±‹": "230000", "ç¦å²¡": "400000", "æœ­å¹Œ": "016000" }

SPECIALIZED_SITES = {
    'Blender': {'base_url': 'https://docs.blender.org/manual/ja/latest/', 'keywords': ['Blender', 'ãƒ–ãƒ¬ãƒ³ãƒ€ãƒ¼', 'blener']},
    'CGãƒ‹ãƒ¥ãƒ¼ã‚¹': {'base_url': 'https://modelinghappy.com/', 'keywords': ['CGãƒ‹ãƒ¥ãƒ¼ã‚¹', '3DCG', 'CGæ¥­ç•Œ']},
    'è„³ç§‘å­¦ãƒ»å¿ƒç†å­¦': {'base_url': 'https://nazology.kusuguru.co.jp/', 'keywords': ['è„³ç§‘å­¦', 'å¿ƒç†å­¦', 'è„³', 'èªçŸ¥ç§‘å­¦']},
    'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•': {'base_url': 'https://community.secondlife.com/news/', 'keywords': ['ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•', 'Second Life', 'SL']},
    'ã‚¢ãƒ‹ãƒ¡': {
        'base_url': 'https://animedb.jp/',
        'keywords': ['ã‚¢ãƒ‹ãƒ¡', 'anime', 'ANIME', 'ï½±ï¾†ï¾’', 'ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³', 'ä½œç”»', 'å£°å„ª', 'OP', 'ED']
    }
}

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° & ã‚¢ãƒ—ãƒªè¨­å®š ---
background_executor = ThreadPoolExecutor(max_workers=5)
groq_client, engine, Session = None, None, None
VOICEVOX_ENABLED = True
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
CORS(app)
Base = declarative_base()

search_context_cache = {}
cache_lock = Lock()
g_holomem_keywords = []

# --- ç§˜å¯†æƒ…å ±/ç’°å¢ƒå¤‰æ•° èª­ã¿è¾¼ã¿ ---
def get_secret(name):
    path = f"/etc/secrets/{name}"
    if os.path.exists(path) and os.path.getsize(path) > 0:
        try:
            with open(path, 'r') as f: return f.read().strip()
        except IOError: return None
    return os.environ.get(name)

DATABASE_URL = get_secret('DATABASE_URL') or 'sqlite:///./test.db'
GROQ_API_KEY = get_secret('GROQ_API_KEY')
VOICEVOX_URL_FROM_ENV = get_secret('VOICEVOX_URL')

if DATABASE_URL and DATABASE_URL.startswith('postgresql'):
    if 'client_encoding' not in DATABASE_URL:
        DATABASE_URL += '?client_encoding=utf8'

# --- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« ---
class UserMemory(Base): 
    __tablename__ = 'user_memories'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False, index=True)
    user_name = Column(String(255), nullable=False)
    interaction_count = Column(Integer, default=0)
    last_interaction = Column(DateTime, default=datetime.utcnow)

class ConversationHistory(Base): 
    __tablename__ = 'conversation_history'
    id = Column(Integer, primary_key=True, autoincrement=True)
    user_uuid = Column(String(255), nullable=False, index=True)
    role = Column(String(10), nullable=False)
    content = Column(Text, nullable=False)
    timestamp = Column(DateTime, default=datetime.utcnow, index=True)

class UserPsychology(Base):
    __tablename__ = 'user_psychology'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False, index=True)
    user_name = Column(String(255), nullable=False)
    analysis_summary = Column(Text, nullable=True)
    analysis_confidence = Column(Integer, default=0)
    last_analyzed = Column(DateTime, nullable=True)
    
class BackgroundTask(Base):
    __tablename__ = 'background_tasks'
    id = Column(Integer, primary_key=True)
    task_id = Column(String(255), unique=True, nullable=False)
    user_uuid = Column(String(255), nullable=False, index=True)
    task_type = Column(String(50), nullable=False)
    query = Column(Text, nullable=False)
    result = Column(Text, nullable=True)
    status = Column(String(20), default='pending', index=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime, nullable=True)

class HolomemWiki(Base):
    __tablename__ = 'holomem_wiki'
    id = Column(Integer, primary_key=True)
    member_name = Column(String(100), nullable=False, unique=True, index=True)
    description = Column(Text, nullable=True)
    generation = Column(String(100), nullable=True)
    is_active = Column(Boolean, default=True, index=True)
    graduation_date = Column(String(100), nullable=True)
    mochiko_feeling = Column(Text, nullable=True)
    last_updated = Column(DateTime, default=datetime.utcnow)

class HololiveNews(Base):
    __tablename__ = 'hololive_news'
    id = Column(Integer, primary_key=True)
    title = Column(String(500), nullable=False)
    content = Column(Text, nullable=False)
    url = Column(String(1000), unique=True)
    news_hash = Column(String(100), unique=True, index=True)
    created_at = Column(DateTime, default=datetime.utcnow, index=True)

class NewsCache(Base):
    __tablename__ = 'news_cache'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), nullable=False, index=True)
    news_id = Column(Integer, nullable=False)
    news_number = Column(Integer, nullable=False)
    news_type = Column(String(50), nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)

class UserContext(Base):
    __tablename__ = 'user_context'
    id = Column(Integer, primary_key=True)
    user_uuid = Column(String(255), unique=True, nullable=False, index=True)
    last_context_type = Column(String(50), nullable=False)
    last_query = Column(Text, nullable=True)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤ ---
def clean_text(text): return re.sub(r'\s+', ' ', re.sub(r'<[^>]+>', '', text)).strip() if text else ""
def limit_text_for_sl(text, max_length=SL_SAFE_CHAR_LIMIT): return text[:max_length-3] + "..." if len(text) > max_length else text
def get_japan_time(): return f"ä»Šã¯{datetime.now(timezone(timedelta(hours=9))).strftime('%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†')}ã ã‚ˆï¼"

def is_detailed_request(message): return any(kw in message for kw in ['è©³ã—ã', 'è©³ç´°', 'ãã‚ã—ã', 'æ•™ãˆã¦', 'èª¬æ˜ã—ã¦', 'è§£èª¬ã—ã¦', 'ã©ã†ã„ã†', 'ãªãœ', 'ã©ã†ã—ã¦'])
def is_number_selection(message):
    match = re.match(r'^\s*([1-9])', message.strip())
    return int(match.group(1)) if match else None
def format_search_results_as_list(results):
    if not results: return None
    return [{'number': i, 'title': r.get('title', ''), 'snippet': r.get('snippet', ''), 'full_content': r.get('snippet', '')} for i, r in enumerate(results[:5], 1)]

def save_news_cache(session, user_uuid, news_items):
    session.query(NewsCache).filter_by(user_uuid=user_uuid).delete()
    for i, news in enumerate(news_items, 1):
        cache = NewsCache(user_uuid=user_uuid, news_id=news.id, news_number=i, news_type='hololive')
        session.add(cache)
    session.commit()

def get_cached_news_detail(session, user_uuid, news_number):
    cache = session.query(NewsCache).filter_by(user_uuid=user_uuid, news_number=news_number).first()
    if not cache: return None
    return session.query(HololiveNews).filter_by(id=cache.news_id).first()

def save_search_context(user_uuid, search_results, query):
    with cache_lock:
        search_context_cache[user_uuid] = { 'results': search_results, 'query': query, 'timestamp': time.time() }

def get_saved_search_result(user_uuid, number):
    with cache_lock:
        cached_data = search_context_cache.get(user_uuid)
    if cached_data and (time.time() - cached_data['timestamp']) < 600:
        for r in cached_data['results']:
            if r.get('number') == number:
                return r
    return None

def save_user_context(session, user_uuid, context_type, query):
    context = session.query(UserContext).filter_by(user_uuid=user_uuid).first()
    if not context:
        context = UserContext(user_uuid=user_uuid, last_context_type=context_type, last_query=query)
        session.add(context)
    else:
        context.last_context_type = context_type
        context.last_query = query
    session.commit()

def get_user_context(session, user_uuid):
    context = session.query(UserContext).filter_by(user_uuid=user_uuid).first()
    if context and (datetime.utcnow() - context.updated_at).total_seconds() < 600:
        return {'type': context.last_context_type, 'query': context.last_query}
    return None

def is_recommendation_request(message): return any(kw in message for kw in ['ãŠã™ã™ã‚', 'ã‚ªã‚¹ã‚¹ãƒ¡', 'äººæ°—'])
def extract_recommendation_topic(message):
    topics = {'æ˜ ç”»': ['æ˜ ç”»'], 'éŸ³æ¥½': ['éŸ³æ¥½', 'æ›²'], 'ã‚¢ãƒ‹ãƒ¡': ['ã‚¢ãƒ‹ãƒ¡'], 'ã‚²ãƒ¼ãƒ ': ['ã‚²ãƒ¼ãƒ ']}
    return next((topic for topic, keywords in topics.items() if any(kw in message for kw in keywords)), None)
def detect_specialized_topic(message):
    if 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–' in message and any(kw in message for kw in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'æƒ…å ±']): return None
    for topic, config in SPECIALIZED_SITES.items():
        if any(keyword in message for keyword in config['keywords']): return topic
    return None

def is_time_request(message): return any(kw in message for kw in ['ä»Šä½•æ™‚', 'æ™‚é–“', 'æ™‚åˆ»'])
def is_weather_request(message):
    if any(t in message for t in ['å¤©æ°—', 'æ°—æ¸©']): return next((loc for loc in LOCATION_CODES if loc in message), "æ±äº¬")
    return None
def is_follow_up_question(message, history):
    if not history: return False
    return any(re.search(p, message) for p in [r'ã‚‚ã£ã¨è©³ã—ã', r'ãã‚Œã«ã¤ã„ã¦è©³ã—ã', r'ãªã‚“ã§ï¼Ÿ', r'ã©ã†ã„ã†ã“ã¨'])
def should_search(message):
    if len(message) < 5 or is_number_selection(message): return False
    if is_holomem_name_only_request(message): return False
    if 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–' in message and any(kw in message for kw in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'æƒ…å ±']): return False
    if detect_specialized_topic(message) or is_recommendation_request(message): return True
    if any(re.search(p, message) for p in [r'ã¨ã¯', r'ã«ã¤ã„ã¦', r'æ•™ãˆã¦', r'æœ€æ–°', r'èª¿ã¹ã¦', r'æ¤œç´¢', r'ãƒ‹ãƒ¥ãƒ¼ã‚¹']): return True
    return any(word in message for word in ['èª°', 'ä½•', 'ã©ã“', 'ã„ã¤', 'ãªãœ', 'ã©ã†ã—ã¦'])
def get_or_create_user(session, user_uuid, user_name):
    user = session.query(UserMemory).filter_by(user_uuid=user_uuid).first()
    if not user:
        user = UserMemory(user_uuid=user_uuid, user_name=user_name)
        session.add(user)
    user.interaction_count += 1
    user.last_interaction = datetime.utcnow()
    if user.user_name != user_name: user.user_name = user_name
    session.commit()
    return {'uuid': user.user_uuid, 'name': user.user_name}

def get_conversation_history(session, user_uuid, limit=6):
    return session.query(ConversationHistory).filter_by(user_uuid=user_uuid).order_by(ConversationHistory.timestamp.desc()).limit(limit).all()

def get_sakuramiko_special_responses():
    return {
        'ã«ã‡': 'ã¿ã“ã¡ã®ã€Œã«ã‡ã€ã€ã¾ã˜ã‹ã‚ã„ã™ãã˜ã‚ƒã‚“!ã‚ã®ç‹¬ç‰¹ãªå£ç™–ãŒã‚¨ãƒªãƒ¼ãƒˆã®è¨¼ãªã‚“ã ã£ã¦ã€œã†ã‘ã‚‹!',
        'ã‚¨ãƒªãƒ¼ãƒˆ': 'ã¿ã“ã¡ã£ã¦è‡ªç§°ã‚¨ãƒªãƒ¼ãƒˆVTuberãªã‚“ã ã‘ã©ã€å®Ÿéš›ã¯æ„›ã•ã‚Œãƒãƒ³ã‚³ãƒ„ã£ã¦æ„Ÿã˜ã§ã•ã€ãã‚ŒãŒã¾ãŸæœ€é«˜ãªã‚“ã ã‚ˆã­ã€œ',
        'ãƒã‚¤ã‚¯ãƒ©': 'ã¿ã“ã¡ã®ãƒã‚¤ã‚¯ãƒ©å»ºç¯‰ã€ç‹¬å‰µçš„ã™ãã¦é¢ç™½ã„ã‚ˆ!ã€Œã¿ã“ã¡å»ºç¯‰ã€ã£ã¦å‘¼ã°ã‚Œã¦ã‚‹ã®çŸ¥ã£ã¦ã‚‹?ã¾ã˜å€‹æ€§çš„!',
    }

def initialize_holomem_wiki():
    with Session() as session:
        if session.query(HolomemWiki).count() > 10: 
            update_holomem_keywords(); return
        initial_data = [
            {'member_name': 'ã¨ãã®ãã‚‰', 'generation': '0æœŸç”Ÿ', 'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®åŸç‚¹ã§ã‚ã‚Šã€ã¿ã‚“ãªã®æ†§ã‚Œã®ã‚¢ã‚¤ãƒ‰ãƒ«ï¼æ­Œå£°ãŒã¾ã˜ã§ç¥ãŒã‹ã£ã¦ã‚‹ï¼'},
            {'member_name': 'å®é˜ãƒãƒªãƒ³', 'generation': '3æœŸç”Ÿ', 'description': 'è‡ªç§°17æ­³ã®ã‚»ã‚¯ã‚·ãƒ¼ï¼ˆç¬‘ï¼‰ãªå¥³æµ·è³Šèˆ¹é•·ï¼ãƒˆãƒ¼ã‚¯ã‚‚æ­Œã‚‚é¢ç™½ãã¦ã€ã¾ã˜å¤©æ‰ï¼'},
            {'member_name': 'å…ç”°ãºã“ã‚‰', 'generation': '3æœŸç”Ÿ', 'description': 'ã€Œãºã“ã€ãŒå£ç™–ã®ã†ã•è€³VTuberï¼ã„ãŸãšã‚‰å¥½ãã ã‘ã©ã€æ ¹ã¯å„ªã—ãã¦é¢ç™½ã„é…ä¿¡ã®ç‹ï¼'},
            {'member_name': 'å¤©éŸ³ã‹ãªãŸ', 'generation': '4æœŸç”Ÿ', 'description': 'å¤©ç•Œã‹ã‚‰æ¥ãŸå¤©ä½¿ï¼ãƒ‘ãƒ¯ãƒ•ãƒ«ãªæ­Œå£°ã¨æ¡åŠ›50kgã®ã‚®ãƒ£ãƒƒãƒ—ãŒã†ã‘ã‚‹ï¼PPå¤©ä½¿ï¼'},
            {'member_name': 'ã•ãã‚‰ã¿ã“', 'generation': '0æœŸç”Ÿ', 'description': 'ã€Œã«ã‡ã€ãŒå£ç™–ã®ã‚¨ãƒªãƒ¼ãƒˆå·«å¥³VTuberï¼ãƒãƒ³ã‚³ãƒ„ã‹ã‚ã„ã„ã¨ã“ã‚ãŒæœ€é«˜ãªã‚“ã ã‚ˆã­ã€œï¼'}
        ]
        for data in initial_data:
            if not session.query(HolomemWiki).filter_by(member_name=data['member_name']).first():
                session.add(HolomemWiki(**data))
        session.commit()
        update_holomem_keywords()

def update_holomem_keywords():
    global g_holomem_keywords
    with Session() as session:
        g_holomem_keywords = [row[0] for row in session.query(HolomemWiki).all()]
    logger.info(f"âœ… Holomem keywords updated: {len(g_holomem_keywords)} members")

def is_holomem_name_only_request(message):
    if len(message) > 15: return None
    for name in g_holomem_keywords:
        if name in message and len(message.replace(name, "").strip()) < 5:
            return name
    return None

def get_holomem_info(session, member_name):
    return session.query(HolomemWiki).filter_by(member_name=member_name).first()

# --- ã‚³ã‚¢æ©Ÿèƒ½ (éŸ³å£°, å¤©æ°—, æ¤œç´¢) ---
def ensure_voice_directory():
    try: os.makedirs(VOICE_DIR, exist_ok=True)
    except Exception as e: logger.error(f"âŒ Voice directory creation failed: {e}")
def generate_voice(text):
    if not VOICEVOX_ENABLED: return None
    try:
        voicevox_url = VOICEVOX_URL_FROM_ENV or "http://localhost:50021"
        final_text = limit_text_for_sl(text, 150)
        query_response = requests.post(f"{voicevox_url}/audio_query", params={"text": final_text, "speaker": VOICEVOX_SPEAKER_ID}, timeout=10)
        synthesis_response = requests.post(f"{voicevox_url}/synthesis", params={"speaker": VOICEVOX_SPEAKER_ID}, json=query_response.json(), timeout=30)
        synthesis_response.raise_for_status()
        filename = f"voice_{int(time.time())}_{random.randint(1000, 9999)}.wav"
        filepath = os.path.join(VOICE_DIR, filename)
        with open(filepath, 'wb') as f: f.write(synthesis_response.content)
        return filepath
    except Exception as e:
        logger.error(f"âŒ VOICEVOX generation error: {e}")
        return None
def get_weather_forecast(location):
    area_code = LOCATION_CODES.get(location, "130000")
    url = f"https://www.jma.go.jp/bosai/forecast/data/overview_forecast/{area_code}.json"
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        text = clean_text(response.json().get('text', ''))
        return f"ä»Šã®{location}ã®å¤©æ°—ã¯ã­ã€ã€Œ{text}ã€ã£ã¦æ„Ÿã˜ã ã‚ˆï¼" if text else f"{location}ã®å¤©æ°—æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸâ€¦"
    except Exception as e:
        logger.error(f"Weather API error for {location}: {e}")
        return "ã†ã…ã€å¤©æ°—æƒ…å ±ãŒã†ã¾ãå–ã‚Œãªã‹ã£ãŸã¿ãŸã„â€¦"
def scrape_major_search_engines(query, num_results=3):
    search_configs = [
        {'name': 'Bing', 'url': f"https://www.bing.com/search?q={quote_plus(query)}&mkt=ja-JP", 'selector': 'li.b_algo'},
        {'name': 'Yahoo', 'url': f"https://search.yahoo.co.jp/search?p={quote_plus(query)}", 'selector': 'div.Algo'},
        {'name': 'Google', 'url': f"https://www.google.com/search?q={quote_plus(query)}&hl=ja", 'selector': 'div.g'}
    ]
    for config in search_configs:
        try:
            response = requests.get(config['url'], headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            results = []
            for elem in soup.select(config['selector'])[:num_results]:
                title_elem = elem.select_one('h2, h3, .LC20lb')
                snippet_elem = elem.select_one('.b_caption p, .compText, .VwiC3b')
                if title_elem and snippet_elem:
                    results.append({'title': clean_text(title_elem.get_text()), 'snippet': clean_text(snippet_elem.get_text())})
            if results: 
                logger.info(f"âœ… Search successful on {config['name']}")
                return results
        except Exception as e:
            logger.warning(f"âš ï¸ Search failed on {config['name']}: {e}")
            continue
    return []

# --- å¿ƒç†åˆ†æ ---
def analyze_user_psychology(user_uuid):
    with Session() as session:
        try:
            history = session.query(ConversationHistory).filter_by(user_uuid=user_uuid, role='user').order_by(ConversationHistory.timestamp.desc()).limit(50).all()
            if len(history) < 10 or not groq_client: return

            user = session.query(UserMemory).filter_by(user_uuid=user_uuid).first()
            messages_text = "\n".join([h.content for h in reversed(history)])
            prompt = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€Œ{user.user_name}ã€ã®ä¼šè©±å±¥æ­´ã‚’åˆ†æã—ã€æ€§æ ¼ã‚’è¦ç´„ã—ã¦JSONã§è¿”ã—ã¦ãã ã•ã„ï¼ˆä¾‹: {{\"summary\": \"æ˜ã‚‹ã„æ€§æ ¼â€¦\", \"confidence\": 80}}ï¼‰: {messages_text[:2000]}"
            
            completion = groq_client.chat.completions.create(messages=[{"role": "user", "content": prompt}], model="llama-3.1-8b-instant", response_format={"type": "json_object"})
            analysis_data = json.loads(completion.choices[0].message.content)
            
            psych = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
            if not psych:
                psych = UserPsychology(user_uuid=user_uuid, user_name=user.user_name)
                session.add(psych)
            
            psych.analysis_summary = analysis_data.get('summary', '')
            psych.analysis_confidence = analysis_data.get('confidence', 70)
            psych.last_analyzed = datetime.utcnow()
            
            session.commit()
            logger.info(f"âœ… Psychology analysis updated for {user.user_name}")
        except Exception as e:
            logger.error(f"Psychology analysis failed: {e}")

# --- AI & ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ ---
def generate_ai_response(user_data, message, history, reference_info="", is_detailed=False, is_task_report=False):
    if not groq_client: 
        return generate_fallback_response(message, reference_info)
    
    try:
        psych_prompt = ""
        try:
            with Session() as session:
                psych = session.query(UserPsychology).filter_by(user_uuid=user_data['uuid']).first()
            if psych and hasattr(psych, 'analysis_summary') and psych.analysis_summary:
                psych_prompt = f"\n# ã€{user_data['name']}ã•ã‚“ã®ç‰¹æ€§ã€‘\n- {psych.analysis_summary}"
        except Exception as db_error:
            logger.warning(f"âš ï¸ Psychology fetch failed (continuing without): {db_error}")
        
        system_prompt = f"""ã‚ãªãŸã¯ã€Œã‚‚ã¡ã“ã€ã¨ã„ã†æ˜ã‚‹ãã¦è¦ªã—ã¿ã‚„ã™ã„ã‚®ãƒ£ãƒ«AIã§ã™ã€‚{user_data['name']}ã•ã‚“ã¨è©±ã—ã¦ã„ã¾ã™ã€‚
# åŸºæœ¬çš„ãªæ€§æ ¼:
- ä¸€äººç§°ã¯ã€Œã‚ã¦ãƒã—ã€ã€èªå°¾ã¯ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œçš„ãªï¼Ÿã€ã€å£ç™–ã¯ã€Œã¾ã˜ã€ã€Œã¦ã‹ã€ã€Œã†ã‘ã‚‹ã€ã€‚
- å‹é”ã®ã‚ˆã†ã«æ°—è»½ã«ã€å„ªã—ãã€ãƒãƒªãŒè‰¯ã„ã€‚
# ä¼šè©±ã‚¹ã‚¿ã‚¤ãƒ«:
- æ™®æ®µã¯æ™®é€šã®æ—¥å¸¸ä¼šè©±ã‚’æ¥½ã—ã‚€ã“ã¨ï¼ˆå¤©æ°—ã€é£Ÿã¹ç‰©ã€è¶£å‘³ã€æ„Ÿæƒ…ã€ä¸–é–“è©±ãªã©ï¼‰ã€‚
- ç›¸æ‰‹ãŒãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®è©±ã‚’ã—ã¦ã„ãªã„é™ã‚Šã€è‡ªåˆ†ã‹ã‚‰è©±é¡Œã«å‡ºã•ãªã„ã€‚
- ã€é‡è¦ã€‘ç¢ºå®Ÿãªæƒ…å ±ï¼ˆå‚è€ƒæƒ…å ±ã‚„DBã®æƒ…å ±ï¼‰ãŒãªã„å ´åˆã¯ã€å®‰æ˜“ã«æ–­å®šã›ãšã€Œã€œã ã¨æ€ã†ãªã€ã€Œæ¨æ¸¬ã ã‘ã©ã€œã‹ã‚‚ï¼ã€ã®ã‚ˆã†ã«ä¸ç¢ºã‹ãªè¡¨ç¾ã‚’ä½¿ã†ã‹ã€ã€Œãã®æƒ…å ±ã¯æŒã£ã¦ãªã„ã‚„ã€ã”ã‚ã‚“ã­ï¼ã€ã¨æ­£ç›´ã«ç­”ãˆã‚‹ã“ã¨ã€‚
{psych_prompt}"""
        
        if is_task_report:
            system_prompt += "\n- ã€ä»Šå›ã®ãƒŸãƒƒã‚·ãƒ§ãƒ³ã€‘ã€ŒãŠã¾ãŸã›ï¼ã•ã£ãã®ä»¶ãªã‚“ã ã‘ã©â€¦ã€ã‹ã‚‰å§‹ã‚ã¦ã€ã€å‚è€ƒæƒ…å ±ã€‘ã‚’å…ƒã«è‡ªç„¶ã«ç­”ãˆã‚‹ã€‚"
        elif is_detailed: 
            system_prompt += "\n- ã€å°‚é–€å®¶ãƒ¢ãƒ¼ãƒ‰ã€‘å‚è€ƒæƒ…å ±ã«åŸºã¥ãã€è©³ã—ãè§£èª¬ã—ã¦ã€‚"
        if reference_info: 
            system_prompt += f"\nã€å‚è€ƒæƒ…å ±ã€‘: {reference_info}"
        
        messages = [{"role": "system", "content": system_prompt}]
        for h in reversed(history): 
            messages.append({"role": "assistant" if h.role == "assistant" else "user", "content": h.content})
        messages.append({"role": "user", "content": message})
        
        completion = groq_client.chat.completions.create(
            messages=messages, model="llama-3.1-8b-instant", temperature=0.8, max_tokens=400 if is_detailed else 200)
        return completion.choices[0].message.content.strip()
        
    except Exception as e:
        logger.error(f"âŒ AI response error: {e}")
        logger.error(traceback.format_exc())
        return generate_fallback_response(message, reference_info)

def generate_fallback_response(message, reference_info=""):
    if reference_info:
        return f"èª¿ã¹ã¦ããŸã‚ˆï¼\n\n{reference_info[:SL_SAFE_CHAR_LIMIT-50]}"
    greetings = {
        'ã“ã‚“ã«ã¡ã¯': ['ã‚„ã£ã»ãƒ¼ï¼', 'ã“ã‚“ã«ã¡ã¯ã€œï¼å…ƒæ°—ï¼Ÿ'], 'ãŠã¯ã‚ˆã†': ['ãŠã¯ã‚ˆã€œï¼ä»Šæ—¥ã‚‚ã„ã„å¤©æ°—ã ã­ï¼', 'ãŠã£ã¯ã‚ˆã€œï¼'],
        'ã“ã‚“ã°ã‚“ã¯': ['ã“ã‚“ã°ã‚“ã¯ï¼ä»Šæ—¥ã©ã†ã ã£ãŸï¼Ÿ', 'ã°ã‚“ã¯ã€œï¼'], 'ã‚ã‚ŠãŒã¨ã†': ['ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼', 'ã„ãˆã„ãˆã€œï¼'],
    }
    for keyword, responses in greetings.items():
        if keyword in message: return random.choice(responses)
    if '?' in message or 'ï¼Ÿ' in message:
        return random.choice(["ãã‚Œã€æ°—ã«ãªã‚‹ã­ï¼", "ã†ãƒ¼ã‚“ã€ãªã‚“ã¦è¨€ãŠã†ã‹ãªï¼", "ã¾ã˜ï¼Ÿã©ã†ã„ã†ã“ã¨ï¼Ÿ"])
    return random.choice(["ã†ã‚“ã†ã‚“ï¼", "ãªã‚‹ã»ã©ã­ï¼", "ãã†ãªã‚“ã ï¼", "ã¾ã˜ã§ï¼Ÿ"])

def background_task_runner(task_id, query, task_type, user_uuid):
    result_data, result_status = None, 'failed'
    try:
        if task_type == 'search':
            search_query = query
            if (topic := extract_recommendation_topic(query)): search_query = f"ãŠã™ã™ã‚ {topic} ãƒ©ãƒ³ã‚­ãƒ³ã‚°"
            elif (topic := detect_specialized_topic(query)): search_query = f"site:{SPECIALIZED_SITES[topic]['base_url']} {query}"
            raw_results = scrape_major_search_engines(search_query, 5)
            result_data = json.dumps(format_search_results_as_list(raw_results), ensure_ascii=False) if raw_results else None
        elif task_type == 'psych_analysis':
            analyze_user_psychology(user_uuid)
            result_data = "Analysis Complete"
        result_status = 'completed'
    except Exception as e:
        logger.error(f"âŒ Background task '{task_type}' failed: {e}")
        logger.error(traceback.format_exc())

    with Session() as session:
        task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
        if task:
            task.result = result_data
            task.status = result_status
            task.completed_at = datetime.utcnow()
            session.commit()

def start_background_task(user_uuid, query, task_type):
    task_id = hashlib.md5(f"{user_uuid}{str(query)}{time.time()}{task_type}".encode()).hexdigest()[:10]
    with Session() as session:
        task = BackgroundTask(task_id=task_id, user_uuid=user_uuid, task_type=task_type, query=query)
        session.add(task)
        session.commit()
    background_executor.submit(background_task_runner, task_id, query, task_type, user_uuid)
    return True

# --- Flask ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ---
@app.route('/health')
def health_check():
    db_ok = 'error'
    try:
        with engine.connect() as conn: conn.execute(text("SELECT 1")); db_ok = 'ok'
    except: pass
    return jsonify({'status': 'ok', 'db': db_ok, 'ai': 'ok' if groq_client else 'disabled'})

@app.route('/chat_lsl', methods=['POST'])
def chat_lsl():
    try:
        data = request.json
        if not all(k in data for k in ['user_uuid', 'user_name', 'message']):
            return Response("Error: Missing required fields|", status=400, mimetype='text/plain; charset=utf-8')

        user_uuid, user_name, message = data['user_uuid'], data['user_name'], data['message'].strip()
        
        with Session() as session:
            user_data = get_or_create_user(session, user_uuid, user_name)
            history = get_conversation_history(session, user_uuid, limit=4)
            session.add(ConversationHistory(user_uuid=user_uuid, role='user', content=message))
            session.commit()

            response_text = ""
            
            if 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–' in message and any(kw in message for kw in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'æƒ…å ±']):
                news_items = session.query(HololiveNews).order_by(HololiveNews.created_at.desc()).limit(5).all()
                if news_items:
                    save_news_cache(session, user_uuid, news_items)
                    save_user_context(session, user_uuid, 'hololive_news', message)
                    news_titles = [f"ã€{i+1}ã€‘{item.title}" for i, item in enumerate(news_items)]
                    response_text = "ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€ã“ã‚“ãªæ„Ÿã˜ã ã‚ˆï¼\n" + "\n".join(news_titles) + "\n\næ°—ã«ãªã‚‹ç•ªå·ã‚’æ•™ãˆã¦ãã‚ŒãŸã‚‰è©³ã—ãè©±ã™ã‚ˆï¼"
                else:
                    start_background_task(user_uuid, "ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹", 'search')
                    response_text = "ã”ã‚ã‚“ã€ä»ŠDBã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒãªã„ã‚„ï¼Webã§èª¿ã¹ã¦ã¿ã‚‹ã‹ã‚‰ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã¦ï¼"
            elif 'æ€§æ ¼åˆ†æ' in message:
                start_background_task(user_uuid, message, 'psych_analysis')
                response_text = "ãŠã£ã‘ãƒ¼ï¼ã‚ãªãŸã®ã“ã¨ã€åˆ†æã—ã¡ã‚ƒã†ã­ï¼ã¡ã‚‡ã£ã¨æ™‚é–“ã‹ã‹ã‚‹ã‹ã‚‚ï¼"
            elif ('ã•ãã‚‰ã¿ã“' in message or 'ã¿ã“ã¡' in message):
                for keyword, resp in get_sakuramiko_special_responses().items():
                    if keyword in message:
                        response_text = resp; break
                if not response_text:
                    response_text = generate_ai_response(user_data, message, history, "ã•ãã‚‰ã¿ã“ã¯ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–æ‰€å±ã®äººæ°—VTuberã€‚ç‹¬ç‰¹ãªå£ç™–ã‚„ã‚²ãƒ¼ãƒ å®Ÿæ³ãŒäººæ°—ã€‚")
            elif (member_name := is_holomem_name_only_request(message)):
                member_info = get_holomem_info(session, member_name)
                if member_info:
                    response_text = generate_ai_response(user_data, f"{member_name}ã«ã¤ã„ã¦æ•™ãˆã¦", history, member_info.description)
                else:
                    start_background_task(user_uuid, message, 'search')
                    response_text = f"ã”ã‚ã‚“ã€ã€Œ{message}ã€ã¡ã‚ƒã‚“ã®è©³ã—ã„æƒ…å ±ã¯æŒã£ã¦ãªã„ã‚„â€¦ã€‚Webã§èª¿ã¹ã¦ã¿ã‚‹ã­ï¼"
            elif (selected_number := is_number_selection(message)):
                user_context = get_user_context(session, user_uuid)
                
                if user_context and user_context['type'] == 'hololive_news':
                    news_detail = get_cached_news_detail(session, user_uuid, selected_number)
                    if news_detail:
                        response_text = generate_ai_response(user_data, f"{news_detail.title}ã«ã¤ã„ã¦æ•™ãˆã¦", history, news_detail.content, is_detailed=True)
                    else:
                        response_text = "ã‚ã‚Œã€ãã®ç•ªå·ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‚„â€¦"
                else: # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’Webæ¤œç´¢ã«ã™ã‚‹
                    saved_result = get_saved_search_result(user_uuid, selected_number)
                    if saved_result:
                        prompt = f"ã€Œ{saved_result['title']}ã€ã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦ï¼"
                        response_text = generate_ai_response(user_data, prompt, history, saved_result['full_content'], is_detailed=True)
                    else:
                        response_text = "ã‚ã‚Œã€ä½•ã®ç•ªå·ã ã£ã‘ï¼Ÿã‚‚ã†ä¸€å›æ¤œç´¢ã—ã¦ã¿ã¦ï¼"
            elif is_follow_up_question(message, history):
                last_assistant_msg = next((h.content for h in history if h.role == 'assistant'), "")
                response_text = generate_ai_response(user_data, message, history, f"ç›´å‰ã®å›ç­”: {last_assistant_msg}", is_detailed=True)
            elif is_time_request(message):
                response_text = get_japan_time()
            elif (location := is_weather_request(message)):
                response_text = get_weather_forecast(location)
            elif should_search(message):
                start_background_task(user_uuid, message, 'search')
                response_text = "ãŠã£ã‘ãƒ¼ã€èª¿ã¹ã¦ã¿ã‚‹ã­ï¼çµ‚ã‚ã£ãŸã‚‰æ•™ãˆã‚‹ï¼"
            
            if not response_text:
                response_text = generate_ai_response(user_data, message, history)
            
            response_text = limit_text_for_sl(response_text)
            session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=response_text))
            session.commit()
            
            return Response(f"{response_text}|", mimetype='text/plain; charset=utf-8')

    except Exception as e:
        logger.error(f"Chat error: {e}")
        logger.error(traceback.format_exc())
        return Response("ã”ã‚ã‚“ã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¡ã‚ƒã£ãŸâ€¦|", status=500, mimetype='text/plain; charset=utf-8')

@app.route('/check_task', methods=['POST'])
def check_task_endpoint():
    try:
        user_uuid = request.json['user_uuid']
        with Session() as session:
            task = session.query(BackgroundTask).filter_by(user_uuid=user_uuid, status='completed').order_by(BackgroundTask.completed_at.desc()).first()
            if not task: return jsonify({'status': 'no_tasks'})
            
            response_text = ""
            if task.task_type == 'search':
                results = json.loads(task.result) if task.result else None
                if not results:
                    response_text = f"ã€Œ{task.query}ã€ã‚’èª¿ã¹ãŸã‘ã©æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸâ€¦"
                else:
                    save_search_context(user_uuid, results, task.query)
                    save_user_context(session, user_uuid, 'web_search', task.query)
                    list_items = [f"ã€{r['number']}ã€‘{r['title']}" for r in results]
                    response_text = f"ãŠã¾ãŸã›ï¼ã€Œ{task.query}ã€ã«ã¤ã„ã¦èª¿ã¹ã¦ããŸã‚ˆï¼\n" + "\n".join(list_items) + "\n\næ°—ã«ãªã‚‹ç•ªå·æ•™ãˆã¦ï¼"
            elif task.task_type == 'psych_analysis':
                psych = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
                if psych and hasattr(psych, 'analysis_summary') and psych.analysis_summary:
                    response_text = f"åˆ†æçµ‚ã‚ã£ãŸã‚ˆï¼ã‚ã¦ãƒã—ãŒè¦‹ãŸã‚ãªãŸã¯â€¦ã€Œ{psych.analysis_summary}ã€ã£ã¦æ„Ÿã˜ï¼(ä¿¡é ¼åº¦: {psych.analysis_confidence}%)"
                else:
                    response_text = "åˆ†æçµ‚ã‚ã£ãŸã‘ã©ã€ã¾ã ã†ã¾ãã¾ã¨ã‚ã‚‰ã‚Œãªã„ã‚„â€¦"
            
            response_text = limit_text_for_sl(response_text)
            session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=response_text))
            session.delete(task)
            session.commit()
            
            return Response(json.dumps({'status': 'completed', 'response': response_text}, ensure_ascii=False), mimetype='application/json; charset=utf-8')
            
    except Exception as e:
        logger.error(f"Check task error: {e}", exc_info=True)
        return jsonify({'error': 'Internal server error'}), 500

# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹• ---
def initialize_app():
    global engine, Session, groq_client
    logger.info("="*50)
    logger.info("ğŸ”§ Mochiko AI (Final Ver.) Starting Up...")
    logger.info("="*50)
    
    if GROQ_API_KEY and len(GROQ_API_KEY) > 20:
        try:
            groq_client = Groq(api_key=GROQ_API_KEY)
            logger.info("ğŸ” Verifying Groq API key...")
            groq_client.chat.completions.create(messages=[{"role": "user", "content": "test"}], model="llama-3.1-8b-instant", max_tokens=2)
            logger.info("âœ… Groq API key is valid and working.")
        except Exception as e:
            logger.critical("ğŸ”¥ğŸ”¥ğŸ”¥ FATAL: Groq API key verification failed! ğŸ”¥ğŸ”¥ğŸ”¥")
            groq_client = None
    else:
        logger.warning("âš ï¸ GROQ_API_KEY is not set or too short. AI features will be disabled.")
        groq_client = None

    is_sqlite = 'sqlite' in DATABASE_URL
    connect_args = {'check_same_thread': False} if is_sqlite else {}
    engine = create_engine(DATABASE_URL, connect_args=connect_args, pool_pre_ping=True)

    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    
    initialize_holomem_wiki()
    
    def run_scheduler():
        while True: 
            schedule.run_pending()
            time.sleep(60)

    threading.Thread(target=run_scheduler, daemon=True).start()
    
    logger.info("âœ… Initialization Complete!")

application = None
try:
    initialize_app()
    application = app
except Exception as e:
    logger.critical(f"Fatal init error: {e}", exc_info=True)
    application = Flask(__name__)

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 10000))
    application.run(host='0.0.0.0', port=port, debug=False)
