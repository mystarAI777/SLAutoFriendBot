# ==============================================================================
# ã‚‚ã¡ã“AI - ç©¶æ¥µã®å…¨æ©Ÿèƒ½çµ±åˆç‰ˆ (v17.1 - æœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼åæ˜ ãƒ»å®Œæˆç‰ˆ)
#
# v17.0ã«å¯¾ã™ã‚‹ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ã™ã¹ã¦åæ˜ ã—ãŸæœ€çµ‚å®‰å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€‚
# - Wikipediaæ©Ÿèƒ½ã®è‡´å‘½çš„ãªãƒã‚°ã‚’å®Œå…¨ã«ä¿®æ­£ã—ã€é«˜å“è³ªãªè¦ç´„ã‚’å®Ÿç¾
# - æ›–æ˜§ã•å›é¿ãƒšãƒ¼ã‚¸ã®æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã‚’å¤§å¹…ã«å¼·åŒ–
# - AIãƒ¢ãƒ‡ãƒ«ã‚’æœ€æ–°ã®Llama 3.1 70Bã«ä¿®æ­£
# - Yahoo! JAPANæ¤œç´¢ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œã‚’è§£æ±º
# - å„æ©Ÿèƒ½ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ­ã‚°ã‚’æ”¹å–„
# ==============================================================================

# ===== ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ =====
import sys
import os
import requests
import logging
import time
import threading
import json
import re
import random
import uuid
import hashlib
from datetime import datetime, timedelta, timezone
import unicodedata
from urllib.parse import quote_plus, urljoin
from functools import wraps
from threading import Lock

# --- ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ---
from flask import Flask, request, jsonify, send_from_directory, Response
from flask_cors import CORS
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Text, Boolean, inspect, text
from sqlalchemy.orm import declarative_base, sessionmaker
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import schedule
import signal
from groq import Groq

# ==============================================================================
# åŸºæœ¬è¨­å®šã¨ãƒ­ã‚®ãƒ³ã‚°
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ==============================================================================
# å®šæ•°è¨­å®š
# ==============================================================================
VOICE_DIR = '/tmp/voices'
SERVER_URL = os.environ.get('RENDER_EXTERNAL_URL', "http://localhost:5000")
background_executor = ThreadPoolExecutor(max_workers=5)
VOICEVOX_SPEAKER_ID = 20
HOLOLIVE_NEWS_URL = "https://hololive-tsuushin.com/category/holonews/"
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
]
LOCATION_CODES = { "æ±äº¬": "130000", "å¤§é˜ª": "270000", "åå¤å±‹": "230000", "ç¦å²¡": "400000", "æœ­å¹Œ": "016000" }
SPECIALIZED_SITES = {
    'Blender': {'base_url': 'https://docs.blender.org/manual/ja/latest/', 'keywords': ['Blender', 'ãƒ–ãƒ¬ãƒ³ãƒ€ãƒ¼']},
    'CGãƒ‹ãƒ¥ãƒ¼ã‚¹': {'base_url': 'https://modelinghappy.com/', 'keywords': ['CGãƒ‹ãƒ¥ãƒ¼ã‚¹', '3DCG', 'CG', 'CGæ¥­ç•Œ']},
    'è„³ç§‘å­¦ãƒ»å¿ƒç†å­¦': {'base_url': 'https://nazology.kusuguru.co.jp/', 'keywords': ['è„³ç§‘å­¦', 'å¿ƒç†å­¦', 'è„³', 'èªçŸ¥ç§‘å­¦']},
    'ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•': {'base_url': 'https://community.secondlife.com/news/', 'keywords': ['ã‚»ã‚«ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ•', 'Second Life', 'SL']},
}
HOLOMEM_KEYWORDS = [
    'ã¨ãã®ãã‚‰', 'ãƒ­ãƒœå­ã•ã‚“', 'ã•ãã‚‰ã¿ã“', 'æ˜Ÿè¡—ã™ã„ã›ã„', 'AZKi', 'å¤œç©ºãƒ¡ãƒ«', 'ã‚¢ã‚­ãƒ»ãƒ­ãƒ¼ã‚¼ãƒ³ã‚¿ãƒ¼ãƒ«', 'èµ¤äº•ã¯ã‚ã¨', 'ç™½ä¸Šãƒ•ãƒ–ã‚­', 'å¤è‰²ã¾ã¤ã‚Š', 'æ¹Šã‚ãã‚', 'ç´«å’²ã‚·ã‚ªãƒ³', 'ç™¾é¬¼ã‚ã‚„ã‚', 'ç™’æœˆã¡ã‚‡ã“', 'å¤§ç©ºã‚¹ãƒãƒ«', 'å¤§ç¥ãƒŸã‚ª', 'çŒ«åˆãŠã‹ã‚†', 'æˆŒç¥ã“ã‚ã­', 'å…ç”°ãºã“ã‚‰', 'ä¸çŸ¥ç«ãƒ•ãƒ¬ã‚¢', 'ç™½éŠ€ãƒã‚¨ãƒ«', 'å®é˜ãƒãƒªãƒ³', 'å¤©éŸ³ã‹ãªãŸ', 'è§’å·»ã‚ãŸã‚', 'å¸¸é—‡ãƒˆãƒ¯', 'å§«æ£®ãƒ«ãƒ¼ãƒŠ', 'é›ªèŠ±ãƒ©ãƒŸã‚£', 'æ¡ƒéˆ´ã­ã­', 'ç…ç™½ã¼ãŸã‚“', 'å°¾ä¸¸ãƒãƒ«ã‚«', 'ãƒ©ãƒ—ãƒ©ã‚¹ãƒ»ãƒ€ãƒ¼ã‚¯ãƒã‚¹', 'é·¹å¶ºãƒ«ã‚¤', 'åšè¡£ã“ã‚ˆã‚Š', 'æ²™èŠ±å‰ã‚¯ãƒ­ãƒ±', 'é¢¨çœŸã„ã‚ã¯', 'æ£®ã‚«ãƒªã‚ªãƒš', 'å°é³¥éŠã‚­ã‚¢ãƒ©', 'ä¸€ä¼Šé‚£å°“æ –', 'ãŒã†ã‚‹ãƒ»ãã‚‰', 'ãƒ¯ãƒˆã‚½ãƒ³ãƒ»ã‚¢ãƒ¡ãƒªã‚¢', 'IRyS', 'ã‚»ãƒ¬ã‚¹ãƒ»ãƒ•ã‚¡ã‚¦ãƒŠ', 'ã‚ªãƒ¼ãƒ­ãƒ»ã‚¯ãƒ­ãƒ‹ãƒ¼', 'ä¸ƒè©©ãƒ ãƒ¡ã‚¤', 'ãƒã‚³ã‚¹ãƒ»ãƒ™ãƒ¼ãƒ«ã‚º', 'ã‚·ã‚ªãƒªãƒ»ãƒãƒ´ã‚§ãƒ©', 'å¤çŸ³ãƒ“ã‚¸ãƒ¥ãƒ¼', 'ãƒãƒªãƒƒã‚µãƒ»ãƒ¬ã‚¤ãƒ´ãƒ³ã‚¯ãƒ­ãƒ•ãƒˆ', 'ãƒ•ãƒ¯ãƒ¯ãƒ»ã‚¢ãƒ“ã‚¹ã‚¬ãƒ¼ãƒ‰', 'ãƒ¢ã‚³ã‚³ãƒ»ã‚¢ãƒ“ã‚¹ã‚¬ãƒ¼ãƒ‰', 'ã‚¢ãƒ¦ãƒ³ãƒ€ãƒ»ãƒªã‚¹', 'ãƒ ãƒ¼ãƒŠãƒ»ãƒ›ã‚·ãƒãƒ´ã‚¡', 'ã‚¢ã‚¤ãƒ©ãƒ‹ãƒ»ã‚¤ã‚ªãƒ•ã‚£ãƒ•ãƒ†ã‚£ãƒ¼ãƒ³', 'ã‚¯ãƒ¬ã‚¤ã‚¸ãƒ¼ãƒ»ã‚ªãƒªãƒ¼', 'ã‚¢ãƒ¼ãƒ‹ãƒ£ãƒ»ãƒ¡ãƒ«ãƒ•ã‚£ãƒƒã‚µ', 'ãƒ‘ãƒ´ã‚©ãƒªã‚¢ãƒ»ãƒ¬ã‚¤ãƒ', 'ç«å¨é’', 'éŸ³ä¹ƒç€¬å¥', 'ä¸€æ¡è‰ã€…è¯', 'å„’çƒé¢¨äº­ã‚‰ã§ã‚“', 'è½Ÿã¯ã˜ã‚', 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–', 'ãƒ›ãƒ­ãƒ¡ãƒ³', 'hololive', 'YAGOO', 'æ¡ç”Ÿã‚³ã‚³', 'æ½¤ç¾½ã‚‹ã—ã‚', 'é­”ä¹ƒã‚¢ãƒ­ã‚¨', 'ä¹åä¹ä½å‘½'
]

# ==============================================================================
# ç§˜å¯†æƒ…å ±/ç’°å¢ƒå¤‰æ•° èª­ã¿è¾¼ã¿
# ==============================================================================
def get_secret(name):
    secret_file_path = f"/etc/secrets/{name}"
    if os.path.exists(secret_file_path):
        try:
            with open(secret_file_path, 'r') as f:
                return f.read().strip()
        except IOError: pass
    return os.environ.get(name)

DATABASE_URL = get_secret('DATABASE_URL') or 'sqlite:///./mochiko.db'
GROQ_API_KEY = get_secret('GROQ_API_KEY')
VOICEVOX_URL_FROM_ENV = get_secret('VOICEVOX_URL')

# ==============================================================================
# AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
# ==============================================================================
groq_client = None
VOICEVOX_ENABLED = True if VOICEVOX_URL_FROM_ENV else False
search_context_cache = {}
cache_lock = Lock()

# ==============================================================================
# Flask & ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
# ==============================================================================
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
CORS(app)

def create_db_engine_with_retry(max_retries=5, retry_delay=5):
    from sqlalchemy.exc import OperationalError
    for attempt in range(max_retries):
        try:
            connect_args = {'check_same_thread': False} if 'sqlite' in DATABASE_URL else {'connect_timeout': 10}
            engine = create_engine(DATABASE_URL, pool_pre_ping=True, pool_recycle=300, connect_args=connect_args)
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            return engine
        except OperationalError as e:
            if attempt < max_retries - 1:
                logger.warning(f"âš ï¸ DBæ¥ç¶šå¤±æ•—: {e}. {retry_delay}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤...")
                time.sleep(retry_delay)
            else:
                raise
        except Exception as e:
            raise

engine = create_db_engine_with_retry()
Base = declarative_base()

# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« (å…¨æ©Ÿèƒ½åˆ†)
# ==============================================================================
class UserMemory(Base): __tablename__ = 'user_memories'; id = Column(Integer, primary_key=True); user_uuid = Column(String(255), unique=True, nullable=False); user_name = Column(String(255), nullable=False); interaction_count = Column(Integer, default=0); last_interaction = Column(DateTime, default=datetime.utcnow)
class ConversationHistory(Base): __tablename__ = 'conversation_history'; id = Column(Integer, primary_key=True, autoincrement=True); user_uuid = Column(String(255), nullable=False, index=True); role = Column(String(10), nullable=False); content = Column(Text, nullable=False); timestamp = Column(DateTime, default=datetime.utcnow, index=True)
class HololiveNews(Base): __tablename__ = 'hololive_news'; id = Column(Integer, primary_key=True); title = Column(String(500), nullable=False); content = Column(Text, nullable=False); url = Column(String(1000)); created_at = Column(DateTime, default=datetime.utcnow, index=True); news_hash = Column(String(100), unique=True)
class SpecializedNews(Base): __tablename__ = 'specialized_news'; id = Column(Integer, primary_key=True); site_name = Column(String(100), nullable=False, index=True); title = Column(String(500), nullable=False); content = Column(Text, nullable=False); url = Column(String(1000)); created_at = Column(DateTime, default=datetime.utcnow, index=True); news_hash = Column(String(100), unique=True)
class BackgroundTask(Base): __tablename__ = 'background_tasks'; id = Column(Integer, primary_key=True); task_id = Column(String(255), unique=True, nullable=False); user_uuid = Column(String(255), nullable=False); task_type = Column(String(50), nullable=False); query = Column(Text, nullable=False); result = Column(Text); status = Column(String(20), default='pending'); created_at = Column(DateTime, default=datetime.utcnow); completed_at = Column(DateTime)
class HolomemWiki(Base):
    __tablename__ = 'holomem_wiki'
    id = Column(Integer, primary_key=True); member_name = Column(String(100), nullable=False, unique=True, index=True); description = Column(Text); debut_date = Column(String(100)); generation = Column(String(100)); tags = Column(Text)
    status = Column(String(50), default='ç¾å½¹', nullable=False); graduation_date = Column(String(100), nullable=True); graduation_reason = Column(Text, nullable=True); mochiko_feeling = Column(Text, nullable=True); last_updated = Column(DateTime, default=datetime.utcnow)
class FriendRegistration(Base): __tablename__ = 'friend_registrations'; id = Column(Integer, primary_key=True); user_uuid = Column(String(255), nullable=False, index=True); friend_uuid = Column(String(255), nullable=False); friend_name = Column(String(255), nullable=False); registered_at = Column(DateTime, default=datetime.utcnow); relationship_note = Column(Text)
class UserPsychology(Base):
    __tablename__ = 'user_psychology'
    id = Column(Integer, primary_key=True); user_uuid = Column(String(255), unique=True, nullable=False, index=True); user_name = Column(String(255), nullable=False)
    openness = Column(Integer, default=50); conscientiousness = Column(Integer, default=50); extraversion = Column(Integer, default=50); agreeableness = Column(Integer, default=50); neuroticism = Column(Integer, default=50)
    interests = Column(Text); favorite_topics = Column(Text); conversation_style = Column(String(100)); emotional_tendency = Column(String(100)); analysis_summary = Column(Text)
    total_messages = Column(Integer, default=0); avg_message_length = Column(Integer, default=0); analysis_confidence = Column(Integer, default=0); last_analyzed = Column(DateTime)
class NewsCache(Base): __tablename__ = 'news_cache'; id = Column(Integer, primary_key=True); user_uuid = Column(String(255), nullable=False, index=True); news_id = Column(Integer, nullable=False); news_number = Column(Integer, nullable=False); news_type = Column(String(50), nullable=False); created_at = Column(DateTime, default=datetime.utcnow)
class UserContext(Base):
    __tablename__ = 'user_context'
    id = Column(Integer, primary_key=True); user_uuid = Column(String(255), unique=True, nullable=False, index=True); last_context_type = Column(String(50), nullable=False); last_query = Column(Text, nullable=True); updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)


Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)

# ==============================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ & ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° (å…¨æ©Ÿèƒ½åˆ†)
# ==============================================================================
def create_json_response(data, status=200): return Response(json.dumps(data, ensure_ascii=False), mimetype='application/json; charset=utf-8', status=status)
def clean_text(text): return re.sub(r'\s+', ' ', re.sub(r'<[^>]+>', '', text or "")).strip()
def get_japan_time(): return f"ä»Šã¯{datetime.now(timezone(timedelta(hours=9))).strftime('%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†')}ã ã‚ˆï¼"
def create_news_hash(title, content): return hashlib.md5(f"{title}{content[:100]}".encode('utf-8')).hexdigest()

def is_what_is_request(message):
    match = re.search(r'(.+?)\s*(?:ã¨ã¯|ã£ã¦ä½•|ã£ã¦ãªã«)\??$', message.strip())
    if match:
        return match.group(1).strip()
    return None
def is_time_request(message): return any(keyword in message for keyword in ['ä»Šä½•æ™‚', 'æ™‚é–“', 'æ™‚åˆ»'])
def is_weather_request(message): return any(keyword in message for keyword in ['å¤©æ°—äºˆå ±', 'æ˜æ—¥ã®å¤©æ°—ã¯ï¼Ÿ', 'ä»Šæ—¥ã®å¤©æ°—ã¯ï¼Ÿ'])
def is_hololive_news_request(message): return 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–' in message and any(kw in message for kw in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'æƒ…å ±'])
def is_friend_request(message): return any(fk in message for fk in ['å‹ã ã¡', 'å‹é”']) and any(ak in message for ak in ['ç™»éŒ²', 'èª°', 'ãƒªã‚¹ãƒˆ'])
def is_anime_request(message): return any(kw in message for kw in ['ã‚¢ãƒ‹ãƒ¡', 'anime', 'ã‚ã«ã‚'])
def detect_specialized_topic(message):
    for topic, config in SPECIALIZED_SITES.items():
        if any(keyword in message for keyword in config['keywords']) and any(kw in message for kw in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'æƒ…å ±']):
            return topic
    return None
def is_explicit_search_request(message): return any(keyword in message for keyword in ['èª¿ã¹ã¦', 'æ¤œç´¢ã—ã¦', 'æ¢ã—ã¦'])
def should_search(message):
    if is_short_response(message) or is_explicit_search_request(message) or is_number_selection(message) or is_hololive_news_request(message) or detect_specialized_topic(message) or is_what_is_request(message):
        return False
    if is_anime_request(message): return True
    for member in HOLOMEM_KEYWORDS:
        if member in message and not any(kw in message for kw in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'æƒ…å ±']):
            if len(message.replace(member, '').strip()) > 5: return True
    patterns = [r'(?:ã«ã¤ã„ã¦|æ•™ãˆã¦)', r'(?:èª°|ä½•|ã©ã“|ã„ã¤|ãªãœ|ã©ã†)'] # "ã¨ã¯" ã‚’é™¤å¤–
    return any(re.search(pattern, message) for pattern in patterns)
def is_detailed_request(message): return any(keyword in message for keyword in ['è©³ã—ã', 'è©³ç´°', 'æ•™ãˆã¦', 'èª¬æ˜ã—ã¦'])
def is_short_response(message): return len(message.strip()) <= 3 or message.strip() in ['ã†ã‚“', 'ãã†', 'ã¯ã„', 'ãã£ã‹', 'ãªã‚‹ã»ã©']
def extract_location(message):
    for location in LOCATION_CODES.keys():
        if location in message: return location
    return "æ±äº¬"
def is_number_selection(message):
    match = re.search(r'^\s*([1-9]|[ï¼‘-ï¼™])\s*$', message.strip())
    if match: return int(unicodedata.normalize('NFKC', match.group(1)))
    return None
def detect_db_correction_request(message):
    pattern = r"(.+?)(?:(?:ã®|ã«é–¢ã™ã‚‹)(?:æƒ…å ±|ãƒ‡ãƒ¼ã‚¿))?(?:ã§|ã€|ã ã‘ã©|ã§ã™ãŒ)ã€?ã€Œ(.+?)ã€ã¯ã€Œ(.+?)ã€ãŒæ­£ã—ã„ã‚ˆ"
    match = re.search(pattern, message)
    if match:
        member_name_raw, field_raw, value_raw = match.groups()
        member_name = member_name_raw.strip()
        field = field_raw.strip()
        value = value_raw.strip()
        if member_name in HOLOMEM_KEYWORDS and field in ['èª¬æ˜', 'ãƒ‡ãƒ“ãƒ¥ãƒ¼æ—¥', 'æœŸ', 'ã‚¿ã‚°', 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹', 'å’æ¥­æ—¥', 'ã‚‚ã¡ã“ã®æ°—æŒã¡']:
            return {'member_name': member_name, 'field': field, 'value': value}
    return None
def get_sakuramiko_special_responses():
    return {
        'ã«ã‡': 'ã¿ã“ã¡ã®ã€Œã«ã‡ã€ã€ã¾ã˜ã‹ã‚ã„ã™ãã˜ã‚ƒã‚“!ã‚ã®ç‹¬ç‰¹ãªå£ç™–ãŒã‚¨ãƒªãƒ¼ãƒˆã®è¨¼ãªã‚“ã ã£ã¦ã€œã†ã‘ã‚‹!',
        'ã‚¨ãƒªãƒ¼ãƒˆ': 'ã¿ã“ã¡ã£ã¦è‡ªç§°ã‚¨ãƒªãƒ¼ãƒˆVTuberãªã‚“ã ã‘ã©ã€å®Ÿéš›ã¯æ„›ã•ã‚Œãƒãƒ³ã‚³ãƒ„ã£ã¦æ„Ÿã˜ã§ã•ã€ãã‚ŒãŒã¾ãŸæœ€é«˜ãªã‚“ã ã‚ˆã­ã€œ',
        'ãƒã‚¤ã‚¯ãƒ©': 'ã¿ã“ã¡ã®ãƒã‚¤ã‚¯ãƒ©å»ºç¯‰ã€ç‹¬å‰µçš„ã™ãã¦é¢ç™½ã„ã‚ˆ!ã€Œã¿ã“ã¡å»ºç¯‰ã€ã£ã¦å‘¼ã°ã‚Œã¦ã‚“ã®çŸ¥ã£ã¦ã‚‹?ã¾ã˜å€‹æ€§çš„!',
        'FAQ': 'ã¿ã“ã¡ã®FAQã£ã¦ã•ã€å®Ÿã¯æœ¬äººãŒç­”ãˆã‚‹ã‚“ã˜ã‚ƒãªãã¦ãƒ•ã‚¡ãƒ³ãŒè³ªå•ã™ã‚‹ã‚³ãƒ¼ãƒŠãƒ¼ãªã®!é¢ç™½ã„ã‚ˆã­ã€œ',
        'GTA': 'ã¿ã“ã¡ã®GTAé…ä¿¡ã€ã‚«ã‚ªã‚¹ã™ãã¦æœ€é«˜!è­¦å¯Ÿã«è¿½ã‚ã‚ŒãŸã‚Šå¤‰ãªã“ã¨ã—ãŸã‚Šã€è¦‹ã¦ã¦é£½ããªã„ã‚“ã ã‚ˆã­ã€œ'
    }

# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ç®¡ç†
# ==============================================================================
def get_or_create_user(session, uuid, name):
    user = session.query(UserMemory).filter_by(user_uuid=uuid).first()
    if user:
        user.interaction_count += 1
        user.last_interaction = datetime.utcnow()
        if user.user_name != name: user.user_name = name
    else:
        user = UserMemory(user_uuid=uuid, user_name=name, interaction_count=1)
        session.add(user)
    session.commit()
    return user
def get_conversation_history(session, uuid, limit=8):
    history = session.query(ConversationHistory).filter_by(user_uuid=uuid).order_by(ConversationHistory.timestamp.desc()).limit(limit).all()
    return list(reversed(history))
def check_completed_tasks(user_uuid):
    with Session() as session:
        task = session.query(BackgroundTask).filter(BackgroundTask.user_uuid == user_uuid, BackgroundTask.status == 'completed').order_by(BackgroundTask.completed_at.desc()).first()
        if task:
            query_data = json.loads(task.query)
            result = {'query': query_data.get('query', query_data) , 'result': task.result, 'type': task.task_type}
            session.delete(task); session.commit()
            return result
    return None
def start_background_task(user_uuid, task_type, query_data):
    task_id = str(uuid.uuid4())[:8]
    with Session() as session:
        session.add(BackgroundTask(task_id=task_id, user_uuid=user_uuid, task_type=task_type, query=json.dumps(query_data, ensure_ascii=False)))
        session.commit()
    
    if task_type == 'search':
        background_executor.submit(background_deep_search, task_id, query_data['query'], query_data.get('is_detailed', False))
    elif task_type == 'db_correction':
        background_executor.submit(background_db_correction, task_id, query_data)
    elif task_type == 'psych_analysis':
        background_executor.submit(analyze_user_psychology, user_uuid)
    return task_id

# ==============================================================================
# AIãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—é–¢æ•° (ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ã)
# ==============================================================================
def _with_retry(api_call_function):
    @wraps(api_call_function)
    def wrapper(*args, **kwargs):
        max_retries = 3
        delay = 1.0
        for attempt in range(max_retries):
            try:
                return api_call_function(*args, **kwargs)
            except Exception as e:
                if "429" in str(e):
                    if attempt < max_retries - 1:
                        wait_time = delay * (2 ** attempt) + random.uniform(0, 1)
                        logger.warning(f"âš ï¸ APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã€‚{wait_time:.2f}ç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™... ({attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"âŒ APIå‘¼ã³å‡ºã—ãŒãƒªãƒˆãƒ©ã‚¤ä¸Šé™ã‚’è¶…ãˆã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
                        raise
                else:
                    logger.error(f"âŒ APIå‘¼ã³å‡ºã—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    raise
        return None
    return wrapper

@_with_retry
def call_llama_advanced(prompt, history, system_prompt, max_tokens=1000):
    if not groq_client: return None
    messages = [{"role": "system", "content": system_prompt}]
    for msg in history[-8:]:
        messages.append({"role": "user" if msg.role == "user" else "assistant", "content": msg.content})
    messages.append({"role": "user", "content": prompt})
    completion = groq_client.chat.completions.create(messages=messages, model="llama-3.1-70b-versatile", temperature=0.7, max_tokens=max_tokens)
    return completion.choices[0].message.content.strip()

def generate_fallback_response(message, reference_info=""):
    if reference_info:
        return f"èª¿ã¹ã¦ããŸã‚ˆï¼\n\n{reference_info[:200]}"
    greetings = {
        'ã“ã‚“ã«ã¡ã¯': ['ã‚„ã£ã»ãƒ¼ï¼', 'ã“ã‚“ã«ã¡ã¯ã€œï¼å…ƒæ°—ï¼Ÿ'], 'ãŠã¯ã‚ˆã†': ['ãŠã¯ã‚ˆã€œï¼ä»Šæ—¥ã‚‚ã„ã„å¤©æ°—ã ã­ï¼', 'ãŠã£ã¯ã‚ˆã€œï¼'],
        'ã“ã‚“ã°ã‚“ã¯': ['ã“ã‚“ã°ã‚“ã¯ï¼ä»Šæ—¥ã©ã†ã ã£ãŸï¼Ÿ', 'ã°ã‚“ã¯ã€œï¼'], 'ã‚ã‚ŠãŒã¨ã†': ['ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼', 'ã„ãˆã„ãˆã€œï¼'],
    }
    for keyword, responses in greetings.items():
        if keyword in message: return random.choice(responses)
    if '?' in message or 'ï¼Ÿ' in message:
        return random.choice(["ãã‚Œã€æ°—ã«ãªã‚‹ã­ï¼", "ã†ãƒ¼ã‚“ã€ãªã‚“ã¦è¨€ãŠã†ã‹ãªï¼", "ã¾ã˜ï¼Ÿã©ã†ã„ã†ã“ã¨ï¼Ÿ"])
    return random.choice(["ã†ã‚“ã†ã‚“ï¼", "ãªã‚‹ã»ã©ã­ï¼", "ãã†ãªã‚“ã ï¼", "ã¾ã˜ã§ï¼Ÿ"])

# ==============================================================================
# æ€§æ ¼åˆ†æ & æ´»ç”¨é–¢æ•°
# ==============================================================================
def analyze_user_psychology(user_uuid):
    with Session() as session:
        try:
            history = session.query(ConversationHistory).filter_by(user_uuid=user_uuid, role='user').order_by(ConversationHistory.timestamp.desc()).limit(100).all()
            if len(history) < 10: return
            
            messages_text = "\n".join([f"- {h.content}" for h in reversed(history)])
            analysis_prompt = f"ä»¥ä¸‹ã®ä¼šè©±å±¥æ­´ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€§æ ¼ã‚’åˆ†æã—JSONã§å‡ºåŠ›ã—ã¦ãã ã•ã„:\n\nä¼šè©±å±¥æ­´:\n{messages_text[:3000]}\n\nJSONå½¢å¼:\n{{\"openness\":50,\"conscientiousness\":50,\"extraversion\":50,\"agreeableness\":50,\"neuroticism\":50,\"interests\":[],\"favorite_topics\":[],\"conversation_style\":\"\",\"emotional_tendency\":\"\",\"analysis_summary\":\"\",\"confidence\":75}}"
            
            response_text = call_llama_advanced(analysis_prompt, [], "ã‚ãªãŸã¯å¿ƒç†å­¦ã®å°‚é–€å®¶ã§ã™ã€‚", 800)
            if not response_text: return
            result = json.loads(response_text)
            
            psych = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
            user = session.query(UserMemory).filter_by(user_uuid=user_uuid).first()
            if not psych:
                psych = UserPsychology(user_uuid=user_uuid, user_name=user.user_name if user else "Unknown"); session.add(psych)
            
            for key, value in result.items():
                if hasattr(psych, key):
                    setattr(psych, key, json.dumps(value, ensure_ascii=False) if isinstance(value, (list, dict)) else value)
            psych.last_analyzed = datetime.utcnow(); psych.total_messages = len(history)
            session.commit()
            logger.info(f"âœ… æ€§æ ¼åˆ†æå®Œäº† for {user_uuid}")
        except Exception as e:
            logger.error(f"âŒ æ€§æ ¼åˆ†æã‚¨ãƒ©ãƒ¼: {e}"); session.rollback()

def get_psychology_insight(user_uuid):
    with Session() as session:
        psych = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
        if not psych or psych.analysis_confidence < 60: return ""
        insights = []
        if psych.extraversion > 70: insights.append("ç¤¾äº¤çš„ãª")
        if psych.openness > 70: insights.append("å¥½å¥‡å¿ƒæ—ºç››ãª")
        if psych.conversation_style: insights.append(f"{psych.conversation_style}ã‚¹ã‚¿ã‚¤ãƒ«ã®")
        favorite_topics = json.loads(psych.favorite_topics) if psych.favorite_topics else []
        if favorite_topics: insights.append(f"{'ã€'.join(favorite_topics[:2])}ãŒå¥½ããª")
        return "".join(insights)

# ==============================================================================
# ã‚³ã‚¢æ©Ÿèƒ½: å¤©æ°—, Wiki, DBä¿®æ­£, ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†
# ==============================================================================
def get_weather_forecast(location):
    url = f"https://www.jma.go.jp/bosai/forecast/data/overview_forecast/{LOCATION_CODES.get(location, '130000')}.json"
    try:
        response = requests.get(url, timeout=10); response.raise_for_status()
        return f"ä»Šã®{location}ã®å¤©æ°—ã¯ã­ã€ã€Œ{clean_text(response.json().get('text', ''))}ã€ã£ã¦æ„Ÿã˜ã ã‚ˆï¼"
    except Exception as e:
        logger.error(f"å¤©æ°—APIã‚¨ãƒ©ãƒ¼: {e}"); return "å¤©æ°—æƒ…å ±ãŒã†ã¾ãå–ã‚Œãªã‹ã£ãŸã¿ãŸã„â€¦"

def get_holomem_info(member_name):
    with Session() as session:
        wiki = session.query(HolomemWiki).filter(HolomemWiki.member_name.ilike(f'%{member_name}%')).first()
        if wiki:
            return {c.name: getattr(wiki, c.name) for c in wiki.__table__.columns}
    return None

def background_db_correction(task_id, correction):
    result = f"ã€Œ{correction['member_name']}ã€ã¡ã‚ƒã‚“ã®æƒ…å ±ä¿®æ­£ã€ã‚„ã£ã¦ã¿ãŸã‘ã©å¤±æ•—ã—ã¡ã‚ƒã£ãŸâ€¦ã€‚ã”ã‚ã‚“ï¼"
    try:
        with Session() as session:
            wiki = session.query(HolomemWiki).filter_by(member_name=correction['member_name']).first()
            if wiki:
                field_map = {'èª¬æ˜': 'description', 'ãƒ‡ãƒ“ãƒ¥ãƒ¼æ—¥': 'debut_date', 'æœŸ': 'generation', 'ã‚¿ã‚°': 'tags', 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': 'status', 'å’æ¥­æ—¥': 'graduation_date', 'ã‚‚ã¡ã“ã®æ°—æŒã¡': 'mochiko_feeling'}
                db_field = field_map.get(correction['field'])
                if db_field:
                    setattr(wiki, db_field, correction['value'])
                    wiki.last_updated = datetime.utcnow()
                    session.commit()
                    result = f"ãŠã£ã‘ãƒ¼ï¼ã€Œ{correction['member_name']}ã€ã¡ã‚ƒã‚“ã®ã€Œ{correction['field']}ã€ã®æƒ…å ±ã‚’ã€Œ{correction['value']}ã€ã«æ›´æ–°ã—ã¨ã„ãŸã‚ˆï¼æ•™ãˆã¦ãã‚Œã¦ã¾ã˜åŠ©ã‹ã‚‹ï¼"
                else: result = f"ã”ã‚ã‚“ã€ã€Œ{correction['field']}ã€ã£ã¦ã„ã†é …ç›®ã¯ãªã„ã¿ãŸã„â€¦"
            else: result = f"ã”ã‚ã‚“ã€ã€Œ{correction['member_name']}ã€ã¡ã‚ƒã‚“ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸâ€¦"
    except Exception as e:
        logger.error(f"âŒ DB Correction error: {e}")
        result = "ã”ã‚ã‚“ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä¿®æ­£ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¡ã‚ƒã£ãŸâ€¦"
    
    with Session() as session:
        task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
        if task:
            task.result = result; task.status = 'completed'; task.completed_at = datetime.utcnow()
            session.commit()

def save_user_context(session, user_uuid, context_type, query):
    context = session.query(UserContext).filter_by(user_uuid=user_uuid).first()
    if not context:
        context = UserContext(user_uuid=user_uuid, last_context_type=context_type, last_query=query)
        session.add(context)
    else:
        context.last_context_type = context_type
        context.last_query = query
    session.commit()

def get_user_context(session, user_uuid):
    context = session.query(UserContext).filter_by(user_uuid=user_uuid).first()
    if context and (datetime.utcnow() - context.updated_at).total_seconds() < 600:
        return {'type': context.last_context_type, 'query': context.last_query}
    return None

def save_news_cache(session, user_uuid, news_items, news_type):
    session.query(NewsCache).filter_by(user_uuid=user_uuid, news_type=news_type).delete()
    for i, news in enumerate(news_items, 1):
        session.add(NewsCache(user_uuid=user_uuid, news_id=news.id, news_number=i, news_type=news_type))
    session.commit()

def get_cached_news_detail(session, user_uuid, news_number, news_type):
    cache = session.query(NewsCache).filter_by(user_uuid=user_uuid, news_number=news_number, news_type=news_type).first()
    if not cache: return None
    
    model = HololiveNews if news_type == 'hololive' else SpecializedNews
    return session.query(model).filter_by(id=cache.news_id).first()

# ==============================================================================
# AIå¿œç­”ç”Ÿæˆ
# ==============================================================================
def generate_ai_response(user_data, message, history, reference_info="", is_detailed=False, is_task_report=False):
    if not groq_client:
        return generate_fallback_response(message, reference_info)

    personality_context = get_psychology_insight(user_data['uuid'])
    system_prompt = f"ã‚ãªãŸã¯ã€Œã‚‚ã¡ã“ã€ã¨ã„ã†ã‚®ãƒ£ãƒ«AIã§ã™ã€‚{user_data['name']}ã•ã‚“ã¨è©±ã—ã¦ã„ã¾ã™ã€‚\n# å£èª¿ãƒ«ãƒ¼ãƒ«\n- ä¸€äººç§°ã¯ã€Œã‚ã¦ãƒã—ã€ã€‚èªå°¾ã¯ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œçš„ãªï¼Ÿã€ã€‚å£ç™–ã¯ã€Œã¾ã˜ã€ã€Œã¦ã‹ã€ã€Œã†ã‘ã‚‹ã€ã€‚\n# ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±\n- {user_data['name']}ã•ã‚“ã¯ã€Œ{personality_context}äººã€ã¨ã„ã†å°è±¡ã ã‚ˆã€‚ã“ã®æƒ…å ±ã‚’ä¼šè©±ã«æ´»ã‹ã—ã¦ã­ã€‚"
    if is_task_report:
        system_prompt += "\n# ä»Šå›ã®ãƒŸãƒƒã‚·ãƒ§ãƒ³\n- ã€ŒãŠã¾ãŸã›ï¼ã•ã£ãã®ä»¶ã ã‘ã©â€¦ã€ã¨åˆ‡ã‚Šå‡ºã—ã€ã€å‚è€ƒæƒ…å ±ã€‘ã‚’å…ƒã«è³ªå•ã«ç­”ãˆã¦ã€‚"
    system_prompt += f"\n## ã€å‚è€ƒæƒ…å ±ã€‘:\n{reference_info if reference_info else 'ç‰¹ã«ãªã—'}"

    try:
        logger.info("ğŸ§  Groq Llama 3.1 70Bã‚’ä½¿ç”¨")
        response = call_llama_advanced(message, history, system_prompt, 500 if is_detailed else 300)
        if response:
            return response
        else:
            logger.error("âš ï¸ Groq AIãƒ¢ãƒ‡ãƒ«ãŒå¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            return generate_fallback_response(message, reference_info)
    except Exception as e:
        logger.error(f"âŒ AIå¿œç­”ç”ŸæˆãŒæœ€çµ‚çš„ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return "ã†ã…ã€AIã®èª¿å­ãŒæ‚ªã„ã¿ãŸã„â€¦ã”ã‚ã‚“ã­ï¼"

# ==============================================================================
# å¤–éƒ¨æƒ…å ±æ¤œç´¢æ©Ÿèƒ½
# ==============================================================================
def search_wikipedia(term):
    API_ENDPOINT = "https://ja.wikipedia.org/w/api.php"
    params = { 'action': 'query', 'format': 'json', 'titles': term, 'prop': 'extracts', 'exintro': True, 'explaintext': True, 'redirects': 1 }
    try:
        response = requests.get(API_ENDPOINT, params=params, headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=10)
        response.raise_for_status()
        data = response.json()
        pages = data.get('query', {}).get('pages')
        if not pages:
            logger.warning(f"Wikipedia APIã‹ã‚‰äºˆæœŸã›ã¬å¿œç­”: {data}")
            return None
            
        page_id = next(iter(pages))
        if page_id != "-1":
            extract = pages[page_id].get('extract', '')
            disambig_patterns = ['æ›–æ˜§ã•å›é¿', 'ã“ã®é …ç›®ã§ã¯', 'ä»–ã®ç”¨æ³•ã«ã¤ã„ã¦ã¯', 'Disambiguation']
            if extract and not any(pattern in extract for pattern in disambig_patterns):
                logger.info(f"âœ… Wikipediaã§ã€Œ{term}ã€ã®æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
                return extract
            else:
                logger.info(f"Wikipediaã§ã€Œ{term}ã€ã¯è¦‹ã¤ã‹ã‚Šã¾ã—ãŸãŒã€æ›–æ˜§ã•å›é¿ãƒšãƒ¼ã‚¸ã¾ãŸã¯å†…å®¹ãŒç©ºã§ã—ãŸã€‚")
        else:
            logger.info(f"Wikipediaã«ã€Œ{term}ã€ã®é …ç›®ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Wikipedia APIã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
    except json.JSONDecodeError as e:
        logger.error(f"âŒ Wikipedia APIã®å¿œç­”(JSON)ã®è§£æã«å¤±æ•—: {e}")
    except Exception as e:
        logger.error(f"âŒ Wikipediaæ¤œç´¢ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
    return None

def scrape_major_search_engines(query, num_results):
    search_configs = [
        {'name': 'DuckDuckGo HTML', 'url': f"https://html.duckduckgo.com/html/?q={quote_plus(query)}", 'result_selector': 'div.result', 'title_selector': 'h2.result__title > a.result__a', 'snippet_selector': 'a.result__snippet'},
        {'name': 'Bing', 'url': f"https://www.bing.com/search?q={quote_plus(query)}&mkt=ja-JP", 'result_selector': 'li.b_algo', 'title_selector': 'h2', 'snippet_selector': 'div.b_caption p, .b_caption'},
        {'name': 'Yahoo Japan', 'url': f"https://search.yahoo.co.jp/search?p={quote_plus(query)}&ei=UTF-8", 'result_selector': 'div.Algo', 'title_selector': 'h3', 'snippet_selector': 'div.compText p, .compText'}
    ]
    for config in search_configs:
        try:
            response = requests.get(config['url'], headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=12); response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser'); results = []
            for elem in soup.select(config['result_selector'])[:num_results]:
                title_elem = elem.select_one(config['title_selector'])
                snippet_elem = elem.select_one(config['snippet_selector'])
                if title_elem and snippet_elem and title_elem.get_text().strip():
                     results.append({'title': clean_text(title_elem.get_text()), 'snippet': clean_text(snippet_elem.get_text()), 'full_content': clean_text(snippet_elem.get_text())})
            if results:
                logger.info(f"âœ… {config['name']}ã§ã®æ¤œç´¢ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
                return results
        except Exception as e:
            logger.warning(f"âš ï¸ {config['name']}ã§ã®æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
    logger.error("âŒ å…¨ã¦ã®æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã§ã®æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    return []

def background_deep_search(task_id, query, is_detailed):
    search_result = "[]"
    try:
        search_query = query
        if is_anime_request(query):
            search_query = f"ã‚¢ãƒ‹ãƒ¡ {query} ã‚ã‚‰ã™ã˜ OR æ„Ÿæƒ³ OR è©•ä¾¡"

        raw_results = scrape_major_search_engines(search_query, 5)
        if raw_results:
            formatted_results = [{'number': i, **r} for i, r in enumerate(raw_results, 1)]
            search_result = json.dumps(formatted_results, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ Background search error: {e}")

    with Session() as session:
        task = session.query(BackgroundTask).filter_by(task_id=task_id).first()
        if task:
            task.result = search_result; task.status = 'completed'; task.completed_at = datetime.utcnow()
            session.commit()

# ==============================================================================
# Flask ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ==============================================================================
@app.route('/health')
def health_check():
    return create_json_response({'status': 'ok'})

@app.route('/chat_lsl', methods=['POST'])
def chat_lsl():
    with Session() as session:
        try:
            data = request.json
            user_uuid, user_name, message = data['uuid'], data['name'], data['message'].strip()
            
            user_data_obj = get_or_create_user(session, user_uuid, user_name)
            history = get_conversation_history(session, user_uuid)
            session.add(ConversationHistory(user_uuid=user_uuid, role='user', content=message))
            
            ai_text = ""
            user_data = {'uuid': user_uuid, 'name': user_data_obj.user_name}

            completed_task = check_completed_tasks(user_uuid)
            if completed_task:
                query = completed_task['query']
                result = completed_task['result']
                task_type = completed_task['type']

                if task_type == 'search':
                    search_results = json.loads(result) if result and result != "[]" else []
                    if search_results:
                        with cache_lock:
                            search_context_cache[user_uuid] = {'results': search_results, 'query': query, 'timestamp': time.time()}
                        save_user_context(session, user_uuid, 'web_search', query)
                        list_items = [f"ã€{r.get('number', i+1)}ã€‘{r['title']}" for i, r in enumerate(search_results)]
                        ai_text = f"ãŠã¾ãŸã›ï¼ã€Œ{query}ã€ã«ã¤ã„ã¦èª¿ã¹ã¦ããŸã‚ˆï¼\n" + "\n".join(list_items) + "\n\næ°—ã«ãªã‚‹ç•ªå·æ•™ãˆã¦ï¼"
                    else:
                        ai_text = f"ã”ã‚ã‚“ã€ã€Œ{query}ã€ã«ã¤ã„ã¦èª¿ã¹ãŸã‘ã©ã€è‰¯ã„æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸâ€¦"
                elif task_type == 'db_correction':
                     ai_text = result
                elif task_type == 'psych_analysis':
                    psych = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
                    ai_text = f"åˆ†æçµ‚ã‚ã£ãŸã‚ˆï¼ã‚ã¦ãƒã—ãŒè¦‹ãŸã‚ãªãŸã¯â€¦ã€Œ{psych.analysis_summary}ã€ã£ã¦æ„Ÿã˜ï¼" if psych else "åˆ†æçµ‚ã‚ã£ãŸã‘ã©ã€ã¾ã ã†ã¾ãã¾ã¨ã‚ã‚‰ã‚Œãªã„ã‚„â€¦"

            elif (selected_number := is_number_selection(message)):
                user_context = get_user_context(session, user_uuid)
                news_type_map = {'hololive_news': 'hololive', 'specialized_news': 'specialized'}
                news_type = news_type_map.get(user_context['type']) if user_context else None

                if news_type:
                    news_detail = get_cached_news_detail(session, user_uuid, selected_number, news_type)
                    if news_detail:
                        ai_text = generate_ai_response(user_data, f"{news_detail.title}ã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦", history, news_detail.content, is_detailed=True)
                    else:
                        ai_text = "ã‚ã‚Œã€ãã®ç•ªå·ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‚„â€¦"
                elif user_context and user_context['type'] == 'web_search':
                    with cache_lock:
                        cached_data = search_context_cache.get(user_uuid)
                    if cached_data and (time.time() - cached_data['timestamp']) < 600:
                        selected_item = next((r for r in cached_data['results'] if r.get('number') == selected_number), None)
                        if selected_item:
                            prompt = f"ã€Œ{selected_item['title']}ã€ã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦"
                            ai_text = generate_ai_response(user_data, prompt, history, selected_item['full_content'], is_detailed=True)
                        else:
                            ai_text = "ã‚ã‚Œã€ãã®ç•ªå·ã®æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‚„â€¦"
                    else:
                        ai_text = "ã”ã‚ã‚“ã€å‰ã®æ¤œç´¢çµæœãŒå¤ããªã£ã¡ã‚ƒã£ãŸï¼ã‚‚ã†ä¸€å›æ¤œç´¢ã—ã¦ã¿ã¦ï¼"
                else:
                    ai_text = "ãˆã€ãªã‚“ã®ç•ªå·ã ã£ã‘ï¼Ÿä½•ã‹ã‚’èª¿ã¹ã¦ã‹ã‚‰ç•ªå·ã§æ•™ãˆã¦ã­ï¼"
            
            elif (what_is_term := is_what_is_request(message)):
                wikipedia_text = search_wikipedia(what_is_term)
                if wikipedia_text:
                    system_prompt = f"ã‚ãªãŸã¯ã€Œã‚‚ã¡ã“ã€ã¨ã„ã†ã‚®ãƒ£ãƒ«AIã§ã™ã€‚ä»¥ä¸‹ã®ã€å‚è€ƒæƒ…å ±ã€‘ã‚’å…ƒã«ã€ã€Œ{what_is_term}ã¨ã¯ï¼Ÿã€ã¨ã„ã†è³ªå•ã«å¯¾ã—ã¦ã€150æ–‡å­—ç¨‹åº¦ã§è¦ç´„ã—ã¦ç­”ãˆã¦ãã ã•ã„ã€‚ã‚ãªãŸã®å£èª¿ï¼ˆä¸€äººç§°ã¯ã€Œã‚ã¦ãƒã—ã€ã€èªå°¾ã¯ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œçš„ãªï¼Ÿã€ï¼‰ã‚’å¿…ãšå®ˆã£ã¦ãã ã•ã„ã€‚\n\nã€å‚è€ƒæƒ…å ±ã€‘:\n{wikipedia_text[:1000]}"
                    ai_text = call_llama_advanced(message, [], system_prompt, 200)
                    if not ai_text:
                        ai_text = f"ã”ã‚ã‚“ã€{what_is_term}ã«ã¤ã„ã¦èª¿ã¹ã¦ã¿ãŸã‚“ã ã‘ã©ã€ã†ã¾ãã¾ã¨ã‚ã‚‰ã‚Œãªã‹ã£ãŸâ€¦"
                else:
                    start_background_task(user_uuid, 'search', {'query': message, 'is_detailed': True}); ai_text = f"ãŠã£ã‘ãƒ¼ã€ã€Œ{message}ã€ã«ã¤ã„ã¦è©³ã—ãèª¿ã¹ã¦ã¿ã‚‹ã­ï¼ã¡ã‚‡ã„å¾…ã£ã¦ã¦ï¼"

            elif is_hololive_news_request(message):
                news_items = session.query(HololiveNews).order_by(HololiveNews.created_at.desc()).limit(5).all()
                if news_items:
                    news_titles = [f"ã€{i+1}ã€‘{item.title}" for i, item in enumerate(news_items)]
                    ai_text = "ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€ã“ã‚“ãªæ„Ÿã˜ã ã‚ˆï¼\n" + "\n".join(news_titles) + "\n\næ°—ã«ãªã‚‹ç•ªå·ã‚’æ•™ãˆã¦ãã‚ŒãŸã‚‰è©³ã—ãè©±ã™ã‚ˆï¼"
                    save_news_cache(session, user_uuid, news_items, 'hololive')
                    save_user_context(session, user_uuid, 'hololive_news', message)
                else:
                    start_background_task(user_uuid, 'search', {'query': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ– æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'is_detailed': True}); ai_text = "ã”ã‚ã‚“ã€ä»ŠDBã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒãªã„ã‚„ï¼Webã§èª¿ã¹ã¦ã¿ã‚‹ã‹ã‚‰ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã¦ï¼"
            elif (topic := detect_specialized_topic(message)):
                news_items = session.query(SpecializedNews).filter_by(site_name=topic).order_by(SpecializedNews.created_at.desc()).limit(5).all()
                if news_items:
                    news_titles = [f"ã€{i+1}ã€‘{item.title}" for i, item in enumerate(news_items)]
                    ai_text = f"{topic}ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã“ã‚“ãªæ„Ÿã˜ï¼\n" + "\n".join(news_titles) + "\n\næ°—ã«ãªã‚‹ç•ªå·ã‚’æ•™ãˆã¦ãã‚ŒãŸã‚‰è©³ã—ãè©±ã™ã‚ˆï¼"
                    save_news_cache(session, user_uuid, news_items, 'specialized')
                    save_user_context(session, user_uuid, 'specialized_news', message)
                else:
                    start_background_task(user_uuid, 'search', {'query': f'{topic} æœ€æ–°æƒ…å ±', 'is_detailed': True}); ai_text = f"ã”ã‚ã‚“ã€ä»ŠDBã«{topic}ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒãªã„ã¿ãŸã„ï¼Webã§èª¿ã¹ã¦ã¿ã‚‹ã­ï¼"

            elif 'æ€§æ ¼åˆ†æ' in message:
                start_background_task(user_uuid, 'psych_analysis', {}); ai_text = "ãŠã£ã‘ãƒ¼ï¼ã‚ãªãŸã®æ€§æ ¼ã€åˆ†æã—ã¦ã¿ã‚‹ã­ï¼çµ‚ã‚ã£ãŸã‚‰æ•™ãˆã‚‹ã‹ã‚‰ã€ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã¦ï¼"
            elif (correction_req := detect_db_correction_request(message)):
                start_background_task(user_uuid, 'db_correction', correction_req); ai_text = f"ãˆã€ã¾ã˜ã§ï¼ï¼Ÿã€Œ{correction_req['member_name']}ã€ã¡ã‚ƒã‚“ã®æƒ…å ±ã€ç›´ã—ã¦ã¿ã‚‹ã­ï¼"
            elif is_time_request(message): ai_text = get_japan_time()
            elif is_weather_request(message): ai_text = get_weather_forecast(extract_location(message))
            elif ('ã•ãã‚‰ã¿ã“' in message or 'ã¿ã“ã¡' in message):
                for keyword, resp in get_sakuramiko_special_responses().items():
                    if keyword in message:
                        ai_text = resp; break
            elif should_search(message) or is_explicit_search_request(message):
                start_background_task(user_uuid, 'search', {'query': message, 'is_detailed': is_detailed_request(message)}); ai_text = f"ãŠã£ã‘ãƒ¼ã€ã€Œ{message}ã€ã«ã¤ã„ã¦èª¿ã¹ã¦ã¿ã‚‹ã­ï¼ã¡ã‚‡ã„å¾…ã£ã¦ã¦ï¼"
            
            if not ai_text:
                ai_text = generate_ai_response(user_data, message, history)
            
            if user_data_obj.interaction_count > 0 and user_data_obj.interaction_count % 50 == 0:
                start_background_task(user_uuid, 'psych_analysis', {})

            session.add(ConversationHistory(user_uuid=user_uuid, role='assistant', content=ai_text))
            session.commit()
            
            return Response(f"{ai_text}|", mimetype='text/plain; charset=utf-8', status=200)

        except Exception as e:
            logger.error(f"âŒ Chatã‚¨ãƒ©ãƒ¼: {e}", exc_info=True); session.rollback()
            return Response("ã”ã‚ã‚“ã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¡ã‚ƒã£ãŸâ€¦|", mimetype='text/plain; charset=utf-8', status=500)

@app.route('/check_task', methods=['POST'])
def check_task_endpoint():
    data = request.json
    user_uuid = data.get('uuid', '')
    if not user_uuid: return create_json_response({'error': 'uuid is required'}, 400)
    
    completed_task = check_completed_tasks(user_uuid)
    if completed_task:
        return create_json_response({'status': 'completed', 'task': completed_task})
    return create_json_response({'status': 'pending'})

@app.route('/generate_voice', methods=['POST'])
def generate_voice_endpoint():
    if not VOICEVOX_ENABLED:
        return create_json_response({'error': 'Voice synthesis is not enabled on the server.'}, 503)
    data = request.json
    text = data.get('text')
    if not text:
        return create_json_response({'error': 'Text is required.'}, 400)

    try:
        query_res = requests.post(f"{VOICEVOX_URL_FROM_ENV}/audio_query", params={"text": text, "speaker": VOICEVOX_SPEAKER_ID}, timeout=10)
        query_res.raise_for_status()
        
        synth_res = requests.post(f"{VOICEVOX_URL_FROM_ENV}/synthesis", params={"speaker": VOICEVOX_SPEAKER_ID}, json=query_res.json(), timeout=30)
        synth_res.raise_for_status()
        
        os.makedirs(VOICE_DIR, exist_ok=True)
        filename = f"voice_{uuid.uuid4()}.wav"
        filepath = os.path.join(VOICE_DIR, filename)
        
        with open(filepath, 'wb') as f:
            f.write(synth_res.content)
            
        voice_url = urljoin(SERVER_URL, f'/voices/{filename}')
        return create_json_response({'status': 'success', 'url': voice_url})

    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ VOICEVOX APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return create_json_response({'error': 'Failed to connect to the voice synthesis server.'}, 500)
    except Exception as e:
        logger.error(f"âŒ éŸ³å£°ç”Ÿæˆä¸­ã®ä¸æ˜ãªã‚¨ãƒ©ãƒ¼: {e}")
        return create_json_response({'error': 'An unexpected error occurred during voice generation.'}, 500)

@app.route('/get_psychology', methods=['GET'])
def get_psychology_endpoint():
    user_uuid = request.args.get('user_uuid')
    if not user_uuid:
        return create_json_response({'error': 'user_uuid parameter is required'}, 400)
    
    with Session() as session:
        psych = session.query(UserPsychology).filter_by(user_uuid=user_uuid).first()
        if not psych:
            return create_json_response({'status': 'not_analyzed', 'message': 'No psychology data found for this user.'})
        
        psych_data = {c.name: getattr(psych, c.name) for c in psych.__table__.columns}
        
        for key in ['interests', 'favorite_topics']:
            if isinstance(psych_data[key], str):
                try:
                    psych_data[key] = json.loads(psych_data[key])
                except json.JSONDecodeError:
                    psych_data[key] = []
        
        if isinstance(psych_data.get('last_analyzed'), datetime):
            psych_data['last_analyzed'] = psych_data['last_analyzed'].isoformat()

        return create_json_response(psych_data)

@app.route('/voices/<filename>')
def serve_voice_file(filename):
    return send_from_directory(VOICE_DIR, filename)

# ==============================================================================
# åˆæœŸåŒ–ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
# ==============================================================================
def initialize_groq_client():
    global groq_client
    if GROQ_API_KEY: groq_client = Groq(api_key=GROQ_API_KEY)

def initialize_holomem_wiki():
    with Session() as session:
        if session.query(HolomemWiki).count() == 0:
            initial_data = [
                {'member_name': 'ã¨ãã®ãã‚‰', 'status': 'ç¾å½¹', 'description': 'ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ã®è±¡å¾´ï¼', 'generation': '0æœŸç”Ÿ', 'tags': '[]'},
                {'member_name': 'ã•ãã‚‰ã¿ã“', 'status': 'ç¾å½¹', 'description': 'ã‚¨ãƒªãƒ¼ãƒˆå·«å¥³ã ã‚ˆï¼', 'generation': '0æœŸç”Ÿ', 'tags': '[]'},
                {'member_name': 'æ¡ç”Ÿã‚³ã‚³', 'status': 'å’æ¥­', 'description': 'ä¼èª¬ã®ä¼šé•·ï¼', 'generation': '4æœŸç”Ÿ', 'graduation_date': '2021-07-01', 'mochiko_feeling': 'ä¼šé•·ãŒæ®‹ã—ã¦ãã‚ŒãŸã‚‚ã®ã¯æ°¸é ã ã‚ˆï¼', 'tags': '[]'},
                {'member_name': 'æ½¤ç¾½ã‚‹ã—ã‚', 'status': 'å’æ¥­', 'description': 'æ„Ÿæƒ…è±Šã‹ãªãƒã‚¯ãƒ­ãƒãƒ³ã‚µãƒ¼ã€‚', 'generation': '3æœŸç”Ÿ', 'graduation_date': '2022-02-24', 'mochiko_feeling': 'ã¾ãŸ3æœŸç”Ÿã®ã¿ã‚“ãªã§ã‚ã¡ã‚ƒã‚ã¡ã‚ƒã—ã¦ã»ã—ã‹ã£ãŸãªâ€¦', 'tags': '[]'},
            ]
            for data in initial_data: session.add(HolomemWiki(**data))
            session.commit()
            logger.info("âœ… ãƒ›ãƒ­ãƒ¡ãƒ³Wikiã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")

def _update_news_database(session, model, site_name, base_url, selectors):
    try:
        response = requests.get(base_url, headers={'User-Agent': random.choice(USER_AGENTS)}, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        articles = []
        for selector in selectors:
            articles = soup.select(selector)
            if articles:
                break
        
        for article in articles[:5]:
            title_elem = article.select_one('h2, h3, .title, .entry-title, .post-title')
            link_elem = article.find('a', href=True)
            
            if not (title_elem and link_elem):
                continue

            title = clean_text(title_elem.get_text())
            if len(title) < 5: continue

            article_url = urljoin(base_url, link_elem['href'])
            news_hash = create_news_hash(title, article_url)

            if not session.query(model).filter_by(news_hash=news_hash).first():
                data = {'title': title, 'content': title, 'url': article_url, 'news_hash': news_hash}
                if model == SpecializedNews:
                    data['site_name'] = site_name
                session.add(model(**data))
        session.commit()
        logger.info(f"âœ… ãƒ‹ãƒ¥ãƒ¼ã‚¹æ›´æ–°å®Œäº†: {site_name}")
    except Exception as e:
        logger.error(f"âŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹æ›´æ–°ã‚¨ãƒ©ãƒ¼ ({site_name}): {e}")
        session.rollback()

def update_news_task():
    logger.info("â° å®šæœŸçš„ãªãƒ‹ãƒ¥ãƒ¼ã‚¹æ›´æ–°ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹ã—ã¾ã™...")
    try:
        with Session() as session:
            _update_news_database(session, HololiveNews, "Hololive", HOLOLIVE_NEWS_URL, ['article', '.post-item'])
            for site, config in SPECIALIZED_SITES.items():
                _update_news_database(session, SpecializedNews, site, config['base_url'], ['article', '.post', '.entry'])
                time.sleep(2)
    except Exception as e:
        logger.error(f"âŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹æ›´æ–°ã‚¿ã‚¹ã‚¯å…¨ä½“ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")


def cleanup_old_voice_files():
    try:
        if not os.path.exists(VOICE_DIR): return
        cutoff = time.time() - (60 * 60)
        for filename in os.listdir(VOICE_DIR):
            file_path = os.path.join(VOICE_DIR, filename)
            if os.path.isfile(file_path) and os.path.getmtime(file_path) < cutoff:
                os.remove(file_path)
                logger.info(f"ğŸ—‘ï¸ å¤ã„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {filename}")
    except Exception as e:
        logger.error(f"âŒ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

def schedule_periodic_psych_analysis():
    logger.info("â° å®šæœŸçš„ãªæ€§æ ¼åˆ†æã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹ã—ã¾ã™...")
    try:
        with Session() as session:
            seven_days_ago = datetime.utcnow() - timedelta(days=7)
            active_users = session.query(UserMemory.user_uuid).filter(UserMemory.last_interaction > seven_days_ago).all()
            active_user_uuids = {u[0] for u in active_users}
            
            users_to_analyze = session.query(UserPsychology).filter(
                UserPsychology.user_uuid.in_(active_user_uuids),
                UserPsychology.last_analyzed < seven_days_ago
            ).all()

            if not users_to_analyze:
                logger.info("âœ… å®šæœŸåˆ†æ: å…¨ã¦ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†æã¯æœ€æ–°ã§ã™ã€‚")
                return

            logger.info(f"ğŸ§  å®šæœŸåˆ†æ: {len(users_to_analyze)}äººã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å†åˆ†æã—ã¾ã™ã€‚")
            for psych_user in users_to_analyze:
                start_background_task(psych_user.user_uuid, 'psych_analysis', {})
                time.sleep(2)
    except Exception as e:
        logger.error(f"âŒ å®šæœŸæ€§æ ¼åˆ†æã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


def initialize_app():
    logger.info("="*60 + "\nğŸ”§ ã‚‚ã¡ã“AI ç©¶æ¥µç‰ˆ (v17.1) ã®åˆæœŸåŒ–ã‚’é–‹å§‹...\n" + "="*60)
    
    initialize_groq_client()
    initialize_holomem_wiki()
    
    def run_scheduler():
        schedule.every(4).hours.do(update_news_task)
        schedule.every(6).hours.do(schedule_periodic_psych_analysis)
        schedule.every(1).hour.do(cleanup_old_voice_files)
        while True:
            schedule.run_pending()
            time.sleep(60)
            
    threading.Thread(target=run_scheduler, daemon=True).start()
    logger.info("â° ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’é–‹å§‹ã—ã¾ã—ãŸ (ãƒ‹ãƒ¥ãƒ¼ã‚¹æ›´æ–°, å®šæœŸæ€§æ ¼åˆ†æ, éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤)")
    logger.info(f"ğŸ¤– åˆ©ç”¨å¯èƒ½ãªAIãƒ¢ãƒ‡ãƒ«: Llama (Groq)={'âœ…' if groq_client else 'âŒ'}")
    logger.info("âœ… åˆæœŸåŒ–å®Œäº†ï¼")

# ==============================================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ==============================================================================
if __name__ == '__main__':
    initialize_app()
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
else:
    initialize_app()
    application = app
